{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6c65c-bea4-4674-b978-efc318aafa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library ---\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import (\n",
    "    label,\n",
    "    binary_dilation,\n",
    "    binary_erosion,\n",
    ")\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    UpSampling2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    Concatenate,\n",
    "    GlobalAveragePooling2D,\n",
    "    Reshape,\n",
    "    SpatialDropout2D,\n",
    "    Multiply,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from skimage.filters import threshold_otsu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a942c52-7a44-4766-a78c-5d4d4311f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------\n",
    "# Paths & source\n",
    "# --------------------\n",
    "BASE_PATH       = \"data\"\n",
    "FILENAME        = \"image_dicts_256_wgrayscale_andcutoffs.pkl\"\n",
    "FILE_PATH       = os.path.join(BASE_PATH, FILENAME)\n",
    "EXCEL_FILE_PATH = os.path.join(BASE_PATH, \"sample_groups.xlsx\")\n",
    "URL             = \"https://github.com/tylervasse/DOCI-Prediction/releases/download/v1.0/image_dicts_256_wgrayscale_andcutoffs.pkl\"\n",
    "\n",
    "# --------------------\n",
    "# IO helpers\n",
    "# --------------------\n",
    "def download_file(url, output_path):\n",
    "    \"\"\"Download file if missing.\"\"\"\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"File already exists at {output_path}\")\n",
    "        return\n",
    "    print(f\"Downloading to {output_path}...\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def load_image_dicts(file_path):\n",
    "    \"\"\"Load list of image dictionaries from pickle.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_sample_groups(excel_file_path):\n",
    "    \"\"\"\n",
    "    Excel must have columns: 'Train Samples', 'Validation Samples', 'Test Samples'.\n",
    "    Returns three lists of sample base names (strings).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(excel_file_path)\n",
    "        norm = lambda col: [s.strip().strip(\"'\") for s in df[col].dropna().tolist()]\n",
    "        return norm('Train Samples'), norm('Validation Samples'), norm('Test Samples')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Sample groups file not found at {excel_file_path}\")\n",
    "        return [], [], []\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel: {e}\")\n",
    "        return [], [], []\n",
    "\n",
    "# --------------------\n",
    "# Basic parsing utils\n",
    "# --------------------\n",
    "def get_base_name(name):\n",
    "    \"\"\"Sample base name = everything before '_DOCI_n'.\"\"\"\n",
    "    return name.split('_DOCI')[0]\n",
    "\n",
    "def get_doci_number(name):\n",
    "    \"\"\"Extract integer n from '_DOCI_n' (or -1 if absent).\"\"\"\n",
    "    m = re.search(r'_DOCI_(\\d+)', name)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "# --------------------\n",
    "# Split image dicts into splits by sample base name\n",
    "# --------------------\n",
    "def categorize_images(image_data, train_samples, val_samples, test_samples):\n",
    "    \"\"\"\n",
    "    Split image dicts into train/val/test by base sample name.\n",
    "    \"\"\"\n",
    "    train_set, val_set, test_set = [], [], []\n",
    "    for d in image_data:\n",
    "        base = \"_\".join(d['name'].split('_')[:2])  # e.g., 'SSW-23-12345_A1'\n",
    "        if base in train_samples:\n",
    "            train_set.append(d)\n",
    "        elif base in val_samples:\n",
    "            val_set.append(d)\n",
    "        elif base in test_samples:\n",
    "            test_set.append(d)\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "# --------------------\n",
    "# Voxelize per-sample (group by base name, sort by DOCI)\n",
    "# --------------------\n",
    "def samples_to_voxels(dataset):\n",
    "    \"\"\"\n",
    "    Group images by base sample, sort by DOCI index, and build:\n",
    "      - grayscale_voxel:  [H, W, D]\n",
    "      - grayscale_image_cutoff_voxel: [H, W, D]\n",
    "      - mask: first available mask kept as-is\n",
    "    Expected keys in each item: 'name', 'grayscale', 'image_grayscale_cutoff', 'mask', 'tissue_type'\n",
    "    \"\"\"\n",
    "    grouped = defaultdict(lambda: {\n",
    "        'names': [], 'grayscale': [], 'image_grayscale_cutoff': [], 'mask': None, 'tissue_type': None\n",
    "    })\n",
    "\n",
    "    for d in dataset:\n",
    "        base = get_base_name(d['name'])\n",
    "        grouped[base]['names'].append(d['name'])\n",
    "        grouped[base]['grayscale'].append(d['grayscale'])\n",
    "        grouped[base]['image_grayscale_cutoff'].append(d['image_grayscale_cutoff'])\n",
    "        grouped[base]['tissue_type'] = d['tissue_type']\n",
    "        if grouped[base]['mask'] is None and d.get('mask') is not None:\n",
    "            grouped[base]['mask'] = d['mask']\n",
    "\n",
    "    voxelized = []\n",
    "    for base, g in grouped.items():\n",
    "        order = sorted(range(len(g['names'])), key=lambda i: get_doci_number(g['names'][i]))\n",
    "        gray     = [g['grayscale'][i] for i in order]\n",
    "        gray_cut = [g['image_grayscale_cutoff'][i] for i in order]\n",
    "        grayscale_voxel                 = np.stack(gray, axis=-1).astype(np.float32)     # [H,W,D]\n",
    "        grayscale_image_cutoff_voxel    = np.stack(gray_cut, axis=-1).astype(np.uint8)   # [H,W,D]\n",
    "\n",
    "        voxelized.append({\n",
    "            'name': base,\n",
    "            'grayscale_voxel': grayscale_voxel,\n",
    "            'grayscale_image_cutoff_voxel': grayscale_image_cutoff_voxel,\n",
    "            'tissue_type': g['tissue_type'],\n",
    "            'mask': g['mask']\n",
    "        })\n",
    "    return voxelized\n",
    "\n",
    "# --------------------\n",
    "# Main flow\n",
    "# --------------------\n",
    "# 1) Ensure data file\n",
    "download_file(URL, FILE_PATH)\n",
    "\n",
    "# 2) Load raw dicts\n",
    "image_dicts = load_image_dicts(FILE_PATH)\n",
    "\n",
    "# 3) Exclude specific samples by substring match in 'name'\n",
    "EXCLUDE_LIST = [\"SSW-23-14395_C2\", \"SSW-23-05363_A7\"]\n",
    "image_dicts = [d for d in image_dicts if not any(excl in d['name'] for excl in EXCLUDE_LIST)]\n",
    "\n",
    "# 4) Load sample groups from Excel\n",
    "train_samples, val_samples, test_samples = load_sample_groups(EXCEL_FILE_PATH)\n",
    "\n",
    "# 5) Assign to splits and shuffle at image level\n",
    "train_set, val_set, test_set = categorize_images(image_dicts, train_samples, val_samples, test_samples)\n",
    "train_set = shuffle(train_set, random_state=42)\n",
    "val_set   = shuffle(val_set,   random_state=42)\n",
    "test_set  = shuffle(test_set,  random_state=42)\n",
    "\n",
    "# 6) Voxelize per sample\n",
    "train_combined = samples_to_voxels(train_set)\n",
    "val_combined   = samples_to_voxels(val_set)\n",
    "test_combined  = samples_to_voxels(test_set)\n",
    "\n",
    "print(f\"Samples -> train: {len(train_combined)} | val: {len(val_combined)} | test: {len(test_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9e284-43af-4010-b169-6e8dcce8d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Load regional categorization results from Excel\n",
    "# --------------------\n",
    "excel_path = \"regional_categorization_results.xlsx\"\n",
    "df_cat = pd.read_excel(excel_path)\n",
    "\n",
    "# Build lookup: (split, name) -> predicted label\n",
    "cat_lookup = {\n",
    "    (str(row[\"split\"]), str(row[\"name\"])): str(row[\"predicted\"])\n",
    "    for _, row in df_cat.iterrows()\n",
    "}\n",
    "print(f\"[LOAD] Loaded {len(cat_lookup)} categorization entries from {excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e42c59-37ae-4af7-be51-7913c0aaa4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- constants ---\n",
    "TISSUES3 = ['Normal', 'Follicular', 'Papillary']\n",
    "CLASS_TO_ID3 = {c: i for i, c in enumerate(TISSUES3)}\n",
    "TARGET_TUMOR = \"Papillary\"   # still used later when you want the TARGET-only map\n",
    "TARGET_ID = CLASS_TO_ID3[TARGET_TUMOR]\n",
    "\n",
    "# ---- Channel selection (0-based indices) ----\n",
    "# Define channels to REMOVE by index (1-based here for readability), then convert to 0-based indices.\n",
    "REMOVE_VOXEL_CHANNELS = [1, 2, 4, 7, 9, 11, 12, 14, 16, 17, 19]\n",
    "REMOVE_VOXEL_CHANNELS = [i - 1 for i in REMOVE_VOXEL_CHANNELS]\n",
    "\n",
    "# Optionally, explicitly define channels to KEEP (overrides REMOVE_* if not None)\n",
    "KEEP_VOXEL_CHANNELS = None \n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# Excel-based regional filtering + voxel-only augmentation\n",
    "# ===========================================\n",
    "# ---- CONFIG: which tumor class to filter out for this TARGET_TUMOR ----\n",
    "if TARGET_TUMOR == \"Papillary\":\n",
    "    FILTER_TUMOR = \"Follicular\"\n",
    "elif TARGET_TUMOR == \"Follicular\":\n",
    "    FILTER_TUMOR = \"Papillary\"\n",
    "else:\n",
    "    FILTER_TUMOR = \"\"  # no filtering if misconfigured\n",
    "\n",
    "FILTER_VALTEST = True               # True: also filter val/test; False: leave val/test intact\n",
    "assert FILTER_TUMOR in (\"Follicular\", \"Papillary\")\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# 1) Filtering using Excel-based predictions\n",
    "# ===========================================\n",
    "def filter_out_by_category_from_excel(samples, split_name, remove_class, lookup):\n",
    "    \"\"\"\n",
    "    Use precomputed regional predictions from Excel to filter samples.\n",
    "\n",
    "    samples:     list of sample dicts (e.g., train_combined)\n",
    "    split_name:  'train' | 'val' | 'test'\n",
    "    remove_class: label to drop, e.g., 'Follicular' or 'Papillary'\n",
    "    lookup:      dict mapping (split, name) -> predicted label\n",
    "\n",
    "    Returns:\n",
    "        kept_samples, dropped_samples\n",
    "    \"\"\"\n",
    "    kept, dropped = [], []\n",
    "    for d in samples:\n",
    "        name = str(d.get(\"name\", \"unknown_sample\"))\n",
    "        pred = lookup.get((split_name, name), None)\n",
    "\n",
    "        # If no prediction is found, keep by default\n",
    "        if pred is None:\n",
    "            kept.append(d)\n",
    "            continue\n",
    "\n",
    "        if pred == remove_class:\n",
    "            dropped.append(d)\n",
    "        else:\n",
    "            kept.append(d)\n",
    "    return kept, dropped\n",
    "\n",
    "\n",
    "# Use Excel lookup to build filtered splits\n",
    "train_px_kept, train_px_dropped = filter_out_by_category_from_excel(\n",
    "    train_combined, \"train\", FILTER_TUMOR, cat_lookup\n",
    ")\n",
    "\n",
    "if FILTER_VALTEST:\n",
    "    val_px_kept,  val_px_dropped  = filter_out_by_category_from_excel(\n",
    "        val_combined,  \"val\",  FILTER_TUMOR, cat_lookup\n",
    "    )\n",
    "    test_px_kept, test_px_dropped = filter_out_by_category_from_excel(\n",
    "        test_combined, \"test\", FILTER_TUMOR, cat_lookup\n",
    "    )\n",
    "    val_px  = val_px_kept\n",
    "    test_px = test_px_kept\n",
    "else:\n",
    "    val_px  = val_combined\n",
    "    test_px = test_combined\n",
    "\n",
    "print(\n",
    "    f\"[FILTER/EXCEL] kept: \"\n",
    "    f\"train={len(train_px_kept)} val={len(val_px)} test={len(test_px)}\"\n",
    ")\n",
    "print(\n",
    "    f\"[FILTER/EXCEL] dropped({FILTER_TUMOR}-pred): \"\n",
    "    f\"train={len(train_px_dropped)} \"\n",
    "    f\"val={len(val_px_dropped) if FILTER_VALTEST else 0} \"\n",
    "    f\"test={len(test_px_dropped) if FILTER_VALTEST else 0}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd792b5f-f8cf-4a57-afbe-b48a9308d8fb",
   "metadata": {},
   "source": [
    "# Augmentation for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be5b54-205e-4afd-8316-d635e97ef1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same regional params picked earlier\n",
    "REGIONAL_PARAMS = dict(\n",
    "    black_tol=5,\n",
    "    win=45,\n",
    "    stride=20,\n",
    "    frac_thresh=0.80,\n",
    "    min_tissue_px_per_win=30,\n",
    ")\n",
    "\n",
    "# ===========================================\n",
    "# 1) Channel selection helpers\n",
    "# ===========================================\n",
    "def _sanitize_indices(n_channels, keep, remove):\n",
    "    \"\"\"Return final list of indices to keep (sorted, unique).\"\"\"\n",
    "    if keep is not None:\n",
    "        keep = sorted(int(i) for i in keep if 0 <= int(i) < n_channels)\n",
    "        return keep\n",
    "    # build keep from remove\n",
    "    remove = set(int(i) for i in (remove or []) if 0 <= int(i) < n_channels)\n",
    "    return [i for i in range(n_channels) if i not in remove]\n",
    "\n",
    "\n",
    "def _apply_channel_filter(arr, keep_idx):\n",
    "    \"\"\"arr: [H,W,C]; keep_idx: list of channel indices to keep.\"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.ndim == 2:\n",
    "        return arr  # nothing to filter if no channel dim\n",
    "    if arr.ndim != 3:\n",
    "        raise ValueError(f\"Expected [H,W,C], got shape {arr.shape}\")\n",
    "    if len(keep_idx) == arr.shape[-1]:\n",
    "        return arr\n",
    "    return arr[..., keep_idx]\n",
    "\n",
    "\n",
    "def filter_out_by_category(samples, px_model_all, remove_class: str):\n",
    "    \"\"\"\n",
    "    Keep samples whose *regional category* != remove_class.\n",
    "    Categorization uses the PCA product computed by px_model_all\n",
    "    on the ORIGINAL (un-augmented) samples.\n",
    "    \"\"\"\n",
    "    kept = []\n",
    "    for d in samples:\n",
    "        pred = categorize_sample_regional(px_model_all, d, **REGIONAL_PARAMS)\n",
    "        if pred != remove_class:\n",
    "            kept.append(d)\n",
    "    return kept\n",
    "\n",
    "# ===========================================\n",
    "# 2) Geometric aug (centered rotate + mild zoom + optional flips/noise)\n",
    "# ===========================================\n",
    "def _build_centered_rotation_transform(height, width, angle_rad):\n",
    "    angle = tf.cast(angle_rad, tf.float32)\n",
    "    c, s  = tf.math.cos(angle), tf.math.sin(angle)\n",
    "    cx = (tf.cast(width,  tf.float32) - 1.0) / 2.0\n",
    "    cy = (tf.cast(height, tf.float32) - 1.0) / 2.0\n",
    "    a0, a1, a3, a4 = c, -s, s, c\n",
    "    a2 = -a0 * cx - a1 * cy + cx\n",
    "    a5 = -a3 * cx - a4 * cy + cy\n",
    "    return tf.stack([a0, a1, a2, a3, a4, a5, 0.0, 0.0], axis=0)\n",
    "\n",
    "\n",
    "def _rotate_any(img, angle_rad, interpolation=\"bilinear\", fill_mode=\"REFLECT\"):\n",
    "    img = tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "    H = tf.shape(img)[0]\n",
    "    W = tf.shape(img)[1]\n",
    "    transform = tf.reshape(_build_centered_rotation_transform(H, W, angle_rad), [1, 8])\n",
    "    interp = \"BILINEAR\" if interpolation.lower().startswith(\"bilinear\") else \"NEAREST\"\n",
    "    out = tf.raw_ops.ImageProjectiveTransformV3(\n",
    "        images=tf.expand_dims(img, axis=0),\n",
    "        transforms=transform,\n",
    "        output_shape=tf.stack([H, W]),\n",
    "        interpolation=interp,\n",
    "        fill_mode=fill_mode,\n",
    "        fill_value=0.0,\n",
    "    )\n",
    "    return tf.squeeze(out, axis=0)\n",
    "\n",
    "\n",
    "def augment_triplet(image, image_cutoff, mask):\n",
    "    \"\"\"\n",
    "    Apply the SAME transform to:\n",
    "      - image        : [H,W,C]  (voxel, float32)\n",
    "      - image_cutoff : [H,W,C]  (cutoff voxel, float32)\n",
    "      - mask         : [H,W] or [H,W,1]  (uint8/float32)\n",
    "    Returns augmented tensors resized/padded to 256x256.\n",
    "    \"\"\"\n",
    "    image        = tf.cast(image, tf.float32)\n",
    "    image_cutoff = tf.cast(image_cutoff, tf.float32)\n",
    "    mask         = tf.cast(mask, tf.float32)\n",
    "\n",
    "    if tf.rank(mask) == 2:\n",
    "        mask = tf.expand_dims(mask, axis=-1)\n",
    "\n",
    "    # random params\n",
    "    flip_lr   = tf.random.uniform([], 0.0, 1.0)\n",
    "    flip_ud   = tf.random.uniform([], 0.0, 1.0)\n",
    "    angle_rad = tf.random.uniform([], -15.0 * np.pi / 180.0, 15.0 * np.pi / 180.0)\n",
    "    zoom      = tf.random.uniform([], 0.90, 1.10)\n",
    "    add_noise = tf.random.uniform([], 0.0, 1.0)\n",
    "\n",
    "    # flips\n",
    "    if flip_lr > 0.5:\n",
    "        image        = tf.image.flip_left_right(image)\n",
    "        image_cutoff = tf.image.flip_left_right(image_cutoff)\n",
    "        mask         = tf.image.flip_left_right(mask)\n",
    "    if flip_ud > 0.5:\n",
    "        image        = tf.image.flip_up_down(image)\n",
    "        image_cutoff = tf.image.flip_up_down(image_cutoff)\n",
    "        mask         = tf.image.flip_up_down(mask)\n",
    "\n",
    "    # rotation\n",
    "    image        = _rotate_any(image,        angle_rad, \"bilinear\", \"REFLECT\")\n",
    "    image_cutoff = _rotate_any(image_cutoff, angle_rad, \"bilinear\", \"REFLECT\")\n",
    "    mask         = _rotate_any(mask,         angle_rad, \"nearest\",  \"REFLECT\")\n",
    "\n",
    "    # zoom\n",
    "    H = tf.shape(image)[0]\n",
    "    W = tf.shape(image)[1]\n",
    "    new_h = tf.cast(tf.cast(H, tf.float32) * zoom, tf.int32)\n",
    "    new_w = tf.cast(tf.cast(W, tf.float32) * zoom, tf.int32)\n",
    "    image        = tf.image.resize(image,        [new_h, new_w])\n",
    "    image_cutoff = tf.image.resize(image_cutoff, [new_h, new_w])\n",
    "    mask         = tf.image.resize(mask, [new_h, new_w],\n",
    "                                   method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    # center crop/pad to 256x256\n",
    "    image        = tf.image.resize_with_crop_or_pad(image,        256, 256)\n",
    "    image_cutoff = tf.image.resize_with_crop_or_pad(image_cutoff, 256, 256)\n",
    "    mask         = tf.image.resize_with_crop_or_pad(mask,         256, 256)\n",
    "\n",
    "    # noise: separate tensors to avoid channel-shape mismatch\n",
    "    if add_noise > 0.5:\n",
    "        noise_img = tf.random.normal(\n",
    "            tf.shape(image), mean=0.0, stddev=0.001, dtype=tf.float32\n",
    "        )\n",
    "        noise_cut = tf.random.normal(\n",
    "            tf.shape(image_cutoff), mean=0.0, stddev=0.001, dtype=tf.float32\n",
    "        )\n",
    "        image        = image + noise_img\n",
    "        image_cutoff = image_cutoff + noise_cut\n",
    "\n",
    "    return image, image_cutoff, tf.squeeze(mask, axis=-1)\n",
    "\n",
    "# --- simple control over aug counts ---\n",
    "POS_AUGS = 3\n",
    "NEG_AUGS = 1\n",
    "\n",
    "\n",
    "def _has_positive(msk):\n",
    "    m = np.asarray(msk)\n",
    "    if m.ndim == 3 and m.shape[-1] == 1:\n",
    "        m = m[..., 0]\n",
    "    return np.sum(m) > 0\n",
    "\n",
    "\n",
    "def make_augmented_copies(samples, pos_augs=POS_AUGS, neg_augs=NEG_AUGS):\n",
    "    \"\"\"\n",
    "    Return base (resized) + N augmented copies per sample; mask NN-resized.\n",
    "\n",
    "    IMPORTANT:\n",
    "    - We DO NOT compute/store any PCA/PX features here.\n",
    "      Downstream code will call px_model_all.predict_proba_map(d)\n",
    "      on each (augmented) dict to recompute PX maps from the augmented voxel.\n",
    "    - Applies channel selection (KEEP_* or REMOVE_*) prior to any resizing/augmentation.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for d in samples:\n",
    "        img = np.asarray(d[\"grayscale_voxel\"], np.float32)              # [H,W,Cv]\n",
    "        cut = np.asarray(d[\"grayscale_image_cutoff_voxel\"], np.float32) # [H,W,Cc]\n",
    "        msk = np.asarray(\n",
    "            d.get(\"mask\", np.zeros(img.shape[:2], np.uint8)),\n",
    "            np.float32,\n",
    "        )\n",
    "\n",
    "        # ---- channel filtering (pre-augmentation) ----\n",
    "        Cv = img.shape[-1] if img.ndim == 3 else 1\n",
    "        Cc = cut.shape[-1] if cut.ndim == 3 else 1\n",
    "\n",
    "        keep_vox = _sanitize_indices(Cv, KEEP_VOXEL_CHANNELS,   REMOVE_VOXEL_CHANNELS)\n",
    "        keep_cut = _sanitize_indices(Cc, KEEP_VOXEL_CHANNELS,  REMOVE_VOXEL_CHANNELS)\n",
    "\n",
    "        # If you also want to filter voxel channels, uncomment:\n",
    "        img = _apply_channel_filter(img, keep_vox)\n",
    "        cut = _apply_channel_filter(cut, keep_cut)   # [H,W,Cc']\n",
    "\n",
    "        # resize to 256x256 once (voxel-only augmentation target)\n",
    "        img0 = tf.image.resize(img, [256, 256]).numpy()\n",
    "        cut0 = tf.image.resize(cut, [256, 256]).numpy()\n",
    "        msk0 = tf.image.resize(\n",
    "            msk[..., None], [256, 256],\n",
    "            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
    "        ).numpy()[..., 0]\n",
    "\n",
    "        # base (unaltered except resize/pad)\n",
    "        base = {\n",
    "            **d,\n",
    "            \"grayscale_voxel\": img0,\n",
    "            \"grayscale_image_cutoff_voxel\": cut0,\n",
    "            \"mask\": msk0,\n",
    "            # helpful metadata for traceability\n",
    "            \"voxel_channels_kept\": keep_vox,\n",
    "            \"cutoff_channels_kept\": keep_cut,\n",
    "        }\n",
    "        # ensure no stale PX fields if present\n",
    "        for k in (\"px_probs\", \"px_map\", \"px_features\"):\n",
    "            base.pop(k, None)\n",
    "        out.append(base)\n",
    "\n",
    "        # augmented copies (voxel-only)\n",
    "        n_aug = pos_augs if _has_positive(msk0) else neg_augs\n",
    "        for _ in range(n_aug):\n",
    "            ai, ac, am = augment_triplet(img0, cut0, msk0)\n",
    "            aug = {\n",
    "                **d,\n",
    "                \"grayscale_voxel\": ai.numpy(),\n",
    "                \"grayscale_image_cutoff_voxel\": ac.numpy(),\n",
    "                \"mask\": am.numpy(),\n",
    "                \"voxel_channels_kept\": keep_vox,\n",
    "                \"cutoff_channels_kept\": keep_cut,\n",
    "            }\n",
    "            for k in (\"px_probs\", \"px_map\", \"px_features\"):\n",
    "                aug.pop(k, None)\n",
    "            out.append(aug)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---- Apply categorize-first filtering (PCA-based) + voxel-only augmentation ----\n",
    "train_px_aug  = make_augmented_copies(train_px_kept, pos_augs=POS_AUGS, neg_augs=NEG_AUGS)\n",
    "val_px_aug  = make_augmented_copies(val_px,  pos_augs=0, neg_augs=0)\n",
    "test_px_aug = make_augmented_copies(test_px, pos_augs=0, neg_augs=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58c09c1-4288-4312-8f44-1eaae4171736",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae8d58c-1b0a-4734-b24c-d33021dac8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Voxel-only SE-UNet pipeline\n",
    "# (channel selection & regional gating applied upstream)\n",
    "# ===========================================\n",
    "# ------------------------------\n",
    "# CONFIG\n",
    "# ------------------------------\n",
    "KEEP_ANAPLASTIC_WHEN_TARGET = True  # used for reporting only (gating done upstream)\n",
    "assert TARGET_TUMOR in (\"Papillary\", \"Follicular\")\n",
    "\n",
    "# ------------------------------\n",
    "# Masks & input tensors (expects 256×256 from augmentation)\n",
    "# ------------------------------\n",
    "def ensure_mask(d, H, W):\n",
    "    \"\"\"Return [H,W,1] float32 mask (0/1), resizing if needed.\"\"\"\n",
    "    gt = d.get(\"tissue_type\", \"\")\n",
    "    if gt == \"Normal\":\n",
    "        m = np.zeros((H, W), np.uint8)\n",
    "    else:\n",
    "        m = np.asarray(d.get(\"mask\", np.zeros((H, W), np.uint8)), np.uint8)\n",
    "        if m.ndim == 3 and m.shape[-1] == 1:\n",
    "            m = m[..., 0]\n",
    "        if m.shape != (H, W):\n",
    "            m = tf.image.resize(\n",
    "                m[..., None], (H, W),\n",
    "                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "            ).numpy().squeeze()\n",
    "    return (m > 0).astype(np.float32)[..., None]\n",
    "\n",
    "\n",
    "def build_vox_inputs(samples):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X_vox: [N,H,W,C] scaled to 0..1 (channels already filtered upstream)\n",
    "      Y    : [N,H,W,1] tumor mask\n",
    "    \"\"\"\n",
    "    Xv, Y = [], []\n",
    "    for d in samples:\n",
    "        voxel = np.asarray(d[\"grayscale_voxel\"], np.float32)\n",
    "        H, W, _ = voxel.shape\n",
    "        mask = ensure_mask(d, H, W)\n",
    "        Xv.append(voxel / 255.0)\n",
    "        Y.append(mask)\n",
    "    return np.stack(Xv, 0), np.stack(Y, 0)\n",
    "\n",
    "# ------------------------------\n",
    "# Metrics & Losses\n",
    "# ------------------------------\n",
    "_bce_none = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=\"none\")\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1e-6, empty_score=1.0):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(tf.cast(y_pred, tf.float32), 1e-6, 1 - 1e-6)\n",
    "    t = tf.reshape(y_true, (tf.shape(y_true)[0], -1))\n",
    "    p = tf.reshape(y_pred, (tf.shape(y_pred)[0], -1))\n",
    "    inter = tf.reduce_sum(t * p, axis=1)\n",
    "    den = tf.reduce_sum(t + p, axis=1)\n",
    "    dice = (2.0 * inter + smooth) / (den + smooth)\n",
    "\n",
    "    both_empty = tf.logical_and(\n",
    "        tf.equal(tf.reduce_sum(t, 1), 0.0),\n",
    "        tf.equal(tf.reduce_sum(p, 1), 0.0)\n",
    "    )\n",
    "    dice = tf.where(both_empty, tf.fill(tf.shape(dice), empty_score), dice)\n",
    "    return tf.reduce_mean(dice)\n",
    "\n",
    "\n",
    "def dice_nonempty(y_true, y_pred, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(tf.cast(y_pred, tf.float32), 1e-6, 1 - 1e-6)\n",
    "    t = tf.reshape(y_true, (tf.shape(y_true)[0], -1))\n",
    "    p = tf.reshape(y_pred, (tf.shape(y_pred)[0], -1))\n",
    "    inter = tf.reduce_sum(t * p, axis=1)\n",
    "    den = tf.reduce_sum(t + p, axis=1)\n",
    "    dice = (2.0 * inter + smooth) / (den + smooth)\n",
    "    nonempty = tf.greater(tf.reduce_sum(t, axis=1), 0.0)\n",
    "    dice = tf.boolean_mask(dice, nonempty)\n",
    "    return tf.cond(\n",
    "        tf.size(dice) > 0,\n",
    "        lambda: tf.reduce_mean(dice),\n",
    "        lambda: tf.constant(1.0),\n",
    "    )\n",
    "\n",
    "\n",
    "def iou_coef(y_true, y_pred, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(tf.cast(y_pred, tf.float32), 1e-6, 1 - 1e-6)\n",
    "    t = tf.reshape(y_true, (tf.shape(y_true)[0], -1))\n",
    "    p = tf.reshape(y_pred, (tf.shape(y_pred)[0], -1))\n",
    "    inter = tf.reduce_sum(t * p, axis=1)\n",
    "    union = tf.reduce_sum(t + p, axis=1) - inter\n",
    "    return tf.reduce_mean((inter + smooth) / (union + smooth))\n",
    "\n",
    "\n",
    "def make_weighted_bce(pos_weight_scalar):\n",
    "    pw = tf.constant(float(pos_weight_scalar), dtype=tf.float32)\n",
    "\n",
    "    def weighted_bce(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(tf.cast(y_pred, tf.float32), 1e-6, 1 - 1e-6)\n",
    "        per_px = _bce_none(y_true, y_pred)\n",
    "        weights = 1.0 + (pw - 1.0) * tf.squeeze(y_true, -1)\n",
    "        num = tf.reduce_sum(weights * per_px, axis=[1, 2])\n",
    "        den = tf.reduce_sum(weights, axis=[1, 2]) + 1e-6\n",
    "        return tf.reduce_mean(num / den)\n",
    "\n",
    "    return weighted_bce\n",
    "\n",
    "\n",
    "def tversky_index(y_true, y_pred, alpha=0.7, beta=0.3, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(tf.cast(y_pred, tf.float32), 1e-6, 1 - 1e-6)\n",
    "    tp = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "    fp = tf.reduce_sum((1.0 - y_true) * y_pred, axis=[1, 2, 3])\n",
    "    fn = tf.reduce_sum(y_true * (1.0 - y_pred), axis=[1, 2, 3])\n",
    "    return tf.reduce_mean((tp + smooth) / (tp + alpha * fp + beta * fn + smooth))\n",
    "\n",
    "\n",
    "def focal_tversky_loss(alpha=0.7, beta=0.3, gamma=0.75):\n",
    "    def _loss(y_true, y_pred):\n",
    "        t = tversky_index(y_true, y_pred, alpha=alpha, beta=beta)\n",
    "        return tf.pow(1.0 - t, gamma)\n",
    "\n",
    "    return _loss\n",
    "\n",
    "\n",
    "def make_combined_loss(pos_weight_scalar):\n",
    "    wbce = make_weighted_bce(pos_weight_scalar)\n",
    "    ftv = focal_tversky_loss(alpha=0.7, beta=0.3, gamma=0.75)\n",
    "\n",
    "    def _loss(y_true, y_pred):\n",
    "        return 0.5 * wbce(y_true, y_pred) + 0.5 * ftv(y_true, y_pred)\n",
    "\n",
    "    return _loss\n",
    "\n",
    "# ------------------------------\n",
    "# SE-UNet (voxel-only)\n",
    "# ------------------------------\n",
    "def squeeze_excite_block(x, reduction=16):\n",
    "    c = x.shape[-1]\n",
    "    s = GlobalAveragePooling2D()(x)\n",
    "    s = Reshape((1, 1, c))(s)\n",
    "    s = Dense(max(c // reduction, 4), activation=\"relu\")(s)\n",
    "    s = Dense(c, activation=\"sigmoid\")(s)\n",
    "    return Multiply()([x, s])\n",
    "\n",
    "\n",
    "def enc_block_SE(x, filters, p_drop=0.10):\n",
    "    x = Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if filters >= 128:\n",
    "        x = squeeze_excite_block(x)\n",
    "    x = SpatialDropout2D(p_drop)(x)\n",
    "    p = Conv2D(filters, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    return x, p\n",
    "\n",
    "\n",
    "def dec_block_SE(x, skip, filters):\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Concatenate()([x, skip])\n",
    "    x = Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_unet_vox_only(vox_ch=23):\n",
    "    vox_in = Input((256, 256, vox_ch), name=\"vox_in\")\n",
    "    x0 = Conv2D(32, 3, padding=\"same\", activation=\"relu\")(vox_in)\n",
    "    e1, p1 = enc_block_SE(x0,   32, 0.10)\n",
    "    e2, p2 = enc_block_SE(p1,   64, 0.10)\n",
    "    e3, p3 = enc_block_SE(p2,  128, 0.10)\n",
    "    e4, p4 = enc_block_SE(p3,  256, 0.10)\n",
    "    e5, p5 = enc_block_SE(p4,  512, 0.15)\n",
    "\n",
    "    b = Conv2D(1024, 3, padding=\"same\", activation=\"relu\")(p5)\n",
    "    b = BatchNormalization()(b)\n",
    "    b = Dropout(0.2)(b)\n",
    "\n",
    "    d5 = dec_block_SE(b,  e5, 512)\n",
    "    d4 = dec_block_SE(d5, e4, 256)\n",
    "    d3 = dec_block_SE(d4, e3, 128)\n",
    "    d2 = dec_block_SE(d3, e2,  64)\n",
    "    d1 = dec_block_SE(d2, e1,  32)\n",
    "\n",
    "    out = Conv2D(1, 1, activation=\"sigmoid\", name=\"seg_output\")(d1)\n",
    "    return Model(vox_in, out, name=\"SE_UNet_VoxelOnly\")\n",
    "\n",
    "# ------------------------------\n",
    "# Build filtered datasets (already gated & augmented upstream)\n",
    "# ------------------------------\n",
    "# Use these consistently downstream\n",
    "train_f = train_px_aug\n",
    "val_f   = val_px_aug\n",
    "test_f  = test_px_aug\n",
    "\n",
    "print(\n",
    "    \"Example channels:\",\n",
    "    f\"train C={np.asarray(train_f[0]['grayscale_voxel']).shape[-1]}\",\n",
    "    f\"val C={np.asarray(val_f[0]['grayscale_voxel']).shape[-1]}\",\n",
    "    f\"test C={np.asarray(test_f[0]['grayscale_voxel']).shape[-1]}\",\n",
    ")\n",
    "\n",
    "Xtr_vox, Ytr = build_vox_inputs(train_f)\n",
    "Xva_vox, Yva = build_vox_inputs(val_f)\n",
    "Xte_vox, Yte = build_vox_inputs(test_f)\n",
    "\n",
    "print(\n",
    "    f\"Kept after regional gate (Normal + {TARGET_TUMOR}\"\n",
    "    f\"{' + Anaplastic' if KEEP_ANAPLASTIC_WHEN_TARGET else ''}): \"\n",
    "    f\"train={len(train_f)}, val={len(val_f)}, test={len(test_f)}\"\n",
    ")\n",
    "print(\n",
    "    \"Shapes:\",\n",
    "    \"\\n  train:\", Xtr_vox.shape, Ytr.shape,\n",
    "    \"\\n  val:  \", Xva_vox.shape, Yva.shape,\n",
    "    \"\\n  test: \", Xte_vox.shape, Yte.shape,\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Compile & train\n",
    "# ------------------------------\n",
    "pos_frac = float(np.mean(Ytr > 0.5)) if Ytr.size else 0.5\n",
    "neg_frac = 1.0 - pos_frac\n",
    "pos_weight_val = min(5.0, float(neg_frac / (pos_frac + 1e-6)))\n",
    "print(f\"pos_frac ~ {pos_frac:.4f} | pos_weight (capped) ~ {pos_weight_val:.3f}\")\n",
    "\n",
    "model = build_unet_vox_only(vox_ch=Xtr_vox.shape[-1])\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=3e-4),\n",
    "    loss=make_combined_loss(pos_weight_val),\n",
    "    metrics=[dice_coef, dice_nonempty, iou_coef],\n",
    ")\n",
    "\n",
    "ckpt_path = (\n",
    "    f\"test1.weights.h5\")\n",
    "\n",
    "early = EarlyStopping(\n",
    "    monitor=\"val_dice_nonempty\", mode=\"max\", patience=20, restore_best_weights=True, verbose=1, )\n",
    "\n",
    "ckpt = ModelCheckpoint(\n",
    "    ckpt_path, monitor=\"val_dice_nonempty\", mode=\"max\", save_best_only=True, save_weights_only=True, verbose=1, )\n",
    "\n",
    "rlrop = ReduceLROnPlateau(\n",
    "    monitor=\"val_dice_nonempty\", mode=\"max\", factor=0.5, patience=6, min_lr=1e-6, verbose=1, )\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluation helper\n",
    "# ------------------------------\n",
    "def evaluate(y_true, y_prob, t):\n",
    "    \"\"\"\n",
    "    Thresholded Dice/IoU summary over a batch.\n",
    "\n",
    "    Returns:\n",
    "      (dice_mean, dice_std, dice_median),\n",
    "      (iou_mean,  iou_std,  iou_median)\n",
    "    \"\"\"\n",
    "    y_true = (y_true > 0.5).astype(np.uint8)\n",
    "    y_pred = (y_prob >= t).astype(np.uint8)\n",
    "\n",
    "    dices, ious = [], []\n",
    "    for i in range(len(y_true)):\n",
    "        gt = y_true[i, ..., 0]\n",
    "        pr = y_pred[i, ..., 0]\n",
    "        inter = np.logical_and(gt, pr).sum()\n",
    "\n",
    "        den_d = gt.sum() + pr.sum()\n",
    "        dice = (2 * inter) / (den_d + 1e-6) if den_d > 0 else 1.0\n",
    "\n",
    "        union = np.logical_or(gt, pr).sum()\n",
    "        iou = inter / (union + 1e-6) if union > 0 else 1.0\n",
    "\n",
    "        dices.append(dice)\n",
    "        ious.append(iou)\n",
    "\n",
    "    return (\n",
    "        (np.mean(dices), np.std(dices), np.median(dices)),\n",
    "        (np.mean(ious),  np.std(ious),  np.median(ious)),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd36b09-fb87-4799-8ea1-60791d126746",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4400ad6-2c33-4c9f-a18d-72d75092eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Channel policy (already applied upstream)\n",
    "# ------------------------------\n",
    "print(f\"[ChannelFilter] Using {Xtr_vox.shape[-1]} channels.\")\n",
    "\n",
    "# ------------------------------\n",
    "# Fit model\n",
    "# ------------------------------\n",
    "history = model.fit(\n",
    "    Xtr_vox, Ytr,\n",
    "    validation_data=(Xva_vox, Yva),\n",
    "    batch_size=2,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    callbacks=[ckpt, rlrop, early],\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Threshold calibration + evaluation\n",
    "# ------------------------------\n",
    "def _mean_dice_for_threshold(y_true, y_prob, t):\n",
    "    \"\"\"Compute mean Dice over a batch at threshold t.\"\"\"\n",
    "    y_true_bin = (y_true > 0.5).astype(np.uint8)\n",
    "    y_pred_bin = (y_prob >= t).astype(np.uint8)\n",
    "\n",
    "    dices = []\n",
    "    for i in range(len(y_true_bin)):\n",
    "        gt = y_true_bin[i, ..., 0]\n",
    "        pr = y_pred_bin[i, ..., 0]\n",
    "        inter = np.logical_and(gt, pr).sum()\n",
    "        den = gt.sum() + pr.sum()\n",
    "        d = (2 * inter) / (den + 1e-6) if den > 0 else 1.0\n",
    "        dices.append(d)\n",
    "    return float(np.mean(dices)) if dices else 1.0\n",
    "\n",
    "\n",
    "def best_dice_threshold(y_true, y_prob, grid=np.linspace(0.2, 0.9, 29)):\n",
    "    \"\"\"\n",
    "    Scan thresholds on grid and return:\n",
    "      best_t, best_mean_dice (on y_true / y_prob).\n",
    "    \"\"\"\n",
    "    best_t, best_d = 0.5, -1.0\n",
    "    for t in grid:\n",
    "        md = _mean_dice_for_threshold(y_true, y_prob, t)\n",
    "        if md > best_d:\n",
    "            best_d, best_t = md, float(t)\n",
    "    return best_t, best_d\n",
    "\n",
    "\n",
    "# --- Calibrate on validation set ---\n",
    "val_probs = model.predict(Xva_vox, batch_size=2)\n",
    "t_star, dice_val = best_dice_threshold(Yva, val_probs)\n",
    "print(f\"[CAL] t*={t_star:.3f} (VAL mean Dice={dice_val:.3f})\")\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_probs = model.predict(Xte_vox, batch_size=2)\n",
    "(d_mean, d_std, d_med), (i_mean, i_std, i_med) = evaluate(Yte, test_probs, t_star)\n",
    "\n",
    "print(f\"[TEST] Dice mean={d_mean:.3f} ± {d_std:.3f} | median={d_med:.3f}\")\n",
    "print(f\"[TEST]  IoU  mean={i_mean:.3f} ± {i_std:.3f} | median={i_med:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b8a97-995d-4b9b-83eb-00c303484f92",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a2d2ae-154a-46e9-8c16-3eb516b59ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights saved for the voxel-only model\n",
    "model.load_weights(\n",
    "    \"test1.weights.h5\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# UID helper\n",
    "# ---------------------------------------------------------------------\n",
    "def sample_uid(d):\n",
    "    \"\"\"Build a stable string identifier for a sample dict.\"\"\"\n",
    "    for k in (\"uid\", \"id\", \"sample_id\", \"image_name\", \"name\", \"file\", \"path\"):\n",
    "        if k in d and d[k] is not None:\n",
    "            return str(d[k])\n",
    "    v   = np.asarray(d[\"grayscale_voxel\"])\n",
    "    cut = np.asarray(d[\"grayscale_image_cutoff_voxel\"])\n",
    "    return f\"auto:{v.shape}-{cut.shape}-{int(v.size % 997)}\"\n",
    "\n",
    "\n",
    "U_gt = [sample_uid(d) for d in test_f]\n",
    "print(\"TEST shapes:\", Xte_vox.shape, Yte.shape, f\"uids={len(U_gt)}\")\n",
    "\n",
    "# --- Predict on test set (same tensors used for metrics) ---\n",
    "test_probs = model.predict(Xte_vox, batch_size=2)\n",
    "\n",
    "# Sanity checks\n",
    "assert test_probs.shape[0] == Yte.shape[0] == len(U_gt)\n",
    "assert Xte_vox.shape[:3] == Yte.shape[:3] == test_probs.shape[:3]\n",
    "\n",
    "GT_mask = Yte          # [N,H,W,1]\n",
    "PROBS   = test_probs   # [N,H,W,1]\n",
    "\n",
    "# Use previously calibrated t_star if present; else compute per-image Otsu fallback\n",
    "t_star_value = globals().get(\"t_star\", None)\n",
    "use_global_thresh = t_star_value is not None\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Per-sample visualization: CNN P(Tumor) / GT / Thresholded Pred\n",
    "# ---------------------------------------------------------------------\n",
    "for i in range(PROBS.shape[0]):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "    # --- Extract data ---\n",
    "    voxel_ch0 = Xte_vox[i, ..., 0]      # <-- FIRST CHANNEL OF VOXEL STACK\n",
    "    pt_cnn    = PROBS[i, ..., 0]        # CNN probability map\n",
    "    gt        = GT_mask[i, ..., 0]      # GT mask\n",
    "\n",
    "    # --- Threshold selection ---\n",
    "    if use_global_thresh:\n",
    "        t_use = float(t_star_value)\n",
    "    else:\n",
    "        try:\n",
    "            t_use = float(threshold_otsu(pt_cnn))\n",
    "        except Exception:\n",
    "            t_use = 0.5\n",
    "\n",
    "    pred = (pt_cnn >= t_use).astype(np.uint8)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # PANEL 1: First voxel channel (instead of PCA or CNN probs)\n",
    "    # ---------------------------------------------------------------------\n",
    "    ax[0].imshow(voxel_ch0, cmap=\"gray\")\n",
    "    ax[0].set_title(\"Voxel: Channel 0\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    # PANEL 2: Ground-truth mask\n",
    "    ax[1].imshow(gt, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    ax[1].set_title(\"GT\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    # PANEL 3: Thresholded prediction\n",
    "    ax[2].imshow(pred, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    ax[2].set_title(f\"Pred (t={t_use:.2f})\")\n",
    "    ax[2].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"sample: {U_gt[i]}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bebb2-d687-4605-be04-4e3416b801b9",
   "metadata": {},
   "source": [
    "# Final Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2289582-68da-4730-91e6-33f7f33b88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Basic helpers\n",
    "# ----------------------------\n",
    "def _ensure_2d(mask):\n",
    "    \"\"\"Cast mask to 2D uint8 array.\"\"\"\n",
    "    arr = np.asarray(mask)\n",
    "    if arr.ndim == 3:\n",
    "        arr = arr[..., 0]\n",
    "    return arr.astype(np.uint8)\n",
    "\n",
    "\n",
    "def _threshold_probs(y_prob, t):\n",
    "    \"\"\"Threshold [N,H,W,1] or [N,H,W] probs at t -> uint8 mask.\"\"\"\n",
    "    arr = np.asarray(y_prob)\n",
    "    if arr.ndim == 4:\n",
    "        arr = arr[..., 0]\n",
    "    return (arr >= float(t)).astype(np.uint8)\n",
    "\n",
    "\n",
    "def dice_per_image(gt, pr, eps=1e-6):\n",
    "    gt = _ensure_2d(gt)\n",
    "    pr = _ensure_2d(pr)\n",
    "    inter = np.logical_and(gt, pr).sum()\n",
    "    den   = gt.sum() + pr.sum()\n",
    "    return (2 * inter) / (den + eps) if den > 0 else 1.0\n",
    "\n",
    "\n",
    "def iou_per_image(gt, pr, eps=1e-6):\n",
    "    gt = _ensure_2d(gt)\n",
    "    pr = _ensure_2d(pr)\n",
    "    inter = np.logical_and(gt, pr).sum()\n",
    "    union = np.logical_or(gt, pr).sum()\n",
    "    return inter / (union + eps) if union > 0 else 1.0\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Threshold calibration + simple Dice/IoU\n",
    "# ----------------------------\n",
    "def best_dice_threshold(y_true, y_prob, grid=np.linspace(0.2, 0.9, 29)):\n",
    "    \"\"\"\n",
    "    Scan thresholds over grid and return:\n",
    "      best_t, best_mean_dice (using per-image Dice).\n",
    "    \"\"\"\n",
    "    y_true_bin = (y_true > 0.5).astype(np.uint8)\n",
    "    best_t, best_d = 0.5, -1.0\n",
    "\n",
    "    for t in grid:\n",
    "        y_pred_bin = _threshold_probs(y_prob, t)\n",
    "        dices = []\n",
    "        for i in range(len(y_true_bin)):\n",
    "            gt = y_true_bin[i, ..., 0]\n",
    "            pr = y_pred_bin[i, ...]\n",
    "            dices.append(dice_per_image(gt, pr))\n",
    "        md = float(np.mean(dices)) if dices else 1.0\n",
    "        if md > best_d:\n",
    "            best_d, best_t = md, float(t)\n",
    "\n",
    "    return best_t, best_d\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_prob, t):\n",
    "    \"\"\"\n",
    "    Basic summary: Dice & IoU mean/std/median at threshold t.\n",
    "    \"\"\"\n",
    "    y_true_bin = (y_true > 0.5).astype(np.uint8)\n",
    "    y_pred_bin = _threshold_probs(y_prob, t)\n",
    "\n",
    "    dices, ious = [], []\n",
    "    for i in range(len(y_true_bin)):\n",
    "        gt = y_true_bin[i, ..., 0]\n",
    "        pr = y_pred_bin[i, ...]\n",
    "        dices.append(dice_per_image(gt, pr))\n",
    "        ious.append(iou_per_image(gt, pr))\n",
    "\n",
    "    dices = np.asarray(dices)\n",
    "    ious  = np.asarray(ious)\n",
    "\n",
    "    return (\n",
    "        (float(dices.mean()), float(dices.std()), float(np.median(dices))),\n",
    "        (float(ious.mean()),  float(ious.std()),  float(np.median(ious))),\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Predict once on all splits ---\n",
    "train_probs = model.predict(Xtr_vox, batch_size=2)\n",
    "val_probs   = model.predict(Xva_vox, batch_size=2)\n",
    "test_probs  = model.predict(Xte_vox, batch_size=2)\n",
    "\n",
    "# Optional sanity checks\n",
    "assert train_probs.shape[0] == Ytr.shape[0]\n",
    "assert val_probs.shape[0]   == Yva.shape[0]\n",
    "assert test_probs.shape[0]  == Yte.shape[0]\n",
    "\n",
    "# --- Calibrate threshold on validation set ---\n",
    "t_star, dice_val = best_dice_threshold(Yva, val_probs)\n",
    "print(f\"[CAL] t*={t_star:.3f} (VAL mean Dice={dice_val:.3f})\")\n",
    "\n",
    "# --- Basic test-set evaluation ---\n",
    "(d_mean, d_std, d_med), (i_mean, i_std, i_med) = evaluate(Yte, test_probs, t_star)\n",
    "print(f\"[TEST] Dice mean={d_mean:.3f} ± {d_std:.3f} | median={d_med:.3f}\")\n",
    "print(f\"[TEST]  IoU  mean={i_mean:.3f} ± {i_std:.3f} | median={i_med:.3f}\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Rich metrics for imbalanced data\n",
    "# ============================\n",
    "def _boundary_band(mask, radius=2):\n",
    "    m = _ensure_2d(mask) > 0\n",
    "    if m.size == 0:\n",
    "        return np.zeros_like(m, dtype=np.uint8)\n",
    "    dil = binary_dilation(m, iterations=radius)\n",
    "    ero = binary_erosion(m,  iterations=radius)\n",
    "    return np.logical_xor(dil, ero).astype(np.uint8)\n",
    "\n",
    "\n",
    "def dice_nonempty_mean(y_true, y_pred):\n",
    "    scores = []\n",
    "    for i in range(len(y_true)):\n",
    "        gt = _ensure_2d(y_true[i])\n",
    "        pr = _ensure_2d(y_pred[i])\n",
    "        if gt.sum() > 0:\n",
    "            scores.append(dice_per_image(gt, pr))\n",
    "    return (np.mean(scores) if scores else 1.0), np.array(scores)\n",
    "\n",
    "\n",
    "def empty_fp_penalty_mean(y_true, y_pred, tissue_masks=None):\n",
    "    \"\"\"\n",
    "    For images with empty GT, measure 1 - (FP / tissue_pixels or total_pixels).\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for i in range(len(y_true)):\n",
    "        gt = _ensure_2d(y_true[i])\n",
    "        if gt.sum() == 0:\n",
    "            pr = _ensure_2d(y_pred[i])\n",
    "            if tissue_masks is not None:\n",
    "                T = (tissue_masks[i] > 0).astype(np.uint8)\n",
    "                denom = max(1, T.sum())\n",
    "                fp = (pr * (1 - gt) * T).sum()\n",
    "            else:\n",
    "                denom = pr.size\n",
    "                fp = (pr * (1 - gt)).sum()\n",
    "            scores.append(1.0 - (fp / denom))\n",
    "    return (np.mean(scores) if scores else 1.0), np.array(scores)\n",
    "\n",
    "\n",
    "def boundary_f1_mean(y_true, y_pred, radius=2, eps=1e-6):\n",
    "    vals = []\n",
    "    for i in range(len(y_true)):\n",
    "        gt = _ensure_2d(y_true[i])\n",
    "        pr = _ensure_2d(y_pred[i])\n",
    "        Bgt = _boundary_band(gt, radius=radius)\n",
    "        Bpr = _boundary_band(pr, radius=radius)\n",
    "        tp = np.logical_and(Bgt, Bpr).sum()\n",
    "        fp = np.logical_and((1 - Bgt), Bpr).sum()\n",
    "        fn = np.logical_and(Bgt, (1 - Bpr)).sum()\n",
    "        prec = tp / (tp + fp + eps)\n",
    "        rec  = tp / (tp + fn + eps)\n",
    "        f1   = (2 * prec * rec) / (prec + rec + eps)\n",
    "        vals.append(f1)\n",
    "    return float(np.mean(vals)), np.array(vals)\n",
    "\n",
    "\n",
    "def lesion_f1_mean(y_true, y_pred, iou_thresh=0.5):\n",
    "    def _components(mask):\n",
    "        lab, n = label(mask > 0)\n",
    "        return [(lab == k).astype(np.uint8) for k in range(1, n + 1)]\n",
    "\n",
    "    f1s = []\n",
    "    for i in range(len(y_true)):\n",
    "        gt = _ensure_2d(y_true[i])\n",
    "        pr = _ensure_2d(y_pred[i])\n",
    "        gt_cs = _components(gt)\n",
    "        pr_cs = _components(pr)\n",
    "\n",
    "        if len(gt_cs) == 0 and len(pr_cs) == 0:\n",
    "            f1s.append(1.0)\n",
    "            continue\n",
    "        if len(gt_cs) == 0 or len(pr_cs) == 0:\n",
    "            f1s.append(0.0)\n",
    "            continue\n",
    "\n",
    "        IoU = np.zeros((len(gt_cs), len(pr_cs)), dtype=np.float32)\n",
    "        gt_areas = np.array([c.sum() for c in gt_cs], dtype=np.float32)\n",
    "        pr_areas = np.array([c.sum() for c in pr_cs], dtype=np.float32)\n",
    "\n",
    "        for g_idx, g in enumerate(gt_cs):\n",
    "            for p_idx, p in enumerate(pr_cs):\n",
    "                inter = np.logical_and(g, p).sum()\n",
    "                union = gt_areas[g_idx] + pr_areas[p_idx] - inter\n",
    "                IoU[g_idx, p_idx] = inter / (union + 1e-6)\n",
    "\n",
    "        matched_gt, matched_pr = set(), set()\n",
    "        pairs = []\n",
    "        all_pairs = [\n",
    "            (g, p)\n",
    "            for g in range(len(gt_cs))\n",
    "            for p in range(len(pr_cs))\n",
    "        ]\n",
    "        for g_idx, p_idx in sorted(all_pairs,\n",
    "                                   key=lambda x: IoU[x[0], x[1]],\n",
    "                                   reverse=True):\n",
    "            if (\n",
    "                IoU[g_idx, p_idx] >= iou_thresh\n",
    "                and g_idx not in matched_gt\n",
    "                and p_idx not in matched_pr\n",
    "            ):\n",
    "                matched_gt.add(g_idx)\n",
    "                matched_pr.add(p_idx)\n",
    "                pairs.append((g_idx, p_idx))\n",
    "\n",
    "        tp = len(pairs)\n",
    "        fp = len(pr_cs) - tp\n",
    "        fn = len(gt_cs) - tp\n",
    "        prec = tp / (tp + fp + 1e-6)\n",
    "        rec  = tp / (tp + fn + 1e-6)\n",
    "        f1   = (2 * prec * rec) / (prec + rec + 1e-6)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    return float(np.mean(f1s)), np.array(f1s)\n",
    "\n",
    "\n",
    "def evaluate_imbalanced(y_true, y_prob, t, tissue_masks=None, boundary_radius=2):\n",
    "    \"\"\"\n",
    "    Rich evaluation set for imbalanced segmentation tasks.\n",
    "    \"\"\"\n",
    "    y_pred = _threshold_probs(y_prob, t)\n",
    "\n",
    "    dice_pos_mean_val, dice_pos_all = dice_nonempty_mean(y_true, y_pred)\n",
    "    empty_pen_mean_val, empty_pen_all = empty_fp_penalty_mean(\n",
    "        y_true, y_pred, tissue_masks=tissue_masks\n",
    "    )\n",
    "    balanced_dice = 0.5 * (dice_pos_mean_val + empty_pen_mean_val)\n",
    "\n",
    "    b_f1_mean_val, b_f1_all = boundary_f1_mean(\n",
    "        y_true, y_pred, radius=boundary_radius\n",
    "    )\n",
    "    l_f1_mean_val, l_f1_all = lesion_f1_mean(\n",
    "        y_true, y_pred, iou_thresh=0.5\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"dice_pos_mean\": float(dice_pos_mean_val),\n",
    "        \"empty_penalty_mean\": float(empty_pen_mean_val),\n",
    "        \"balanced_dice\": float(balanced_dice),\n",
    "        \"boundary_f1_mean\": float(b_f1_mean_val),\n",
    "        \"lesion_f1_mean\": float(l_f1_mean_val),\n",
    "        \"per_image\": {\n",
    "            \"dice_pos\": dice_pos_all,\n",
    "            \"empty_penalty\": empty_pen_all,\n",
    "            \"boundary_f1\": b_f1_all,\n",
    "            \"lesion_f1\": l_f1_all,\n",
    "        },\n",
    "    }\n",
    "\n",
    "train_tissue_masks = None\n",
    "val_tissue_masks   = None\n",
    "test_tissue_masks  = None\n",
    "\n",
    "train_metrics = evaluate_imbalanced(Ytr, train_probs, t_star, tissue_masks=train_tissue_masks)\n",
    "val_metrics   = evaluate_imbalanced(Yva, val_probs,   t_star, tissue_masks=val_tissue_masks)\n",
    "test_metrics  = evaluate_imbalanced(Yte, test_probs,  t_star, tissue_masks=test_tissue_masks)\n",
    "\n",
    "print(\n",
    "    \"[TRAIN] dice_pos_mean={:.3f} | empty_penalty_mean={:.3f} | \"\n",
    "    \"balanced_dice={:.3f} | boundary_f1_mean={:.3f} | lesion_f1_mean={:.3f}\"\n",
    "    .format(\n",
    "        train_metrics[\"dice_pos_mean\"],\n",
    "        train_metrics[\"empty_penalty_mean\"],\n",
    "        train_metrics[\"balanced_dice\"],\n",
    "        train_metrics[\"boundary_f1_mean\"],\n",
    "        train_metrics[\"lesion_f1_mean\"],\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"\\n[VAL]  dice_pos_mean={:.3f} | empty_penalty_mean={:.3f} | \"\n",
    "    \"balanced_dice={:.3f} | boundary_f1_mean={:.3f} | lesion_f1_mean={:.3f}\"\n",
    "    .format(\n",
    "        val_metrics[\"dice_pos_mean\"],\n",
    "        val_metrics[\"empty_penalty_mean\"],\n",
    "        val_metrics[\"balanced_dice\"],\n",
    "        val_metrics[\"boundary_f1_mean\"],\n",
    "        val_metrics[\"lesion_f1_mean\"],\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"[TEST] dice_pos_mean={:.3f} | empty_penalty_mean={:.3f} | \"\n",
    "    \"balanced_dice={:.3f} | boundary_f1_mean={:.3f} | lesion_f1_mean={:.3f}\"\n",
    "    .format(\n",
    "        test_metrics[\"dice_pos_mean\"],\n",
    "        test_metrics[\"empty_penalty_mean\"],\n",
    "        test_metrics[\"balanced_dice\"],\n",
    "        test_metrics[\"boundary_f1_mean\"],\n",
    "        test_metrics[\"lesion_f1_mean\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc12477-a6d2-4ad4-bb4f-1034e6e834a0",
   "metadata": {},
   "source": [
    "# Filter Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b104042-6648-4256-910f-f1e4463009e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resolve_threshold(y_true, probs):\n",
    "    # Prefer calibrated global thresholds if available\n",
    "    if \"t_star_calibrated\" in globals():\n",
    "        try:\n",
    "            return float(t_star_calibrated)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if \"t_star\" in globals():\n",
    "        try:\n",
    "            return float(t_star)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Otherwise calibrate on provided set (val/test) using  best_dice_threshold()\n",
    "    if \"best_dice_threshold\" in globals():\n",
    "        t_, _ = best_dice_threshold(y_true, probs)\n",
    "        return float(t_)\n",
    "    # Last resort\n",
    "    return 0.5\n",
    "\n",
    "# -- Helper: evaluate a probability map tensor at threshold t using evaluate() --\n",
    "def _eval_with_threshold(y_true, probs, t):\n",
    "    return evaluate(y_true, probs, t)  # returns ((dice_mean, dice_std, dice_med), (iou_mean, iou_std, iou_med))\n",
    "\n",
    "# -- Optional: permutation within image (spatial shuffle) for a channel --\n",
    "def _permute_channel_inplace(x, chan, rng):\n",
    "    # x: [N,H,W,C]\n",
    "    N, H, W, C = x.shape\n",
    "    flat = x[..., chan].reshape(N, -1)\n",
    "    for i in range(N):\n",
    "        rng.shuffle(flat[i])\n",
    "    x[..., chan] = flat.reshape(N, H, W)\n",
    "\n",
    "def run_filter_ablation(\n",
    "    model,\n",
    "    X, Y,\n",
    "    batch_size=2,\n",
    "    mode=\"zero\",          # {\"zero\", \"permute\"}\n",
    "    repeats=1,            # repeats for permutation mode (mean over trials)\n",
    "    seed=0,\n",
    "    channel_map_1based=None  # list of length C with 1-based channel labels; if None, auto = [1..C]\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a pandas.DataFrame with per-channel metrics and drops vs baseline.\n",
    "    X: [N,H,W,C] input tensor used for evaluation (e.g., val or test stack)\n",
    "    Y: [N,H,W,1] ground-truth mask (float or uint8)\n",
    "    mode=\"zero\": set channel to 0\n",
    "    mode=\"permute\": spatially shuffle pixels of that channel within each image\n",
    "    \"\"\"\n",
    "    assert X.ndim == 4 and Y.ndim == 4 and X.shape[:3] == Y.shape[:3]\n",
    "    C = X.shape[-1]\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    # Build nice channel labels\n",
    "    if channel_map_1based is None:\n",
    "        # Try to back-map to original selection if used KEEP_IDX; else fall back to 1..C\n",
    "        if \"KEEP_IDX\" in globals():\n",
    "            # KEEP_IDX are zero-based original indices; convert to 1-based labels\n",
    "            channel_map_1based = [int(i + 1) for i in list(KEEP_IDX)]\n",
    "        else:\n",
    "            channel_map_1based = list(range(1, C + 1))\n",
    "    assert len(channel_map_1based) == C, \"channel_map_1based length must match X[...,C]\"\n",
    "\n",
    "    # ---- Baseline ----\n",
    "    probs_base = model.predict(X, batch_size=batch_size)\n",
    "    t_use = _resolve_threshold(Y, probs_base)\n",
    "    (d_mean_b, d_std_b, d_med_b), (i_mean_b, i_std_b, i_med_b) = _eval_with_threshold(Y, probs_base, t_use)\n",
    "\n",
    "    rows = []\n",
    "    for c in range(C):\n",
    "        if mode == \"zero\":\n",
    "            X_mod = X.copy()\n",
    "            X_mod[..., c] = 0.0\n",
    "            probs_c = model.predict(X_mod, batch_size=batch_size)\n",
    "            (d_mean, d_std, d_med), (i_mean, i_std, i_med) = _eval_with_threshold(Y, probs_c, t_use)\n",
    "        elif mode == \"permute\":\n",
    "            d_means, i_means = [], []\n",
    "            for r in range(repeats):\n",
    "                X_mod = X.copy()\n",
    "                _permute_channel_inplace(X_mod, c, rng)\n",
    "                probs_c = model.predict(X_mod, batch_size=batch_size)\n",
    "                (d_mean, _, _), (i_mean, _, _) = _eval_with_threshold(Y, probs_c, t_use)\n",
    "                d_means.append(d_mean); i_means.append(i_mean)\n",
    "            # aggregate over repeats\n",
    "            d_mean = float(np.mean(d_means)); i_mean = float(np.mean(i_means))\n",
    "            # std over repeats for quick uncertainty (not pixel-level)\n",
    "            d_std, i_std = float(np.std(d_means)), float(np.std(i_means))\n",
    "            d_med = np.nan; i_med = np.nan  # (optional)\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'zero' or 'permute'\")\n",
    "\n",
    "        rows.append({\n",
    "            \"chan_idx_0b\": c,\n",
    "            \"chan_label_1b\": channel_map_1based[c],\n",
    "            \"dice_mean\": d_mean,\n",
    "            \"dice_drop\": d_mean_b - d_mean,\n",
    "            \"iou_mean\": i_mean,\n",
    "            \"iou_drop\": i_mean_b - i_mean,\n",
    "            \"baseline_dice_mean\": d_mean_b,\n",
    "            \"baseline_iou_mean\": i_mean_b,\n",
    "            \"threshold_used\": t_use,\n",
    "            \"mode\": mode,\n",
    "            \"repeats\": repeats\n",
    "        })\n",
    "\n",
    "    # ---- Aggregate results ----\n",
    "    df = (\n",
    "        pd.DataFrame(rows)\n",
    "        .sort_values(by=[\"dice_drop\", \"iou_drop\"], ascending=[False, False])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Nicely print both top and bottom 10 filters\n",
    "    with pd.option_context('display.max_rows', None, 'display.width', 120):\n",
    "        print(\"=== 10 Most Impactful Filters (largest Dice drop) ===\")\n",
    "        print(\n",
    "            df[[\n",
    "                \"chan_label_1b\", \"chan_idx_0b\",\n",
    "                \"dice_mean\", \"dice_drop\",\n",
    "                \"iou_mean\", \"iou_drop\",\n",
    "                \"threshold_used\", \"mode\"\n",
    "            ]].head(10)\n",
    "        )\n",
    "\n",
    "        print(\"\\n=== 10 Least Impactful Filters (smallest Dice drop) ===\")\n",
    "        print(\n",
    "            df[[\n",
    "                \"chan_label_1b\", \"chan_idx_0b\",\n",
    "                \"dice_mean\", \"dice_drop\",\n",
    "                \"iou_mean\", \"iou_drop\",\n",
    "                \"threshold_used\", \"mode\"\n",
    "            ]].tail(10)\n",
    "        )\n",
    "\n",
    "    # --- summarize ---\n",
    "    most10 = df.sort_values(\"dice_drop\", ascending=False).head(10)\n",
    "    least10 = df.sort_values(\"dice_drop\", ascending=True).head(10)\n",
    "    \n",
    "    print(\"\\n=== Summary ===\")\n",
    "    print(f\"Baseline Dice: {d_mean_b:.3f}\")\n",
    "    print(f\"Baseline IoU : {i_mean_b:.3f}\")\n",
    "    print(f\"Top filter impact range: {most10['dice_drop'].min():.3f}–{most10['dice_drop'].max():.3f}\")\n",
    "    print(f\"Least filter impact range: {least10['dice_drop'].min():.3f}–{least10['dice_drop'].max():.3f}\")\n",
    "    \n",
    "    # --- visualization ---\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.bar(most10[\"chan_label_1b\"].astype(str), most10[\"dice_drop\"], color=\"salmon\", label=\"Most Impactful\")\n",
    "    plt.bar(least10[\"chan_label_1b\"].astype(str), least10[\"dice_drop\"], color=\"skyblue\", label=\"Least Impactful\")\n",
    "    plt.axhline(0, color=\"gray\", lw=1)\n",
    "    plt.xlabel(\"Channel Label (1-based)\")\n",
    "    plt.ylabel(\"ΔDice vs Baseline\")\n",
    "    plt.title(\"Filter Importance via Ablation\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- optional: export results to CSV ---\n",
    "    df.to_csv(\"filter_ablation_results.csv\", index=False)\n",
    "    print(\"\\nSaved full results to 'filter_ablation_results.csv'\")\n",
    "\n",
    "    return df, {\n",
    "        \"baseline\": {\"dice_mean\": d_mean_b, \"iou_mean\": i_mean_b, \"t\": t_use},\n",
    "        \"mode\": mode,\n",
    "        \"repeats\": repeats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4863477-912e-4a60-a579-76b677822a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perm, meta_perm = run_filter_ablation(\n",
    "    model=model,\n",
    "    X=Xva_vox,\n",
    "    Y=Yva.astype(np.float32),\n",
    "    batch_size=2,\n",
    "    mode=\"permute\",\n",
    "    repeats=3,s\n",
    "    seed=123\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
