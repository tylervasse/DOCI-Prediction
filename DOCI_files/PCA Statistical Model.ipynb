{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd09e21-ad5d-4f2d-9018-293acee6ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library ---\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.ndimage import (\n",
    "    gaussian_filter,\n",
    "    sobel,\n",
    "    binary_dilation, \n",
    "    binary_erosion\n",
    ")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc5dc1-7b5b-43df-88af-78917f365b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Clean Imports (deduplicated and organized)\n",
    "# ===========================================\n",
    "\n",
    "# --------------------\n",
    "# Paths & source\n",
    "# --------------------\n",
    "BASE_PATH       = \"data\"\n",
    "FILENAME        = \"image_dicts_256_wgrayscale_andcutoffs.pkl\"\n",
    "FILE_PATH       = os.path.join(BASE_PATH, FILENAME)\n",
    "EXCEL_FILE_PATH = os.path.join(BASE_PATH, \"sample_groups.xlsx\")\n",
    "URL             = \"https://github.com/tylervasse/DOCI-Prediction/releases/download/v1.0/image_dicts_256_wgrayscale_andcutoffs.pkl\"\n",
    "\n",
    "# --------------------\n",
    "# IO helpers\n",
    "# --------------------\n",
    "def download_file(url, output_path):\n",
    "    \"\"\"\n",
    "    Downloads a file from a given URL if it does not already exist locally.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL pointing to the file to download.\n",
    "        output_path (str): Local path where the downloaded file should be saved.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints status messages indicating whether the file was downloaded\n",
    "            or already existed at the target path.\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"File already exists at {output_path}\")\n",
    "        return\n",
    "    print(f\"Downloading to {output_path}...\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def load_image_dicts(file_path):\n",
    "    \"\"\"\n",
    "    Loads a list of image dictionaries from a pickle file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the pickle file containing serialized image data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing image metadata and pixel data.\n",
    "              Returns an empty list if the file does not exist or loading fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_sample_groups(excel_file_path):\n",
    "    \"\"\"\n",
    "    Loads training, validation, and test sample group identifiers from an Excel file.\n",
    "\n",
    "    Args:\n",
    "        excel_file_path (str): Path to the Excel file specifying sample groups.\n",
    "                               Must contain the columns:\n",
    "                               'Train Samples', 'Validation Samples', 'Test Samples'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three lists:\n",
    "            - list of str: Training sample base names.\n",
    "            - list of str: Validation sample base names.\n",
    "            - list of str: Test sample base names.\n",
    "        Each list will be empty if the file is missing or cannot be parsed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(excel_file_path)\n",
    "        norm = lambda col: [s.strip().strip(\"'\") for s in df[col].dropna().tolist()]\n",
    "        return norm('Train Samples'), norm('Validation Samples'), norm('Test Samples')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Sample groups file not found at {excel_file_path}\")\n",
    "        return [], [], []\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel: {e}\")\n",
    "        return [], [], []\n",
    "\n",
    "# --------------------\n",
    "# Basic parsing utils\n",
    "# --------------------\n",
    "def get_base_name(name):\n",
    "    \"\"\"\n",
    "    Extracts the base sample name from a DOCI image filename.\n",
    "\n",
    "    Args:\n",
    "        name (str): Full image filename containing a suffix like '_DOCI_n'.\n",
    "\n",
    "    Returns:\n",
    "        str: The base name preceding '_DOCI_n', used to associate slices\n",
    "             belonging to the same specimen.\n",
    "    \"\"\"\n",
    "    return name.split('_DOCI')[0]\n",
    "\n",
    "def get_doci_number(name):\n",
    "    \"\"\"\n",
    "    Extracts the DOCI index number from an image filename.\n",
    "\n",
    "    Args:\n",
    "        name (str): Filename containing the pattern '_DOCI_<number>'.\n",
    "\n",
    "    Returns:\n",
    "        int: The extracted DOCI slice index. Returns -1 if no index is found.\n",
    "    \"\"\"\n",
    "    m = re.search(r'_DOCI_(\\d+)', name)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "# --------------------\n",
    "# Split image dicts into splits by sample base name\n",
    "# --------------------\n",
    "def categorize_images(image_data, train_samples, val_samples, test_samples):\n",
    "    \"\"\"\n",
    "    Categorizes image dictionaries into training, validation, and test sets\n",
    "    based on their base sample names.\n",
    "\n",
    "    Args:\n",
    "        image_data (list of dict): List of image dictionaries containing at least\n",
    "                                   the key 'name'.\n",
    "        train_samples (list of str): Base names assigned to the training split.\n",
    "        val_samples (list of str): Base names assigned to the validation split.\n",
    "        test_samples (list of str): Base names assigned to the test split.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three lists:\n",
    "            - list of dict: Training image dictionaries.\n",
    "            - list of dict: Validation image dictionaries.\n",
    "            - list of dict: Test image dictionaries.\n",
    "    \"\"\"\n",
    "    train_set, val_set, test_set = [], [], []\n",
    "    for d in image_data:\n",
    "        base = \"_\".join(d['name'].split('_')[:2])  # e.g., 'SSW-23-12345_A1'\n",
    "        if base in train_samples:\n",
    "            train_set.append(d)\n",
    "        elif base in val_samples:\n",
    "            val_set.append(d)\n",
    "        elif base in test_samples:\n",
    "            test_set.append(d)\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "# --------------------\n",
    "# Voxelize per-sample (group by base name, sort by DOCI)\n",
    "# --------------------\n",
    "def samples_to_voxels(dataset):\n",
    "    \"\"\"\n",
    "    Groups individual DOCI images by specimen, sorts them by DOCI index,\n",
    "    and constructs voxel stacks across the depth dimension.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of dict): List of image dictionaries containing keys:\n",
    "            - 'name' (str): Filename used to infer sample grouping.\n",
    "            - 'grayscale' (numpy.ndarray): 2D grayscale DOCI image.\n",
    "            - 'image_grayscale_cutoff' (numpy.ndarray): Cutoff-processed grayscale image.\n",
    "            - 'mask' (numpy.ndarray or None): Tumor mask, if available.\n",
    "            - 'tissue_type' (str): Annotated tissue label for the sample.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of voxelized sample dictionaries, each containing:\n",
    "            - 'name' (str): Base sample name.\n",
    "            - 'grayscale_voxel' (numpy.ndarray): Stacked grayscale images [H, W, D].\n",
    "            - 'grayscale_image_cutoff_voxel' (numpy.ndarray): Stacked cutoff images [H, W, D].\n",
    "            - 'tissue_type' (str): Tissue class for the sample.\n",
    "            - 'mask' (numpy.ndarray or None): First available mask across slices.\n",
    "    \"\"\"\n",
    "    grouped = defaultdict(lambda: {\n",
    "        'names': [], 'grayscale': [], 'image_grayscale_cutoff': [], 'mask': None, 'tissue_type': None\n",
    "    })\n",
    "\n",
    "    for d in dataset:\n",
    "        base = get_base_name(d['name'])\n",
    "        grouped[base]['names'].append(d['name'])\n",
    "        grouped[base]['grayscale'].append(d['grayscale'])\n",
    "        grouped[base]['image_grayscale_cutoff'].append(d['image_grayscale_cutoff'])\n",
    "        grouped[base]['tissue_type'] = d['tissue_type']\n",
    "        if grouped[base]['mask'] is None and d.get('mask') is not None:\n",
    "            grouped[base]['mask'] = d['mask']\n",
    "\n",
    "    voxelized = []\n",
    "    for base, g in grouped.items():\n",
    "        order = sorted(range(len(g['names'])), key=lambda i: get_doci_number(g['names'][i]))\n",
    "        gray     = [g['grayscale'][i] for i in order]\n",
    "        gray_cut = [g['image_grayscale_cutoff'][i] for i in order]\n",
    "        grayscale_voxel                 = np.stack(gray, axis=-1).astype(np.float32)     # [H,W,D]\n",
    "        grayscale_image_cutoff_voxel    = np.stack(gray_cut, axis=-1).astype(np.uint8)   # [H,W,D]\n",
    "\n",
    "        voxelized.append({\n",
    "            'name': base,\n",
    "            'grayscale_voxel': grayscale_voxel,\n",
    "            'grayscale_image_cutoff_voxel': grayscale_image_cutoff_voxel,\n",
    "            'tissue_type': g['tissue_type'],\n",
    "            'mask': g['mask']\n",
    "        })\n",
    "    return voxelized\n",
    "\n",
    "# ====================\n",
    "# Main flow\n",
    "# ====================\n",
    "# 1) Ensure data file\n",
    "download_file(URL, FILE_PATH)\n",
    "\n",
    "# 2) Load raw dicts\n",
    "image_dicts = load_image_dicts(FILE_PATH)\n",
    "\n",
    "# 3) Exclude specific samples by substring match in 'name'\n",
    "EXCLUDE_LIST = [\"SSW-23-14395_C2\", \"SSW-23-05363_A7\"]\n",
    "image_dicts = [d for d in image_dicts if not any(excl in d['name'] for excl in EXCLUDE_LIST)]\n",
    "\n",
    "# 4) Load sample groups from Excel\n",
    "train_samples, val_samples, test_samples = load_sample_groups(EXCEL_FILE_PATH)\n",
    "\n",
    "# 5) Assign to splits and shuffle at image level\n",
    "train_set, val_set, test_set = categorize_images(image_dicts, train_samples, val_samples, test_samples)\n",
    "train_set = shuffle(train_set, random_state=42)\n",
    "val_set   = shuffle(val_set,   random_state=42)\n",
    "test_set  = shuffle(test_set,  random_state=42)\n",
    "\n",
    "# 6) Voxelize per sample\n",
    "train_combined = samples_to_voxels(train_set)\n",
    "val_combined   = samples_to_voxels(val_set)\n",
    "test_combined  = samples_to_voxels(test_set)\n",
    "\n",
    "print(f\"Samples -> train: {len(train_combined)} | val: {len(val_combined)} | test: {len(test_combined)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73059f33-f4a7-4c68-9303-512eb00c71c1",
   "metadata": {},
   "source": [
    "# Regional Categorization from the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e1887-a063-4405-a145-bd1df17b9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Train PCA/Classifier on ALL tumor types, then\n",
    "# filter out Follicular-dominant samples from\n",
    "# TRAIN / VAL / TEST (based on PCA-feature model)\n",
    "# ===========================================\n",
    "\n",
    "# --- constants ---\n",
    "TISSUES3 = ['Normal', 'Follicular', 'Papillary']\n",
    "CLASS_TO_ID3 = {c: i for i, c in enumerate(TISSUES3)}\n",
    "TARGET_TUMOR = \"Follicular\"\n",
    "NONTARGET_TUMOR = {\"Papillary\": \"Follicular\",\n",
    "                   \"Follicular\": \"Papillary\"}.get(TARGET_TUMOR)\n",
    "TARGET_ID = CLASS_TO_ID3[TARGET_TUMOR]\n",
    "\n",
    "# ---- Channel selection (0-based indices) ----\n",
    "# Define channels to REMOVE by index (1-based here for readability), then convert to 0-based indices.\n",
    "REMOVE_VOXEL_CHANNELS = [1, 2, 4, 7, 9, 11, 12, 14, 16, 17, 19]\n",
    "REMOVE_VOXEL_CHANNELS = [i - 1 for i in REMOVE_VOXEL_CHANNELS]\n",
    "\n",
    "# Optionally, explicitly define channels to KEEP (overrides REMOVE_* if not None)\n",
    "KEEP_VOXEL_CHANNELS = None        # e.g., [2,3,4,5,6]\n",
    "\n",
    "# ------------------------------\n",
    "# Channel policy for PCA pipeline\n",
    "# ------------------------------\n",
    "def _sanitize_indices_pca(C, keep=None, remove=None):\n",
    "    \"\"\"\n",
    "    Computes the set of voxel channel indices to retain for PCA preprocessing\n",
    "    based on user-specified KEEP or REMOVE channel lists.\n",
    "\n",
    "    Args:\n",
    "        C (int): Total number of available voxel channels.\n",
    "        keep (list of int or None): Explicit list of channels to retain\n",
    "            (0-based indices). If provided, REMOVE is ignored.\n",
    "        remove (list of int or None): List of channels to remove\n",
    "            (0-based indices). Used only if KEEP is None.\n",
    "\n",
    "    Returns:\n",
    "        list of int: Sorted list of channel indices to keep.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If both keep and remove are provided, or if the resulting\n",
    "            set of retained channels is empty.\n",
    "    \"\"\"\n",
    "    if keep not in (None, []) and remove not in (None, []):\n",
    "        raise ValueError(\"Use either KEEP_VOXEL_CHANNELS or REMOVE_VOXEL_CHANNELS, not both.\")\n",
    "    if keep not in (None, []):\n",
    "        idx = sorted({int(i) for i in keep if 0 <= int(i) < C})\n",
    "    elif remove not in (None, []):\n",
    "        bad = {int(i) for i in remove if 0 <= int(i) < C}\n",
    "        idx = [i for i in range(C) if i not in bad]\n",
    "    else:\n",
    "        idx = list(range(C))\n",
    "    if not idx:\n",
    "        raise ValueError(\"No channels left after applying channel policy.\")\n",
    "    return idx\n",
    "\n",
    "def _apply_channel_filter_to_samples_pca(sample_list, keep_idx):\n",
    "    \"\"\"\n",
    "    Applies an in-place voxel-channel filter to a list of samples by retaining\n",
    "    only the specified channel indices.\n",
    "\n",
    "    Args:\n",
    "        sample_list (list of dict or None): List of sample dictionaries,\n",
    "            each containing a 'grayscale_voxel' array of shape [H, W, C].\n",
    "        keep_idx (list of int): Channel indices to retain (0-based).\n",
    "\n",
    "    Returns:\n",
    "        list or None: The modified sample list with filtered voxel channels.\n",
    "                      Returns None if the input list is None.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If a voxel has incompatible shape or if keep_idx contains\n",
    "            indices outside the available channel range.\n",
    "    \"\"\"\n",
    "    if sample_list is None:\n",
    "        return None\n",
    "    for d in sample_list:\n",
    "        if 'grayscale_voxel' not in d:\n",
    "            continue\n",
    "        v = np.asarray(d['grayscale_voxel'], np.float32)\n",
    "        if v.ndim != 3:\n",
    "            raise ValueError(f\"Expected voxel shape [H,W,C], got {v.shape}\")\n",
    "        if max(keep_idx) >= v.shape[-1]:\n",
    "            raise ValueError(\n",
    "                f\"keep_idx {keep_idx} incompatible with voxel shape {v.shape}\"\n",
    "            )\n",
    "        d['grayscale_voxel'] = v[..., keep_idx]\n",
    "    return sample_list\n",
    "\n",
    "# Apply channel policy to each set *before* PCA/scaler training\n",
    "try:\n",
    "    # Infer original channel count from train_combined\n",
    "    C0 = None\n",
    "    for d0 in train_combined:\n",
    "        if 'grayscale_voxel' in d0:\n",
    "            C0 = np.asarray(d0['grayscale_voxel']).shape[-1]\n",
    "            break\n",
    "    if C0 is None:\n",
    "        raise RuntimeError(\"Could not infer voxel channel count from train_combined.\")\n",
    "\n",
    "    keep_idx_pca = _sanitize_indices_pca(C0, KEEP_VOXEL_CHANNELS, REMOVE_VOXEL_CHANNELS)\n",
    "    print(f\"[PCA] Applying voxel channel policy: {C0} -> {len(keep_idx_pca)} using {[i+1 for i in keep_idx_pca]}\")\n",
    "\n",
    "    # Apply to all relevant sample sets in-place\n",
    "    train_combined = _apply_channel_filter_to_samples_pca(train_combined, keep_idx_pca)\n",
    "    try:\n",
    "        val_combined = _apply_channel_filter_to_samples_pca(val_combined, keep_idx_pca)\n",
    "    except NameError:\n",
    "        pass\n",
    "    try:\n",
    "        test_combined = _apply_channel_filter_to_samples_pca(test_combined, keep_idx_pca)\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "except NameError:\n",
    "    # If KEEP_VOXEL_CHANNELS / REMOVE_VOXEL_CHANNELS or train_combined not defined yet\n",
    "    print(\"[PCA] Channel policy not found or sample sets undefined here; using all voxel channels for PCA.\")\n",
    "\n",
    "# ------------------------------\n",
    "# Tissue mask from cutoff\n",
    "# ------------------------------\n",
    "def _tissue_mask_from_cutoff(cutvoxel, black_tolerance=5):\n",
    "    \"\"\"\n",
    "    Generates a binary tissue mask from a grayscale cutoff image or voxel.\n",
    "\n",
    "    Args:\n",
    "        cutvoxel (numpy.ndarray): Either a 2D grayscale cutoff image (H×W)\n",
    "            or a 3D cutoff voxel stack (H×W×C), in which case only the first\n",
    "            channel is used.\n",
    "        black_tolerance (int): Pixel intensity threshold below which pixels\n",
    "            are considered background.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A binary mask of shape (H, W) where tissue pixels are 1\n",
    "            and background pixels are 0.\n",
    "    \"\"\"\n",
    "    arr = np.asarray(cutvoxel)\n",
    "    if arr.ndim == 3:  # HxWxC\n",
    "        arr = arr[..., 0]\n",
    "    arr = arr.astype(np.uint8)\n",
    "    return (arr > black_tolerance).astype(np.uint8)\n",
    "\n",
    "def _resize_mask_to(img_mask, H, W):\n",
    "    \"\"\"\n",
    "    Resizes an input mask to match specified image dimensions using nearest\n",
    "    neighbor interpolation.\n",
    "\n",
    "    Args:\n",
    "        img_mask (numpy.ndarray): Input mask (H×W or H×W×1) to be resized.\n",
    "        H (int): Target height.\n",
    "        W (int): Target width.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Resized binary mask of shape (H, W) with dtype uint8.\n",
    "    \"\"\"\n",
    "    m = np.asarray(img_mask, np.uint8)\n",
    "    if m.ndim == 3 and m.shape[-1] == 1:\n",
    "        m = m[..., 0]\n",
    "    if m.shape != (H, W):\n",
    "        m = tf.image.resize(\n",
    "            m[..., None], (H, W),\n",
    "            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "        ).numpy().squeeze().astype(np.uint8)\n",
    "    return m\n",
    "\n",
    "# ------------------------------\n",
    "# Pixels for PCA ONLY\n",
    "#   - Normal slides: pixels from tissue region\n",
    "#   - Follicular/Papillary/Anaplastic slides: pixels from tumor region ∩ tissue\n",
    "# ------------------------------\n",
    "def collect_pixels_for_pca_regions(samples, max_pixels_per_image=200000, rng=42, black_tolerance=5, include_anaplastic=False,):\n",
    "    \"\"\"\n",
    "    Collects voxel pixel samples for PCA fitting, selecting only pixels\n",
    "    belonging to relevant tissue or tumor regions.\n",
    "\n",
    "    Args:\n",
    "        samples (list of dict): Sample dictionaries containing:\n",
    "            - 'grayscale_voxel': Voxel image [H, W, C]\n",
    "            - 'grayscale_image_cutoff_voxel': Cutoff voxel [H, W, C] or [H, W]\n",
    "            - 'mask': Tumor mask (optional)\n",
    "            - 'tissue_type': One of {'Normal','Follicular','Papillary',...}\n",
    "        max_pixels_per_image (int): Maximum number of pixels sampled per image.\n",
    "        rng (int): Seed for random subsampling.\n",
    "        black_tolerance (int): Threshold for determining tissue presence.\n",
    "        include_anaplastic (bool): Whether to include anaplastic tumor pixels.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of collected pixels with shape (N_pixels, C),\n",
    "            suitable for PCA training or projection. Returns an empty array\n",
    "            if no valid pixels are found.\n",
    "    \"\"\"\n",
    "    rs = np.random.RandomState(rng)\n",
    "    chunks = []\n",
    "    for d in samples:\n",
    "        voxel = np.asarray(d['grayscale_voxel'], np.float32)  # [H,W,C] (already filtered if policy applied)\n",
    "        cut   = np.asarray(d['grayscale_image_cutoff_voxel'], np.uint8)\n",
    "        H, W, _ = voxel.shape\n",
    "        tissue = _tissue_mask_from_cutoff(cut, black_tolerance)\n",
    "        tt = d.get('tissue_type', '')\n",
    "\n",
    "        if tt == 'Normal':\n",
    "            ys, xs = np.where(tissue > 0)\n",
    "\n",
    "        elif tt in ('Follicular', 'Papillary') or (include_anaplastic and tt == 'Anaplastic'):\n",
    "            tumor = _resize_mask_to(d.get('mask', np.zeros((H, W), np.uint8)), H, W)\n",
    "            ys, xs = np.where((tumor > 0) & (tissue > 0))\n",
    "\n",
    "        else:\n",
    "            continue  # ignore anything else\n",
    "\n",
    "        if ys.size == 0:\n",
    "            continue\n",
    "        cap = min(max_pixels_per_image, ys.size)\n",
    "        idx = rs.choice(ys.size, size=cap, replace=False)\n",
    "        chunks.append(voxel[ys[idx], xs[idx], :])\n",
    "\n",
    "    if not chunks:\n",
    "        return np.empty((0, 0), np.float32)\n",
    "    return np.vstack(chunks).astype(np.float32)\n",
    "\n",
    "# ------------------------------\n",
    "# PCA + multi-scale features\n",
    "# ------------------------------\n",
    "def build_pca_and_scaler(train_X, n_components=2):\n",
    "    \"\"\"\n",
    "    Fits a StandardScaler and PCA model on the given training pixel matrix.\n",
    "\n",
    "    Args:\n",
    "        train_X (numpy.ndarray): Pixel features of shape (N_samples, C).\n",
    "        n_components (int): Number of principal components to retain.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - StandardScaler: Fitted scaler for mean/variance normalization.\n",
    "            - PCA: Fitted PCA model for dimensionality reduction.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    Xs = scaler.fit_transform(train_X)\n",
    "    pca = PCA(n_components=n_components, random_state=0)\n",
    "    pca.fit(Xs)\n",
    "    return scaler, pca\n",
    "\n",
    "def project_image_to_pcs(voxel, scaler, pca):\n",
    "    \"\"\"\n",
    "    Projects a voxel image into PCA space on a per-pixel basis.\n",
    "\n",
    "    Args:\n",
    "        voxel (numpy.ndarray): Voxel image of shape (H, W, C).\n",
    "        scaler (StandardScaler): Fitted scaler for preprocessing.\n",
    "        pca (PCA): Fitted PCA model.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: PCA projection maps of shape (H, W, n_components),\n",
    "            containing the per-pixel PCA feature values.\n",
    "    \"\"\"\n",
    "    H, W, C = voxel.shape\n",
    "    Xs = scaler.transform(voxel.reshape(-1, C))\n",
    "    Z  = pca.transform(Xs).reshape(H, W, -1)   # [H,W,D]\n",
    "    return Z\n",
    "\n",
    "def _grad_mag(img):\n",
    "    \"\"\"\n",
    "    Computes the gradient magnitude of a 2D image using Sobel filters.\n",
    "\n",
    "    Args:\n",
    "        img (numpy.ndarray): 2D image array.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Gradient magnitude map of the same shape as the input.\n",
    "    \"\"\"\n",
    "    gx = sobel(img, axis=0, mode='nearest')\n",
    "    gy = sobel(img, axis=1, mode='nearest')\n",
    "    return np.hypot(gx, gy)\n",
    "\n",
    "def image_features_from_pcs(pc_maps, sigmas=(0.0, 1.0, 2.0)):\n",
    "    \"\"\"\n",
    "    Constructs multi-scale contextual features from PCA projection maps by\n",
    "    applying Gaussian smoothing and gradient magnitude filtering.\n",
    "\n",
    "    Args:\n",
    "        pc_maps (numpy.ndarray): PCA maps of shape (H, W, D).\n",
    "        sigmas (tuple of float): Standard deviations for Gaussian blurring.\n",
    "                                 sigma=0 indicates no blur.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Feature tensor of shape (H, W, F) where F includes:\n",
    "            - original PCA maps\n",
    "            - blurred PCA maps for each sigma\n",
    "            - gradient magnitude maps at each scale\n",
    "    \"\"\"\n",
    "    H, W, D = pc_maps.shape\n",
    "    blurs = [pc_maps] + [\n",
    "        np.stack([gaussian_filter(pc_maps[..., i], s) for i in range(D)], axis=-1)\n",
    "        for s in sigmas if s > 0\n",
    "    ]\n",
    "    feats = blurs[:]\n",
    "    for arr in blurs:\n",
    "        gm = np.stack([_grad_mag(arr[..., i]) for i in range(arr.shape[-1])], axis=-1)\n",
    "        feats.append(gm)\n",
    "    return np.concatenate(feats, axis=-1).astype(np.float32)  # [H,W,F]\n",
    "\n",
    "# ------------------------------\n",
    "# Classifier trained on ALL tumors (modified fit: PCA source only)\n",
    "# ------------------------------\n",
    "class PixelPCAContextClassifierAll:\n",
    "    \"\"\"\n",
    "    Pixel-wise classifier that combines PCA-based dimensionality reduction with\n",
    "    multi-scale contextual features to classify DOCI images into Normal,\n",
    "    Follicular, or Papillary tissue types.\n",
    "\n",
    "    The model:\n",
    "        1. Learns PCA from Normal + Tumor pixels (excluding Anaplastic).\n",
    "        2. Builds multi-scale contextual features from PCA maps.\n",
    "        3. Trains either multinomial logistic regression or a calibrated\n",
    "           LinearSVC classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_pcs=8, sigmas=(0.0,1.0,2.0),\n",
    "                 use_linear_svc=False, calibration='sigmoid',\n",
    "                 C=1.0, max_iter=1000):\n",
    "        self.n_pcs = n_pcs\n",
    "        self.sigmas = tuple(sigmas)\n",
    "        self.use_linear_svc = use_linear_svc\n",
    "        self.calibration = calibration\n",
    "        self.C = C\n",
    "        self.max_iter = max_iter\n",
    "        self.scaler_ = None\n",
    "        self.pca_ = None\n",
    "        self.clf_ = None\n",
    "        self.full_order_ids_ = np.array([CLASS_TO_ID3[c] for c in TISSUES3])\n",
    "        self.present_ids_ = None\n",
    "\n",
    "    def fit(self, train_samples, max_pixels_per_image=200000, class_weight='balanced'):\n",
    "        \"\"\"\n",
    "        Fits the full PCA + multi-scale feature + classifier pipeline using\n",
    "        training samples containing Normal, Follicular, and Papillary tissue.\n",
    "\n",
    "        Args:\n",
    "            train_samples (list of dict): Sample dictionaries with voxel data,\n",
    "                cutoff masks, tumor masks, and tissue labels.\n",
    "            max_pixels_per_image (int): Pixel cap per image for training.\n",
    "            class_weight (str or dict): Class weighting strategy for the\n",
    "                underlying classifier ('balanced' recommended).\n",
    "\n",
    "        Returns:\n",
    "            PixelPCAContextClassifierAll: The fitted model instance.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If insufficient pixels are collected for PCA.\n",
    "        \"\"\"\n",
    "        # (A) PCA/scaler: train WITHOUT anaplastic\n",
    "        X_raw_pca = collect_pixels_for_pca_regions(\n",
    "            train_samples,\n",
    "            max_pixels_per_image=max_pixels_per_image,\n",
    "            rng=123,\n",
    "            black_tolerance=5,\n",
    "            include_anaplastic=False\n",
    "        )\n",
    "        if X_raw_pca.size == 0:\n",
    "            raise RuntimeError(\"No pixels collected for PCA.\")\n",
    "        self.scaler_, self.pca_ = build_pca_and_scaler(X_raw_pca, n_components=self.n_pcs)\n",
    "\n",
    "        # (B) Build contextual TRAIN set for classifier\n",
    "        feats_list, y_list = [], []\n",
    "        rs = np.random.RandomState(123)\n",
    "        for d in train_samples:\n",
    "            voxel  = np.asarray(d['grayscale_voxel'], np.float32)\n",
    "            cutvox = np.asarray(d['grayscale_image_cutoff_voxel'], np.uint8)\n",
    "            H, W, _ = voxel.shape\n",
    "            tissue = _tissue_mask_from_cutoff(cutvox)\n",
    "\n",
    "            tt = d.get('tissue_type', '')\n",
    "            if tt == 'Normal':\n",
    "                labels = np.full((H, W), CLASS_TO_ID3['Normal'], np.uint8)\n",
    "                labels[tissue == 0] = 255\n",
    "            elif tt in ('Follicular', 'Papillary'):\n",
    "                tumor = _resize_mask_to(d.get('mask', np.zeros((H, W), np.uint8)), H, W)\n",
    "                labels = np.full((H, W), CLASS_TO_ID3['Normal'], np.uint8)\n",
    "                labels[(tumor > 0) & (tissue > 0)] = CLASS_TO_ID3[tt]\n",
    "                labels[tissue == 0] = 255\n",
    "            else:\n",
    "                continue  # Anaplastic excluded from classifier labels\n",
    "\n",
    "            pcs = project_image_to_pcs(voxel, self.scaler_, self.pca_)      # [H,W,D]\n",
    "            F   = image_features_from_pcs(pcs, sigmas=self.sigmas)          # [H,W,F]\n",
    "            ys, xs = np.where(labels != 255)\n",
    "            if ys.size == 0:\n",
    "                continue\n",
    "            cap = min(30000, ys.size)\n",
    "            idx = rs.choice(ys.size, size=cap, replace=False)\n",
    "            feats_list.append(F[ys[idx], xs[idx], :])\n",
    "            y_list.append(labels[ys[idx], xs[idx]])\n",
    "\n",
    "        X_feat = np.vstack(feats_list)\n",
    "        y_feat = np.concatenate(y_list)\n",
    "\n",
    "        # (C) multinomial LR (or LinearSVC + calibration)\n",
    "        if not self.use_linear_svc:\n",
    "            base = LogisticRegression(\n",
    "                multi_class='multinomial',\n",
    "                solver='lbfgs',\n",
    "                C=self.C,\n",
    "                max_iter=self.max_iter,\n",
    "                class_weight=class_weight\n",
    "            )\n",
    "            self.clf_ = base.fit(X_feat, y_feat)\n",
    "            self.present_ids_ = self.clf_.classes_.astype(int)\n",
    "        else:\n",
    "            base = LinearSVC(C=self.C, max_iter=self.max_iter, class_weight=class_weight)\n",
    "            self.clf_ = CalibratedClassifierCV(base_estimator=base, cv=4, method=self.calibration)\n",
    "            self.clf_.fit(X_feat, y_feat)\n",
    "            self.present_ids_ = self.clf_.classes_.astype(int)\n",
    "        return self\n",
    "\n",
    "    def _expand_to_full(self, proba_small):\n",
    "        \"\"\"\n",
    "        Expands classifier output probabilities to include all tissue classes\n",
    "        (Normal, Follicular, Papillary) even when some classes were absent\n",
    "        during training.\n",
    "\n",
    "        Args:\n",
    "            proba_small (numpy.ndarray): Probability matrix for present classes\n",
    "                of shape (N, K').\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Expanded probability matrix of shape (N, 3),\n",
    "                aligned with CLASS_TO_ID3 ordering.\n",
    "        \"\"\"\n",
    "        N = proba_small.shape[0]\n",
    "        proba_full = np.zeros((N, len(self.full_order_ids_)), dtype=np.float32)\n",
    "        for j, cls_id in enumerate(self.present_ids_):\n",
    "            proba_full[:, cls_id] = proba_small[:, j]\n",
    "        return proba_full\n",
    "\n",
    "    def predict_proba_map(self, sample):\n",
    "        \"\"\"\n",
    "        Computes per-pixel class probabilities for a voxelized sample using\n",
    "        the PCA projection + feature extraction + trained classifier.\n",
    "\n",
    "        Args:\n",
    "            sample (dict): Sample dictionary containing:\n",
    "                - 'grayscale_voxel': Voxel image [H, W, C]\n",
    "                - 'tissue_type': Optional label (not required for inference)\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Probability tensor of shape (H, W, 3) containing\n",
    "                per-pixel probabilities for Normal, Follicular, and Papillary.\n",
    "        \"\"\"\n",
    "        voxel = np.asarray(sample['grayscale_voxel'], np.float32)  # already filtered channels\n",
    "        pcs   = project_image_to_pcs(voxel, self.scaler_, self.pca_)\n",
    "        F     = image_features_from_pcs(pcs, sigmas=self.sigmas)\n",
    "        H, W, Fdim = F.shape\n",
    "        proba_small = self.clf_.predict_proba(F.reshape(-1, Fdim))     # [H*W,K']\n",
    "        P = self._expand_to_full(proba_small).reshape(H, W, -1)        # [H,W,3] (N,F,P)\n",
    "        return P\n",
    "\n",
    "# ------------------------------\n",
    "# Fit PCA + classifier on TRAIN (all tumors)\n",
    "# ------------------------------\n",
    "px_model_all = PixelPCAContextClassifierAll(\n",
    "    n_pcs=8, sigmas=(0.0,1.0,2.0),\n",
    "    use_linear_svc=False, calibration='sigmoid',\n",
    "    C=1.0, max_iter=1000\n",
    ")\n",
    "print(\"[PX] Fitting PCA+Classifier on TRAIN with Normal + Follicular + Papillary ...\")\n",
    "px_model_all.fit(train_combined, max_pixels_per_image=200000, class_weight='balanced')\n",
    "print(\"[PX] ... done.\")\n",
    "\n",
    "# (optional) save\n",
    "with open(\"pixel_pca_context_classifier_all.pkl\", \"wb\") as f:\n",
    "    pickle.dump(px_model_all, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53dd31-d17e-4663-b4af-646ed57cfdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Tunable regional categorization (categorize-first, then filter)\n",
    "# =========================\n",
    "\n",
    "VALID_CLASSES3 = ['Normal', 'Follicular', 'Papillary']\n",
    "\n",
    "def _pred_label_map_over_tissue(px_model_all, d, black_tol=5):\n",
    "    \"\"\"\n",
    "    Generates pixel-wise predicted class labels, tissue mask, and probability maps\n",
    "    for a voxelized DOCI sample.\n",
    "\n",
    "    Args:\n",
    "        px_model_all (PixelPCAContextClassifierAll): Trained pixel-wise classifier\n",
    "            used to generate per-pixel class probabilities.\n",
    "        d (dict): Sample dictionary containing:\n",
    "            - 'grayscale_voxel': Voxel image [H, W, C]\n",
    "            - 'grayscale_image_cutoff_voxel': Cutoff voxel [H, W, C] or [H, W]\n",
    "        black_tol (int): Threshold for constructing the tissue mask from the cutoff image.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - numpy.ndarray: Per-pixel class labels of shape (H, W), where each entry is\n",
    "              one of {'Normal', 'Follicular', 'Papillary'}.\n",
    "            - numpy.ndarray: Binary tissue mask of shape (H, W) with values {0, 1}.\n",
    "            - numpy.ndarray: Probability map of shape (H, W, 3), in order VALID_CLASSES3.\n",
    "    \"\"\"\n",
    "    P = px_model_all.predict_proba_map(d).astype(np.float32)  # [H,W,3]\n",
    "    cut = np.asarray(d['grayscale_image_cutoff_voxel'], np.uint8)\n",
    "    tissue = _tissue_mask_from_cutoff(cut, black_tolerance=black_tol)\n",
    "    if tissue.sum() == 0:\n",
    "        tissue = np.ones_like(tissue, np.uint8)\n",
    "\n",
    "    lbl_idx = np.argmax(P, axis=-1)  # [H,W] ints 0..2\n",
    "    labels_map = np.empty(lbl_idx.shape, dtype=object)\n",
    "    for k, name in enumerate(TISSUES3):\n",
    "        labels_map[lbl_idx == k] = name\n",
    "    return labels_map, tissue, P\n",
    "\n",
    "def _sliding_window_indices(H, W, win=64, stride=32):\n",
    "    \"\"\"\n",
    "    Computes top-left coordinates for sliding windows across an image,\n",
    "    ensuring full coverage including edge-aligned windows.\n",
    "\n",
    "    Args:\n",
    "        H (int): Image height.\n",
    "        W (int): Image width.\n",
    "        win (int): Window size (square window of size win × win).\n",
    "        stride (int): Step size between adjacent window positions.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - list of int: Row indices (y) for window start positions.\n",
    "            - list of int: Column indices (x) for window start positions.\n",
    "    \"\"\"\n",
    "    ys = list(range(0, max(1, H - win + 1), stride))\n",
    "    xs = list(range(0, max(1, W - win + 1), stride))\n",
    "    if len(ys) == 0:\n",
    "        ys = [0]\n",
    "    if len(xs) == 0:\n",
    "        xs = [0]\n",
    "    if ys[-1] != max(0, H - win):\n",
    "        ys.append(max(0, H - win))\n",
    "    if xs[-1] != max(0, W - win):\n",
    "        xs.append(max(0, W - win))\n",
    "    return ys, xs\n",
    "\n",
    "def categorize_sample_regional(px_model_all, d, black_tol=5, win=64, stride=32, frac_thresh=0.60, min_tissue_px_per_win=50):\n",
    "    \"\"\"\n",
    "    Performs regional sample-level tumor classification based on pixel-wise\n",
    "    predictions aggregated over sliding windows.\n",
    "\n",
    "    Regional decision rule:\n",
    "        1) Compute per-pixel class labels and restrict to tissue pixels.\n",
    "        2) Slide a window across the image.\n",
    "        3) In each window, if ≥ frac_thresh of tissue pixels are a cancer class,\n",
    "           mark that cancer as present.\n",
    "        4) If multiple cancers are present, choose the one with greater total\n",
    "           tissue occupancy across the full image.\n",
    "        5) If no cancer meets the threshold, classify the sample as Normal.\n",
    "\n",
    "    Args:\n",
    "        px_model_all (PixelPCAContextClassifierAll): Trained pixel-wise classifier.\n",
    "        d (dict): Sample dictionary with voxel, cutoff, and optional mask fields.\n",
    "        black_tol (int): Tissue mask threshold.\n",
    "        win (int): Sliding window size.\n",
    "        stride (int): Sliding window stride.\n",
    "        frac_thresh (float): Minimum fraction of tissue pixels in a window that must\n",
    "            be cancer for that class to be flagged.\n",
    "        min_tissue_px_per_win (int): Minimum number of tissue pixels required for a\n",
    "            window to contribute to regional decisions.\n",
    "\n",
    "    Returns:\n",
    "        str: Sample-level predicted class in {'Normal','Follicular','Papillary'}.\n",
    "    \"\"\"\n",
    "    labels_map, tissue, _ = _pred_label_map_over_tissue(px_model_all, d, black_tol=black_tol)\n",
    "    H, W = tissue.shape\n",
    "\n",
    "    tissue_idx = tissue.astype(bool)\n",
    "    if tissue_idx.sum() == 0:\n",
    "        return 'Normal'\n",
    "\n",
    "    overall_counts = {\n",
    "        'Follicular': np.sum((labels_map == 'Follicular') & tissue_idx),\n",
    "        'Papillary':  np.sum((labels_map == 'Papillary')  & tissue_idx)\n",
    "    }\n",
    "\n",
    "    present_cancers = set()\n",
    "    ys, xs = _sliding_window_indices(H, W, win=win, stride=stride)\n",
    "\n",
    "    for y0 in ys:\n",
    "        for x0 in xs:\n",
    "            y1, x1 = y0 + win, x0 + win\n",
    "            sub_tissue = tissue[y0:y1, x0:x1].astype(bool)\n",
    "            tp = int(sub_tissue.sum())\n",
    "            if tp < min_tissue_px_per_win:\n",
    "                continue\n",
    "\n",
    "            sub_lbl = labels_map[y0:y1, x0:x1]\n",
    "            f_cnt = int(np.sum((sub_lbl == 'Follicular') & sub_tissue))\n",
    "            p_cnt = int(np.sum((sub_lbl == 'Papillary')  & sub_tissue))\n",
    "            f_frac = f_cnt / tp\n",
    "            p_frac = p_cnt / tp\n",
    "\n",
    "            if f_frac >= frac_thresh:\n",
    "                present_cancers.add('Follicular')\n",
    "            if p_frac >= frac_thresh:\n",
    "                present_cancers.add('Papillary')\n",
    "\n",
    "    if len(present_cancers) == 0:\n",
    "        return 'Normal'\n",
    "    if len(present_cancers) == 1:\n",
    "        return next(iter(present_cancers))\n",
    "\n",
    "    f_total = overall_counts['Follicular']\n",
    "    p_total = overall_counts['Papillary']\n",
    "    return 'Follicular' if f_total >= p_total else 'Papillary'\n",
    "\n",
    "# ------------------------------\n",
    "# Categorize FIRST, then filter out Follicular-dominant\n",
    "# ------------------------------\n",
    "def categorize_split(samples, px_model_all, black_tol=5, win=64, stride=32, frac_thresh=0.60, min_tissue_px_per_win=50):\n",
    "    \"\"\"\n",
    "    Applies regional tumor categorization to a dataset split and produces a\n",
    "    summarized results list for downstream evaluation.\n",
    "\n",
    "    Args:\n",
    "        samples (list of dict): Sample dictionaries, each with voxel and metadata.\n",
    "        px_model_all (PixelPCAContextClassifierAll): Trained pixel-wise classifier.\n",
    "        black_tol (int): Tissue mask threshold.\n",
    "        win (int): Sliding window size.\n",
    "        stride (int): Window stride.\n",
    "        frac_thresh (float): Window-level class occupancy threshold.\n",
    "        min_tissue_px_per_win (int): Minimum window tissue requirement.\n",
    "\n",
    "    Returns:\n",
    "        list of dict: Each dictionary contains:\n",
    "            - 'name'  (str): Sample identifier.\n",
    "            - 'gt'    (str): Ground-truth label ('Normal','Follicular','Papillary').\n",
    "            - 'pred'  (str): Regional predicted class.\n",
    "            - 'sample' (dict): Original sample dictionary.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for d in samples:\n",
    "        pred = categorize_sample_regional(\n",
    "            px_model_all, d,\n",
    "            black_tol=black_tol,\n",
    "            win=win, stride=stride,\n",
    "            frac_thresh=frac_thresh,\n",
    "            min_tissue_px_per_win=min_tissue_px_per_win\n",
    "        )\n",
    "        results.append({\n",
    "            'name': d.get('name', 'unknown_sample'),\n",
    "            'gt': d.get('tissue_type', None),\n",
    "            'pred': pred,\n",
    "            'sample': d\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def filter_after_categorization(cat_results):\n",
    "    \"\"\"\n",
    "    Filters categorized samples by removing those predicted to belong to the\n",
    "    non-target tumor class (NONTARGET_TUMOR).\n",
    "\n",
    "    Args:\n",
    "        cat_results (list of dict): Output entries from categorize_split(), each\n",
    "            containing predicted and ground-truth labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - list of dict: Samples retained after filtering.\n",
    "            - list of dict: Samples removed because predicted == NONTARGET_TUMOR.\n",
    "    \"\"\"\n",
    "    kept, dropped = [], []\n",
    "    for r in cat_results:\n",
    "        if r['pred'] == NONTARGET_TUMOR:\n",
    "            dropped.append(r['sample'])\n",
    "        else:\n",
    "            kept.append(r['sample'])\n",
    "    return kept, dropped\n",
    "\n",
    "# ---- RUN: categorize first, then filter ----\n",
    "REGIONAL_PARAMS = dict(black_tol=5, win=80, stride=10,\n",
    "                       frac_thresh=0.80, min_tissue_px_per_win=35)\n",
    "\n",
    "train_cat = categorize_split(train_combined, px_model_all, **REGIONAL_PARAMS)\n",
    "val_cat   = categorize_split(val_combined,   px_model_all, **REGIONAL_PARAMS)\n",
    "test_cat  = categorize_split(test_combined,  px_model_all, **REGIONAL_PARAMS)\n",
    "\n",
    "train_filtered, train_dropped = filter_after_categorization(train_cat)\n",
    "val_filtered,   val_dropped   = filter_after_categorization(val_cat)\n",
    "test_filtered,  test_dropped  = filter_after_categorization(test_cat)\n",
    "\n",
    "print(f\"[CATEGORIZE] counts (pred): \"\n",
    "      f\"train N/F/P = \"\n",
    "      f\"{sum(r['pred']=='Normal' for r in train_cat)}/\"\n",
    "      f\"{sum(r['pred']=='Follicular' for r in train_cat)}/\"\n",
    "      f\"{sum(r['pred']=='Papillary' for r in train_cat)}\")\n",
    "print(f\"[FILTER] kept: train={len(train_filtered)} val={len(val_filtered)} test={len(test_filtered)}\")\n",
    "print(f\"[FILTER] drop(Follicular-pred): train={len(train_dropped)} val={len(val_dropped)} test={len(test_dropped)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Confusion matrices & verbose misclassifications\n",
    "# ------------------------------\n",
    "def confusion_from_cat(cat_results, split_name):\n",
    "    \"\"\"\n",
    "    Computes and displays a confusion matrix for regional sample classification\n",
    "    on a given dataset split, and prints misclassification details.\n",
    "\n",
    "    Args:\n",
    "        cat_results (list of dict): Categorization results from categorize_split().\n",
    "        split_name (str): Name of the dataset split (e.g., 'TRAIN', 'VAL', 'TEST').\n",
    "\n",
    "    Returns:\n",
    "        None: Displays a seaborn heatmap, prints misclassified samples, and\n",
    "        lists Anaplastic→Normal cases (if present).\n",
    "    \"\"\"\n",
    "    y_true = [r['gt'] for r in cat_results if r['gt'] in VALID_CLASSES3]\n",
    "    y_pred = [r['pred'] for r in cat_results if r['gt'] in VALID_CLASSES3]\n",
    "    names  = [r['name'] for r in cat_results if r['gt'] in VALID_CLASSES3]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=VALID_CLASSES3)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=VALID_CLASSES3, yticklabels=VALID_CLASSES3)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Ground Truth\")\n",
    "    plt.title(f\"{split_name} Confusion Matrix (Regional categorization)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    mis = [(n, gt, pr) for n, gt, pr in zip(names, y_true, y_pred) if gt != pr]\n",
    "    anaplastic_as_normal = [r['name'] for r in cat_results\n",
    "                            if r['gt'] == 'Anaplastic' and r['pred'] == 'Normal']\n",
    "\n",
    "    print(f\"\\n[DETAILS] {split_name}\")\n",
    "    if mis:\n",
    "        print(f\"Misclassified samples ({len(mis)}):\")\n",
    "        for n, gt, pr in mis:\n",
    "            print(f\"  - {n}: GT={gt}, Pred={pr}\")\n",
    "    else:\n",
    "        print(\"No misclassifications in this split.\")\n",
    "\n",
    "    if anaplastic_as_normal:\n",
    "        print(f\"\\nAnaplastic → Normal cases ({len(anaplastic_as_normal)}):\")\n",
    "        for n in anaplastic_as_normal:\n",
    "            print(f\"  - {n}\")\n",
    "    print()\n",
    "\n",
    "# Pre-filter confusion matrices (to inspect gate performance)\n",
    "confusion_from_cat(train_cat, \"TRAIN (pre-filter)\")\n",
    "confusion_from_cat(val_cat,   \"VAL (pre-filter)\")\n",
    "confusion_from_cat(test_cat,  \"TEST (pre-filter)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88209b8a-2b56-4a07-b7ed-b41f6665f75d",
   "metadata": {},
   "source": [
    "# Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7da436-d347-4a76-976b-5f5380088413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Patient-level bootstrapped CIs for regional categorization\n",
    "# ------------------------------\n",
    "def _patient_id_from_name(name):\n",
    "    \"\"\"\n",
    "    Extracts the patient-level ID from a sample name.\n",
    "\n",
    "    Assumes sample names follow the format 'PatientID_SectionID',\n",
    "    e.g., 'SSW-23-12345_A1' → 'SSW-23-12345'.\n",
    "\n",
    "    Args:\n",
    "        name (str): Full sample name.\n",
    "\n",
    "    Returns:\n",
    "        str: Patient-level identifier derived from the sample name.\n",
    "    \"\"\"\n",
    "    s = str(name)\n",
    "    parts = s.split(\"_\")\n",
    "    if len(parts) >= 2:\n",
    "        return \"_\".join(parts[:2])\n",
    "    return s\n",
    "\n",
    "\n",
    "def _extract_arrays_for_bootstrap(cat_results):\n",
    "    \"\"\"\n",
    "    Extracts arrays of ground truth labels, predicted labels, and patient IDs\n",
    "    from categorized results for use in bootstrapped evaluation.\n",
    "\n",
    "    Args:\n",
    "        cat_results (list of dict): Output of `categorize_split`, where each entry\n",
    "            contains keys: 'gt', 'pred', and 'name'.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - numpy.ndarray: Ground truth labels (strings).\n",
    "            - numpy.ndarray: Predicted labels (strings).\n",
    "            - numpy.ndarray: Patient IDs derived from sample names (strings).\n",
    "    \"\"\"\n",
    "    y_true, y_pred, pids = [], [], []\n",
    "    for r in cat_results:\n",
    "        gt = r.get(\"gt\", None)\n",
    "        if gt not in VALID_CLASSES3:\n",
    "            continue\n",
    "        y_true.append(gt)\n",
    "        y_pred.append(r.get(\"pred\", None))\n",
    "        pids.append(_patient_id_from_name(r.get(\"name\", \"unknown_sample\")))\n",
    "    return np.array(y_true, dtype=object), np.array(y_pred, dtype=object), np.array(pids, dtype=object)\n",
    "\n",
    "\n",
    "def _compute_metrics(y_true, y_pred, classes=VALID_CLASSES3):\n",
    "    \"\"\"\n",
    "    Computes overall accuracy and per-class recall.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Ground truth labels.\n",
    "        y_pred (array-like): Predicted labels.\n",
    "        classes (list of str): Valid class names to compute recall for.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - float: Overall accuracy.\n",
    "            - dict: Per-class recall values as a dictionary.\n",
    "                    Recall is defined as TP / (TP + FN).\n",
    "                    Returns NaN for classes with no ground truth instances.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=object)\n",
    "    y_pred = np.asarray(y_pred, dtype=object)\n",
    "\n",
    "    if y_true.size == 0:\n",
    "        return np.nan, {c: np.nan for c in classes}\n",
    "\n",
    "    acc = np.mean(y_true == y_pred)\n",
    "\n",
    "    recalls = {}\n",
    "    for c in classes:\n",
    "        mask = (y_true == c)\n",
    "        if mask.sum() == 0:\n",
    "            recalls[c] = np.nan  # avoid division by zero\n",
    "        else:\n",
    "            tp = np.sum((y_pred == c) & mask)\n",
    "            fn = np.sum((y_pred != c) & mask)\n",
    "            recalls[c] = tp / (tp + fn + 1e-6)\n",
    "    return float(acc), recalls\n",
    "\n",
    "\n",
    "def bootstrap_regional_metrics(cat_results, n_boot=1000, random_state=0):\n",
    "    \"\"\"\n",
    "    Performs patient-level bootstrap to estimate confidence intervals\n",
    "    for regional categorization accuracy and per-class recall.\n",
    "\n",
    "    Patient IDs are derived from sample names of the form\n",
    "    'SSW-16-01636_A3', where the patient-level ID is everything\n",
    "    before the first underscore (e.g., 'SSW-16-01636').\n",
    "\n",
    "    Args:\n",
    "        cat_results (list of dict): Regional categorization results, typically\n",
    "            produced by categorize_split(), containing keys:\n",
    "            - 'name' (str): Sample name used to derive patient ID.\n",
    "            - 'gt'   (str): Ground-truth label in VALID_CLASSES3.\n",
    "            - 'pred' (str): Predicted label from regional classification.\n",
    "        n_boot (int): Number of bootstrap resamples to draw at the\n",
    "            patient level.\n",
    "        random_state (int): Seed for the random number generator to\n",
    "            ensure reproducible bootstrap sampling.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary summarizing bootstrap results with keys:\n",
    "            - 'unique_patient_ids' (list of str): List of distinct patient IDs.\n",
    "            - 'accuracy' (dict):\n",
    "                - 'point' (float): Point estimate of overall accuracy.\n",
    "                - 'ci_95' (tuple): (lower, upper) 95% CI for accuracy.\n",
    "            - 'recall' (dict): For each class in VALID_CLASSES3:\n",
    "                - 'point' (float): Point estimate of recall.\n",
    "                - 'ci_95' (tuple): (lower, upper) 95% CI for recall.\n",
    "            - 'n_patients' (int): Number of unique patients.\n",
    "            - 'n_boot' (int): Number of bootstrap iterations used.\n",
    "\n",
    "        The function also prints:\n",
    "            - The unique patient IDs and their count.\n",
    "            - The 95% CI for accuracy and per-class recall.\n",
    "    \"\"\"\n",
    "    y_true, y_pred, pids = _extract_arrays_for_bootstrap(cat_results)\n",
    "    classes = VALID_CLASSES3\n",
    "\n",
    "    # ---- NEW: Convert tile-level IDs into patient-level IDs ----\n",
    "    patient_ids = np.array([pid.split(\"_\")[0] for pid in pids])\n",
    "    unique_patients = np.unique(patient_ids)\n",
    "    n_patients = len(unique_patients)\n",
    "\n",
    "    # Print unique patient IDs\n",
    "    print(\"\\n[BOOT] Unique patient-level IDs:\")\n",
    "    for pid in unique_patients:\n",
    "        print(\" \", pid)\n",
    "    print(f\"[BOOT] Total unique patients: {n_patients}\\n\")\n",
    "\n",
    "    if n_patients == 0:\n",
    "        print(\"[BOOT] No patients found for bootstrap.\")\n",
    "        return {}\n",
    "\n",
    "    # Point estimates\n",
    "    acc_point, rec_point = _compute_metrics(y_true, y_pred, classes=classes)\n",
    "\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    acc_boot = []\n",
    "    rec_boot = {c: [] for c in classes}\n",
    "\n",
    "    # ---- Index map: patient → indices of all tiles for that patient ----\n",
    "    pid_to_idx = {pid: np.where(patient_ids == pid)[0] for pid in unique_patients}\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        # sample patients with replacement\n",
    "        sampled = rng.choice(unique_patients, size=n_patients, replace=True)\n",
    "\n",
    "        # gather all tile indices from these patients\n",
    "        idx_all = np.concatenate([pid_to_idx[pid] for pid in sampled], axis=0)\n",
    "\n",
    "        y_true_b = y_true[idx_all]\n",
    "        y_pred_b = y_pred[idx_all]\n",
    "\n",
    "        acc_b, rec_b = _compute_metrics(y_true_b, y_pred_b, classes=classes)\n",
    "        acc_boot.append(acc_b)\n",
    "\n",
    "        for c in classes:\n",
    "            if not np.isnan(rec_b[c]):\n",
    "                rec_boot[c].append(rec_b[c])\n",
    "\n",
    "    acc_boot = np.array(acc_boot, float)\n",
    "    rec_boot = {c: np.array(v, float) for c, v in rec_boot.items()}\n",
    "\n",
    "    def _ci(arr):\n",
    "        arr = arr[~np.isnan(arr)]\n",
    "        if arr.size == 0:\n",
    "            return (np.nan, np.nan)\n",
    "        return (float(np.percentile(arr, 2.5)),\n",
    "                float(np.percentile(arr, 97.5)))\n",
    "\n",
    "    results = {\n",
    "        \"unique_patient_ids\": unique_patients.tolist(),   # NEW\n",
    "        \"accuracy\": {\n",
    "            \"point\": float(acc_point),\n",
    "            \"ci_95\": _ci(acc_boot),\n",
    "        },\n",
    "        \"recall\": {},\n",
    "        \"n_patients\": int(n_patients),\n",
    "        \"n_boot\": int(n_boot),\n",
    "    }\n",
    "\n",
    "    for c in classes:\n",
    "        results[\"recall\"][c] = {\n",
    "            \"point\": float(rec_point[c]) if not np.isnan(rec_point[c]) else np.nan,\n",
    "            \"ci_95\": _ci(rec_boot[c]),\n",
    "        }\n",
    "\n",
    "    # Pretty print\n",
    "    print(f\"[BOOT] Patient-level bootstrap (n_patients={n_patients}, n_boot={n_boot})\")\n",
    "    lo, hi = results[\"accuracy\"][\"ci_95\"]\n",
    "    print(f\"  Accuracy: {results['accuracy']['point']:.3f} \"\n",
    "          f\"(95% CI {lo:.3f}–{hi:.3f})\")\n",
    "\n",
    "    for c in classes:\n",
    "        pt = results[\"recall\"][c][\"point\"]\n",
    "        lo, hi = results[\"recall\"][c][\"ci_95\"]\n",
    "        print(f\"  Recall {c:10s}: {pt:.3f} (95% CI {lo:.3f}–{hi:.3f})\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n[BOOTSTRAP] TRAIN split (regional categorization)\")\n",
    "train_boot = bootstrap_regional_metrics(train_cat, n_boot=2000, random_state=0)\n",
    "\n",
    "print(\"\\n[BOOTSTRAP] VAL split (regional categorization)\")\n",
    "val_boot = bootstrap_regional_metrics(val_cat, n_boot=2000, random_state=1)\n",
    "\n",
    "print(\"\\n[BOOTSTRAP] TEST split (regional categorization)\")\n",
    "test_boot = bootstrap_regional_metrics(test_cat, n_boot=2000, random_state=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b088e42-e57b-4759-a5d8-469e74ae0a8c",
   "metadata": {},
   "source": [
    "# Visualizing Pixel Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3a12e-491d-4d95-8629-37de5fafee0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------ visualize per-image probability maps ------------\n",
    "def visualize_px_maps_combined(samples, model, max_imgs=20, save_path=\"px_maps_combined2.png\"):\n",
    "    \"\"\"\n",
    "    Generates a single combined figure showing pixel-wise probability maps for\n",
    "    multiple DOCI samples, using the output of a trained\n",
    "    PixelPCAContextClassifierAll model.\n",
    "\n",
    "    The figure arranges:\n",
    "        - One row per sample (up to `max_imgs` samples)\n",
    "        - Three columns per row, corresponding to:\n",
    "            * P(Normal)\n",
    "            * P(Follicular)\n",
    "            * P(Papillary)\n",
    "      Each subplot displays the predicted probability map for its class.\n",
    "\n",
    "    Args:\n",
    "        samples (list of dict): A list of sample dictionaries, each containing\n",
    "            'grayscale_voxel' and associated metadata required by\n",
    "            `model.predict_proba_map()`.\n",
    "        model (PixelPCAContextClassifierAll): A trained pixel-wise classifier\n",
    "            capable of generating per-pixel class probability maps.\n",
    "        max_imgs (int, optional): Maximum number of samples to visualize.\n",
    "            If fewer samples are provided, all will be used. Defaults to 20.\n",
    "        save_path (str, optional): File path where the final combined figure\n",
    "            will be saved. Defaults to \"px_maps_combined2.png\".\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the combined figure and saves it to disk. Prints the\n",
    "        save path upon completion.\n",
    "    \"\"\"\n",
    "    k = min(max_imgs, len(samples))\n",
    "\n",
    "    # 3 columns: Normal, Follicular, Papillary\n",
    "    fig, axes = plt.subplots(k, 3, figsize=(12, 4 * k))\n",
    "\n",
    "    # Ensure axes is always 2D\n",
    "    if k == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "    for row in range(k):\n",
    "        ex = samples[row]\n",
    "        P  = model.predict_proba_map(ex)  # [H,W,3]\n",
    "\n",
    "        # Extract individual probability maps\n",
    "        pN = P[..., CLASS_TO_ID3['Normal']]\n",
    "        pF = P[..., CLASS_TO_ID3['Follicular']]\n",
    "        pP = P[..., CLASS_TO_ID3['Papillary']]\n",
    "\n",
    "        # Row plots\n",
    "        maps = [pN, pF, pP]\n",
    "        titles = [\"P(Normal)\", \"P(Follicular)\", \"P(Papillary)\"]\n",
    "\n",
    "        for col in range(3):\n",
    "            ax = axes[row, col]\n",
    "            ax.imshow(maps[col], vmin=0, vmax=1)\n",
    "            ax.set_title(titles[col])\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"[PX] Saved combined figure: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3983c191-0789-4c90-92e7-6e5686854f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_px_maps_combined(train_filtered, model=px_model_all, max_imgs=20, save_prefix=\"pixel_prob_maps_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e35ae03-0d58-4203-acec-f77ac4e3fa28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_px_maps_combined(test_filtered, model=px_model_all, max_imgs=20, save_prefix=\"pixel_prob_maps_val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead53c15-2df6-41ce-bf0d-be6f7f2c5600",
   "metadata": {},
   "source": [
    "# Visualizing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67316aa-4e8f-4199-b08c-41060ea1e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# PCA diagnostics (PC1–PC2 scatter, variance, loadings)\n",
    "# Reuses the PCA/scaler already trained in px_model_all\n",
    "# ===========================================\n",
    "\n",
    "def _boundary_mask(bin_mask):\n",
    "    \"\"\"\n",
    "    Computes the boundary pixels of a binary mask using morphological\n",
    "    dilation–erosion differencing.\n",
    "\n",
    "    Args:\n",
    "        bin_mask (numpy.ndarray): Binary mask of shape (H, W) or (H, W, 1).\n",
    "            Nonzero values indicate the region of interest.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A binary mask of shape (H, W) where boundary pixels\n",
    "        (i.e., mask edges) are marked with 1 and non-boundary pixels with 0.\n",
    "    \"\"\"\n",
    "    m = np.asarray(bin_mask)\n",
    "    if m.ndim == 3 and m.shape[-1] == 1:\n",
    "        m = m[..., 0]\n",
    "    m = m.astype(bool)\n",
    "    if m.size == 0:\n",
    "        return np.zeros_like(m, dtype=np.uint8)\n",
    "    dil = binary_dilation(m, iterations=1)\n",
    "    ero = binary_erosion(m, iterations=1)\n",
    "    return np.logical_xor(dil, ero).astype(np.uint8)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Collect pixels for Normal + Follicular + Papillary\n",
    "# (balanced; boundary-emphasized for tumors)\n",
    "# ---------------------------------------------------------------------\n",
    "def collect_pixel_samples_all_tumors_balanced_boundary(samples, max_pixels_per_image=20000, per_class_cap=150000, rng=42, black_tolerance=5, boundary_boost=0.5):\n",
    "    \"\"\"\n",
    "    Collects pixel samples for PCA diagnostics using class-balanced sampling\n",
    "    with additional emphasis on tumor–normal boundaries.\n",
    "\n",
    "    Sampling policy:\n",
    "        • Normal slides → Uniform sampling of tissue pixels.\n",
    "        • Tumor slides → Separate sampling of:\n",
    "            - Tumor boundary pixels (boosted by `boundary_boost`)\n",
    "            - Tumor interior pixels\n",
    "            - Same-slide normal tissue pixels\n",
    "        • Final class balance enforced via `per_class_cap`.\n",
    "\n",
    "    Args:\n",
    "        samples (list of dict): Sample dictionaries containing voxel data,\n",
    "            cutoff images, and optional tumor masks.\n",
    "        max_pixels_per_image (int): Max pixels to draw per slide.\n",
    "        per_class_cap (int): Maximum number of pixels retained per class\n",
    "            after balancing.\n",
    "        rng (int): Random seed for reproducibility.\n",
    "        black_tolerance (int): Tissue mask threshold.\n",
    "        boundary_boost (float): Fraction (0–1) of the tumor sampling\n",
    "            dedicated to boundary pixels.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - numpy.ndarray: Pixel matrix X_raw of shape (N, C), suitable for PCA.\n",
    "            - numpy.ndarray: Class labels y of shape (N,), encoded according to CLASS_TO_ID3.\n",
    "    \"\"\"\n",
    "    rs = np.random.RandomState(rng)\n",
    "    per_class = {c: [] for c in TISSUES3}\n",
    "\n",
    "    # infer channel count\n",
    "    C_channels = None\n",
    "    for d0 in samples:\n",
    "        if 'grayscale_voxel' in d0:\n",
    "            C_channels = int(np.asarray(d0['grayscale_voxel']).shape[-1])\n",
    "            break\n",
    "    if C_channels is None:\n",
    "        return np.empty((0, 0), np.float32), np.empty((0,), np.int32)\n",
    "\n",
    "    for d in samples:\n",
    "        voxel = np.asarray(d['grayscale_voxel'], np.float32)  # [H,W,C]\n",
    "        cut   = np.asarray(d['grayscale_image_cutoff_voxel'], np.uint8)\n",
    "        H, W, _ = voxel.shape\n",
    "        tissue = _tissue_mask_from_cutoff(cut, black_tolerance)\n",
    "        tt = d.get('tissue_type', '')\n",
    "\n",
    "        # Normal slides: sample tissue pixels\n",
    "        if tt == 'Normal':\n",
    "            ys, xs = np.where(tissue > 0)\n",
    "            if ys.size == 0:\n",
    "                continue\n",
    "            cap = min(max_pixels_per_image, ys.size)\n",
    "            idx = rs.choice(ys.size, size=cap, replace=False)\n",
    "            per_class['Normal'].append(voxel[ys[idx], xs[idx], :])\n",
    "            continue\n",
    "\n",
    "        # Tumor slides: Follicular / Papillary\n",
    "        if tt in ('Follicular', 'Papillary'):\n",
    "            tumor = np.asarray(d.get('mask', np.zeros((H, W), np.uint8)), np.uint8)\n",
    "            if tumor.ndim == 3 and tumor.shape[-1] == 1:\n",
    "                tumor = tumor[..., 0]\n",
    "            if tumor.shape != (H, W):\n",
    "                tumor = tf.image.resize(\n",
    "                    tumor[..., None], (H, W),\n",
    "                    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "                ).numpy().squeeze().astype(np.uint8)\n",
    "\n",
    "            y_t, x_t = np.where((tumor > 0) & (tissue > 0))   # tumor pixels\n",
    "            y_n, x_n = np.where((tumor == 0) & (tissue > 0))  # same-slide normal pixels\n",
    "\n",
    "            bnd = _boundary_mask(tumor)\n",
    "            half_cap = max_pixels_per_image // 2\n",
    "            cap_b  = int(max(0, min(1.0, boundary_boost)) * half_cap)\n",
    "            cap_nt = max(0, half_cap - cap_b)\n",
    "\n",
    "            def sub(ys, xs, k):\n",
    "                if ys.size <= 0 or k <= 0:\n",
    "                    return np.empty((0,), int), np.empty((0,), int)\n",
    "                k = min(k, ys.size)\n",
    "                idx = rs.choice(ys.size, size=k, replace=False)\n",
    "                return ys[idx], xs[idx]\n",
    "\n",
    "            if y_t.size:\n",
    "                tumor_is_bnd = (bnd[y_t, x_t] > 0)\n",
    "                y_tb,  x_tb  = y_t[tumor_is_bnd],  x_t[tumor_is_bnd]\n",
    "                y_tnb, x_tnb = y_t[~tumor_is_bnd], x_t[~tumor_is_bnd]\n",
    "            else:\n",
    "                y_tb = x_tb = y_tnb = x_tnb = np.empty((0,), int)\n",
    "\n",
    "            y_tb,  x_tb  = sub(y_tb,  x_tb,  cap_b)\n",
    "            y_tnb, x_tnb = sub(y_tnb, x_tnb, cap_nt)\n",
    "            y_n,   x_n   = sub(y_n,   x_n,   half_cap)\n",
    "\n",
    "            if y_tb.size + y_tnb.size:\n",
    "                y_all = np.concatenate([y_tb, y_tnb])\n",
    "                x_all = np.concatenate([x_tb, x_tnb])\n",
    "                per_class[tt].append(voxel[y_all, x_all, :])\n",
    "            if y_n.size:\n",
    "                per_class['Normal'].append(voxel[y_n, x_n, :])\n",
    "\n",
    "    # stack & balance classes\n",
    "    for k in per_class:\n",
    "        per_class[k] = (\n",
    "            np.vstack(per_class[k]).astype(np.float32)\n",
    "            if len(per_class[k]) else\n",
    "            np.empty((0, C_channels), np.float32)\n",
    "        )\n",
    "\n",
    "    sizes = {k: per_class[k].shape[0] for k in TISSUES3}\n",
    "    nonzero = [v for v in sizes.values() if v > 0]\n",
    "    if not nonzero:\n",
    "        return np.empty((0, C_channels), np.float32), np.empty((0,), np.int32)\n",
    "    target_count = min(min(nonzero), per_class_cap)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    for cls_name in TISSUES3:\n",
    "        Xc = per_class[cls_name]\n",
    "        if Xc.shape[0] == 0:\n",
    "            continue\n",
    "        if Xc.shape[0] > target_count:\n",
    "            idx = np.random.RandomState(rng).choice(Xc.shape[0], size=target_count, replace=False)\n",
    "            Xc = Xc[idx]\n",
    "        X_list.append(Xc)\n",
    "        y_list.append(np.full((Xc.shape[0],), CLASS_TO_ID3[cls_name], np.int32))\n",
    "\n",
    "    X_raw = np.vstack(X_list)\n",
    "    y     = np.concatenate(y_list)\n",
    "    return X_raw, y\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Build pixel dataset for PCA diagnostics\n",
    "# ---------------------------------------------------------------------\n",
    "def get_pixel_dataset_for_pca(train_samples, max_pixels_per_image=12000, per_class_cap=150000, rng=123):\n",
    "    \"\"\"\n",
    "    Builds a balanced, boundary-aware pixel dataset from training samples\n",
    "    for use in PCA diagnostics.\n",
    "\n",
    "    Args:\n",
    "        train_samples (list of dict): Training sample dictionaries containing\n",
    "            voxel images and metadata.\n",
    "        max_pixels_per_image (int): Maximum pixels to sample per image.\n",
    "        per_class_cap (int): Limit for per-class balancing.\n",
    "        rng (int): Random seed.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - numpy.ndarray: Pixel matrix X_raw of shape (N, C).\n",
    "            - numpy.ndarray: Integer class labels y of shape (N,).\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If no pixels are collected (e.g., missing masks or images).\n",
    "    \"\"\"\n",
    "    X_raw, y = collect_pixel_samples_all_tumors_balanced_boundary(\n",
    "        train_samples,\n",
    "        max_pixels_per_image=max_pixels_per_image,\n",
    "        per_class_cap=per_class_cap,\n",
    "        rng=rng\n",
    "    )\n",
    "    if X_raw.size == 0:\n",
    "        raise RuntimeError(\"No pixel samples collected for PCA diagnostics.\")\n",
    "    return X_raw, y\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Reuse trained PCA / scaler from px_model_all\n",
    "# ---------------------------------------------------------------------\n",
    "if getattr(px_model_all, \"scaler_\", None) is None or getattr(px_model_all, \"pca_\", None) is None:\n",
    "    raise RuntimeError(\"px_model_all must be trained first (scaler_/pca_ not found).\")\n",
    "\n",
    "scaler_used = px_model_all.scaler_\n",
    "pca_used    = px_model_all.pca_\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2D PCA scatter (PC1 vs PC2)\n",
    "# ---------------------------------------------------------------------\n",
    "def plot_pca_scatter_2d(X_raw, y, scaler, pca, sample_cap=200000, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Generates a 2D PCA scatter plot (PC1 vs PC2) using a subset of pixels,\n",
    "    colored by tissue class.\n",
    "\n",
    "    Args:\n",
    "        X_raw (numpy.ndarray): Pixel feature matrix (N, C).\n",
    "        y (numpy.ndarray): Class labels (N,), encoded as integers:\n",
    "            0=Normal, 1=Follicular, 2=Papillary.\n",
    "        scaler (StandardScaler): Fitted scaler used to normalize X_raw.\n",
    "        pca (PCA): Fitted PCA model.\n",
    "        sample_cap (int or None): If provided, randomly subsample up to this\n",
    "            many pixels for display.\n",
    "        alpha (float): Transparency for scatter points.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the scatter plot.\n",
    "    \"\"\"\n",
    "    if sample_cap is not None and X_raw.shape[0] > sample_cap:\n",
    "        rs = np.random.RandomState(0)\n",
    "        idx = rs.choice(X_raw.shape[0], size=sample_cap, replace=False)\n",
    "        X_plot = X_raw[idx]\n",
    "        y_plot = y[idx]\n",
    "    else:\n",
    "        X_plot = X_raw\n",
    "        y_plot = y\n",
    "\n",
    "    Xs = scaler.transform(X_plot)\n",
    "    Z  = pca.transform(Xs)\n",
    "    pc1, pc2 = Z[:, 0], Z[:, 1]\n",
    "\n",
    "    label_names = ['Normal', 'Follicular', 'Papillary']\n",
    "    colors = {0: 'tab:blue', 1: 'tab:orange', 2: 'tab:green'}\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    for lab in range(3):\n",
    "        m = (y_plot == lab)\n",
    "        if np.any(m):\n",
    "            plt.scatter(pc1[m], pc2[m], s=5, alpha=alpha,\n",
    "                        label=label_names[lab], c=colors[lab])\n",
    "    plt.axhline(0, lw=0.5, c='k', alpha=0.2)\n",
    "    plt.axvline(0, lw=0.5, c='k', alpha=0.2)\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title('PCA scatter (PC1 vs PC2)')\n",
    "    plt.legend(markerscale=3, frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Explained variance (bar + cumulative line)\n",
    "# ---------------------------------------------------------------------\n",
    "def plot_pca_explained_variance(pca, max_components=8):\n",
    "    \"\"\"\n",
    "    Plots the explained variance ratio and cumulative variance for the first\n",
    "    principal components of a trained PCA model.\n",
    "\n",
    "    Args:\n",
    "        pca (PCA): Trained PCA model with loaded explained variance information.\n",
    "        max_components (int): Maximum number of PCs to display.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays a bar+line plot of variance contributions.\n",
    "    \"\"\"\n",
    "    r = np.asarray(pca.explained_variance_ratio_)\n",
    "    k = min(max_components, r.shape[0])\n",
    "    x = np.arange(1, k + 1)\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.bar(x, r[:k], label='Explained variance ratio')\n",
    "    cum = np.cumsum(r[:k])\n",
    "    plt.plot(x, cum, marker='o', label='Cumulative')\n",
    "    plt.xticks(x)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Variance Ratio')\n",
    "    plt.title(f'Explained Variance (first {k} PCs)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Loadings plots (stacked per channel; abs values; prints top-K)\n",
    "# ---------------------------------------------------------------------\n",
    "def plot_pca_loadings_stacked(pca, feature_names=None, max_components=8, weight_by_variance=True, top_k=10, start_index=1):\n",
    "    \"\"\"\n",
    "    Visualizes PCA loadings across channels using stacked bar plots and lists\n",
    "    the most influential channels based on loading magnitude.\n",
    "\n",
    "    Args:\n",
    "        pca (PCA): Trained PCA model (components_ and explained variance loaded).\n",
    "        feature_names (list of str or None): Labels for each channel; if None,\n",
    "            channels are labeled 'ch<index>'.\n",
    "        max_components (int): Number of leading PCs to include in stacked bars.\n",
    "        weight_by_variance (bool): If True, multiply loadings by explained variance\n",
    "            to better reflect component contributions.\n",
    "        top_k (int): Number of top-importance channels to list and plot.\n",
    "        start_index (int): Starting index for auto-generated feature names.\n",
    "\n",
    "    Returns:\n",
    "        dict: Diagnostic information containing:\n",
    "            - 'importance': Per-channel importance scores\n",
    "            - 'order_desc': Channels sorted by importance (descending)\n",
    "            - 'top_idx': Indices of top-k channels\n",
    "            - 'stack_vals': Stacked loading matrix for plotted PCs\n",
    "            - 'evr': Explained variance ratios\n",
    "    \"\"\"\n",
    "    comps = np.asarray(pca.components_)               # (n_components, n_features)\n",
    "    evr   = np.asarray(pca.explained_variance_ratio_) # (n_components,)\n",
    "    n_comp, n_feat = comps.shape\n",
    "    k = min(max_components, n_comp)\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"ch{start_index + j}\" for j in range(n_feat)]\n",
    "\n",
    "    vals = np.abs(comps[:k, :])\n",
    "    if weight_by_variance:\n",
    "        vals = (vals.T * evr[:k]).T\n",
    "\n",
    "    importance = vals.sum(axis=0)\n",
    "\n",
    "    # Full stacked bar plot\n",
    "    x = np.arange(n_feat)\n",
    "    plt.figure(figsize=(max(10, 0.5 * n_feat), 5))\n",
    "    bottom = np.zeros(n_feat, dtype=float)\n",
    "    for pc_idx in range(k):\n",
    "        plt.bar(x, vals[pc_idx, :], bottom=bottom, label=f\"PC{pc_idx + 1}\")\n",
    "        bottom += vals[pc_idx, :]\n",
    "    plt.xticks(x, feature_names, rotation=90)\n",
    "    plt.ylabel(\"Stacked |loading|\" + (\" × EVR\" if weight_by_variance else \"\"))\n",
    "    plt.title(f\"PCA Loadings (first {k} PCs) — stacked per channel\")\n",
    "    plt.legend(ncol=min(4, k), fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Top-K channels by importance\n",
    "    order = np.argsort(importance)[::-1]\n",
    "    top_idx = order[:top_k]\n",
    "    print(f\"\\nTop {top_k} channels by importance \"\n",
    "          f\"({'|loading|×EVR' if weight_by_variance else '|loading|'} over first {k} PCs):\")\n",
    "    for rnk, i in enumerate(top_idx, 1):\n",
    "        print(f\"{rnk:2d}. {feature_names[i]}  —  score={importance[i]:.6f}\")\n",
    "\n",
    "    plt.figure(figsize=(max(8, 0.7 * top_k), 4))\n",
    "    plt.bar(np.arange(top_k), importance[top_idx])\n",
    "    plt.xticks(np.arange(top_k), [feature_names[i] for i in top_idx],\n",
    "               rotation=45, ha='right')\n",
    "    plt.ylabel(\"Importance (sum of |loading|\" +\n",
    "               (\" × EVR\" if weight_by_variance else \"\") + \")\")\n",
    "    plt.title(f\"Top {top_k} Channels by PCA Loading Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Stacked breakdown for top-K channels\n",
    "    plt.figure(figsize=(max(8, 0.7 * top_k), 4.5))\n",
    "    bottom = np.zeros(top_k, dtype=float)\n",
    "    for pc_idx in range(k):\n",
    "        plt.bar(np.arange(top_k), vals[pc_idx, top_idx],\n",
    "                bottom=bottom, label=f\"PC{pc_idx + 1}\")\n",
    "        bottom += vals[pc_idx, top_idx]\n",
    "    plt.xticks(np.arange(top_k), [feature_names[i] for i in top_idx],\n",
    "               rotation=45, ha='right')\n",
    "    plt.ylabel(\"Stacked |loading|\" + (\" × EVR\" if weight_by_variance else \"\"))\n",
    "    plt.title(f\"Top {top_k}: Stacked Contribution by PC\")\n",
    "    plt.legend(ncol=min(4, k), fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"importance\": importance,\n",
    "        \"order_desc\": order,\n",
    "        \"top_idx\": top_idx,\n",
    "        \"stack_vals\": vals,\n",
    "        \"evr\": evr,\n",
    "    }\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Per-sample PCA averages and plotting (one point per slide)\n",
    "# ---------------------------------------------------------------------\n",
    "def compute_sample_pca_averages(samples, scaler, pca, black_tolerance=5, split_tags=None):\n",
    "    \"\"\"\n",
    "    Computes per-sample mean PCA feature vectors by averaging PCA projections\n",
    "    over tissue pixels.\n",
    "\n",
    "    Args:\n",
    "        samples (list of dict): Samples with voxel data and cutoff images.\n",
    "        scaler (StandardScaler): Fitted scaler for voxel normalization.\n",
    "        pca (PCA): Trained PCA model.\n",
    "        black_tolerance (int): Cutoff threshold for tissue masking.\n",
    "        split_tags (list of str or None): Optional split labels\n",
    "            ('train', 'val', 'test', or 'unknown') for each sample.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - numpy.ndarray: Z_avg, shape (N, D), mean PCA embedding per sample.\n",
    "            - list of str: Sample names.\n",
    "            - list of str: Ground-truth tissue labels.\n",
    "            - list of str: Split tags corresponding to each sample.\n",
    "    \"\"\"\n",
    "    if split_tags is None:\n",
    "        split_tags = ['unknown'] * len(samples)\n",
    "\n",
    "    Z_list, names, labels, splits = [], [], [], []\n",
    "    for d, sp in zip(samples, split_tags):\n",
    "        voxel  = np.asarray(d['grayscale_voxel'], np.float32)            # [H,W,C]\n",
    "        cutvox = np.asarray(d['grayscale_image_cutoff_voxel'], np.uint8) # [H,W,C]\n",
    "        pcs    = project_image_to_pcs(voxel, scaler, pca)                # [H,W,D]\n",
    "        tissue = (cutvox > black_tolerance).any(axis=-1).astype(np.uint8)\n",
    "\n",
    "        m = tissue > 0\n",
    "        if np.any(m):\n",
    "            Z_mean = pcs[m].mean(axis=0)\n",
    "        else:\n",
    "            Z_mean = pcs.reshape(-1, pcs.shape[-1]).mean(axis=0)\n",
    "\n",
    "        Z_list.append(Z_mean.astype(np.float32))\n",
    "        names.append(d.get('name', 'unknown'))\n",
    "        labels.append(d.get('tissue_type', 'unknown'))\n",
    "        splits.append(sp)\n",
    "\n",
    "    return np.vstack(Z_list), names, labels, splits\n",
    "\n",
    "def plot_sample_pca_averages(Z_avg, sample_labels, split_tags, alpha=0.9):\n",
    "    \"\"\"\n",
    "    Plots per-sample PCA means in PC1–PC2 space, using:\n",
    "        - Color to indicate tissue type\n",
    "        - Marker style to indicate dataset split (train/val/test)\n",
    "\n",
    "    Args:\n",
    "        Z_avg (numpy.ndarray): Mean PCA vectors for each sample (N, 2 or more PCs).\n",
    "        sample_labels (list of str): Ground-truth tissue types.\n",
    "        split_tags (list of str): Split identifiers: 'train', 'val', 'test', or 'unknown'.\n",
    "        alpha (float): Marker transparency.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays a scatter plot with class- and split-specific markers.\n",
    "    \"\"\"\n",
    "    color_map  = {'Normal':'tab:blue', 'Follicular':'tab:orange',\n",
    "                  'Papillary':'tab:green', 'Anaplastic':'tab:red'}\n",
    "    marker_map = {'train':'o', 'val':'s', 'test':'^', 'unknown':'x'}\n",
    "    order_lbl  = ['Normal', 'Follicular', 'Papillary', 'Anaplastic']\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    for lbl in sorted(set(sample_labels),\n",
    "                      key=lambda x: order_lbl.index(x) if x in order_lbl else 999):\n",
    "        for sp in ['train', 'val', 'test', 'unknown']:\n",
    "            idx = [i for i, (L, S) in enumerate(zip(sample_labels, split_tags))\n",
    "                   if L == lbl and S == sp]\n",
    "            if not idx:\n",
    "                continue\n",
    "            pts = Z_avg[idx]\n",
    "            plt.scatter(pts[:, 0], pts[:, 1],\n",
    "                        s=40, alpha=alpha,\n",
    "                        c=color_map.get(lbl, 'gray'),\n",
    "                        marker=marker_map.get(sp, 'x'),\n",
    "                        label=f\"{lbl} — {sp}\")\n",
    "\n",
    "    plt.axhline(0, lw=0.5, c='k', alpha=0.2)\n",
    "    plt.axvline(0, lw=0.5, c='k', alpha=0.2)\n",
    "    plt.xlabel('PC1 (sample mean)')\n",
    "    plt.ylabel('PC2 (sample mean)')\n",
    "    plt.title('Per-sample PCA (mean over tissue pixels) — split-marked')\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    uniq = dict(zip(labels, handles))\n",
    "    plt.legend(uniq.values(), uniq.keys(), frameon=True, fontsize=9, ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Example usage — BEFORE any filtering\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Pixel dataset from TRAIN\n",
    "X_raw, y = get_pixel_dataset_for_pca(\n",
    "    train_combined,\n",
    "    max_pixels_per_image=12000,\n",
    "    per_class_cap=150000,\n",
    "    rng=123\n",
    ")\n",
    "\n",
    "# 2) Pixel-level PCA diagnostics\n",
    "plot_pca_scatter_2d(X_raw, y, scaler_used, pca_used,\n",
    "                    sample_cap=200000, alpha=0.25)\n",
    "plot_pca_explained_variance(pca_used, max_components=8)\n",
    "\n",
    "feature_names = None  # auto-label as ch1..chN\n",
    "_ = plot_pca_loadings_stacked(\n",
    "    pca_used,\n",
    "    feature_names=feature_names,\n",
    "    max_components=8,\n",
    "    weight_by_variance=True,\n",
    "    top_k=10,\n",
    "    start_index=1\n",
    ")\n",
    "\n",
    "# 3) Per-sample PCA means across train/val/test\n",
    "samples_for_plot = []\n",
    "split_tags = []\n",
    "for d in train_combined:\n",
    "    samples_for_plot.append(d); split_tags.append('train')\n",
    "for d in val_combined:\n",
    "    samples_for_plot.append(d); split_tags.append('val')\n",
    "for d in test_combined:\n",
    "    samples_for_plot.append(d); split_tags.append('test')\n",
    "\n",
    "Z_avg, sample_names, sample_labels, splits = compute_sample_pca_averages(\n",
    "    samples_for_plot, scaler_used, pca_used, black_tolerance=5, split_tags=split_tags\n",
    ")\n",
    "\n",
    "plot_sample_pca_averages(Z_avg, sample_labels, splits, alpha=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf548a-7815-4d05-89c2-9bead986253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Save regional categorization results to Excel\n",
    "# -----------------------------------------\n",
    "rows = []\n",
    "\n",
    "for split_name, cat_list in [\n",
    "    (\"train\", train_cat),\n",
    "    (\"val\",   val_cat),\n",
    "    (\"test\",  test_cat),\n",
    "]:\n",
    "    for r in cat_list:\n",
    "        rows.append({\n",
    "            \"split\":        split_name,\n",
    "            \"name\":         r.get(\"name\", \"unknown_sample\"),\n",
    "            \"ground_truth\": r.get(\"gt\", None),\n",
    "            \"predicted\":    r.get(\"pred\", None),\n",
    "        })\n",
    "\n",
    "df_cat = pd.DataFrame(rows)\n",
    "excel_path = \"regional_categorization_results.xlsx\"\n",
    "df_cat.to_excel(excel_path, index=False)\n",
    "print(f\"[SAVE] Wrote regional categorization results to {excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b104042-6648-4256-910f-f1e4463009e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
