{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd09e21-ad5d-4f2d-9018-293acee6ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library ---\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from scipy.ndimage import (\n",
    "    gaussian_filter,\n",
    "    sobel,\n",
    ")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc5dc1-7b5b-43df-88af-78917f365b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Clean Imports (deduplicated and organized)\n",
    "# ===========================================\n",
    "\n",
    "# --------------------\n",
    "# Paths & source\n",
    "# --------------------\n",
    "BASE_PATH       = \"data\"\n",
    "FILENAME        = \"image_dicts_256_wgrayscale_andcutoffs.pkl\"\n",
    "FILE_PATH       = os.path.join(BASE_PATH, FILENAME)\n",
    "EXCEL_FILE_PATH = os.path.join(BASE_PATH, \"sample_groups.xlsx\")\n",
    "URL             = \"https://github.com/tylervasse/DOCI-Prediction/releases/download/v1.0/image_dicts_256_wgrayscale_andcutoffs.pkl\"\n",
    "\n",
    "# --------------------\n",
    "# IO helpers\n",
    "# --------------------\n",
    "def download_file(url, output_path):\n",
    "    \"\"\"Download file if missing.\"\"\"\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"File already exists at {output_path}\")\n",
    "        return\n",
    "    print(f\"Downloading to {output_path}...\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def load_image_dicts(file_path):\n",
    "    \"\"\"Load list of image dictionaries from pickle.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_sample_groups(excel_file_path):\n",
    "    \"\"\"\n",
    "    Excel must have columns: 'Train Samples', 'Validation Samples', 'Test Samples'.\n",
    "    Returns three lists of sample base names (strings).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(excel_file_path)\n",
    "        norm = lambda col: [s.strip().strip(\"'\") for s in df[col].dropna().tolist()]\n",
    "        return norm('Train Samples'), norm('Validation Samples'), norm('Test Samples')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Sample groups file not found at {excel_file_path}\")\n",
    "        return [], [], []\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel: {e}\")\n",
    "        return [], [], []\n",
    "\n",
    "# --------------------\n",
    "# Basic parsing utils\n",
    "# --------------------\n",
    "def get_base_name(name):\n",
    "    \"\"\"Sample base name = everything before '_DOCI_n'.\"\"\"\n",
    "    return name.split('_DOCI')[0]\n",
    "\n",
    "def get_doci_number(name):\n",
    "    \"\"\"Extract integer n from '_DOCI_n' (or -1 if absent).\"\"\"\n",
    "    m = re.search(r'_DOCI_(\\d+)', name)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "# --------------------\n",
    "# Split image dicts into splits by sample base name\n",
    "# --------------------\n",
    "def categorize_images(image_data, train_samples, val_samples, test_samples):\n",
    "    \"\"\"\n",
    "    Split image dicts into train/val/test by base sample name.\n",
    "    \"\"\"\n",
    "    train_set, val_set, test_set = [], [], []\n",
    "    for d in image_data:\n",
    "        base = \"_\".join(d['name'].split('_')[:2])  # e.g., 'SSW-23-12345_A1'\n",
    "        if base in train_samples:\n",
    "            train_set.append(d)\n",
    "        elif base in val_samples:\n",
    "            val_set.append(d)\n",
    "        elif base in test_samples:\n",
    "            test_set.append(d)\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "# --------------------\n",
    "# Voxelize per-sample (group by base name, sort by DOCI)\n",
    "# --------------------\n",
    "def samples_to_voxels(dataset):\n",
    "    \"\"\"\n",
    "    Group images by base sample, sort by DOCI index, and build:\n",
    "      - grayscale_voxel:  [H, W, D]\n",
    "      - grayscale_image_cutoff_voxel: [H, W, D]\n",
    "      - mask: first available mask kept as-is\n",
    "    Expected keys in each item: 'name', 'grayscale', 'image_grayscale_cutoff', 'mask', 'tissue_type'\n",
    "    \"\"\"\n",
    "    grouped = defaultdict(lambda: {\n",
    "        'names': [], 'grayscale': [], 'image_grayscale_cutoff': [], 'mask': None, 'tissue_type': None\n",
    "    })\n",
    "\n",
    "    for d in dataset:\n",
    "        base = get_base_name(d['name'])\n",
    "        grouped[base]['names'].append(d['name'])\n",
    "        grouped[base]['grayscale'].append(d['grayscale'])\n",
    "        grouped[base]['image_grayscale_cutoff'].append(d['image_grayscale_cutoff'])\n",
    "        grouped[base]['tissue_type'] = d['tissue_type']\n",
    "        if grouped[base]['mask'] is None and d.get('mask') is not None:\n",
    "            grouped[base]['mask'] = d['mask']\n",
    "\n",
    "    voxelized = []\n",
    "    for base, g in grouped.items():\n",
    "        order = sorted(range(len(g['names'])), key=lambda i: get_doci_number(g['names'][i]))\n",
    "        gray     = [g['grayscale'][i] for i in order]\n",
    "        gray_cut = [g['image_grayscale_cutoff'][i] for i in order]\n",
    "        grayscale_voxel                 = np.stack(gray, axis=-1).astype(np.float32)     # [H,W,D]\n",
    "        grayscale_image_cutoff_voxel    = np.stack(gray_cut, axis=-1).astype(np.uint8)   # [H,W,D]\n",
    "\n",
    "        voxelized.append({\n",
    "            'name': base,\n",
    "            'grayscale_voxel': grayscale_voxel,\n",
    "            'grayscale_image_cutoff_voxel': grayscale_image_cutoff_voxel,\n",
    "            'tissue_type': g['tissue_type'],\n",
    "            'mask': g['mask']\n",
    "        })\n",
    "    return voxelized\n",
    "\n",
    "# ====================\n",
    "# Main flow\n",
    "# ====================\n",
    "# 1) Ensure data file\n",
    "download_file(URL, FILE_PATH)\n",
    "\n",
    "# 2) Load raw dicts\n",
    "image_dicts = load_image_dicts(FILE_PATH)\n",
    "\n",
    "# 3) Exclude specific samples by substring match in 'name'\n",
    "EXCLUDE_LIST = [\"SSW-23-14395_C2\", \"SSW-23-05363_A7\"]\n",
    "image_dicts = [d for d in image_dicts if not any(excl in d['name'] for excl in EXCLUDE_LIST)]\n",
    "\n",
    "# 4) Load sample groups from Excel\n",
    "train_samples, val_samples, test_samples = load_sample_groups(EXCEL_FILE_PATH)\n",
    "\n",
    "# 5) Assign to splits and shuffle at image level\n",
    "train_set, val_set, test_set = categorize_images(image_dicts, train_samples, val_samples, test_samples)\n",
    "train_set = shuffle(train_set, random_state=42)\n",
    "val_set   = shuffle(val_set,   random_state=42)\n",
    "test_set  = shuffle(test_set,  random_state=42)\n",
    "\n",
    "# 6) Voxelize per sample\n",
    "train_combined = samples_to_voxels(train_set)\n",
    "val_combined   = samples_to_voxels(val_set)\n",
    "test_combined  = samples_to_voxels(test_set)\n",
    "\n",
    "print(f\"Samples -> train: {len(train_combined)} | val: {len(val_combined)} | test: {len(test_combined)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73059f33-f4a7-4c68-9303-512eb00c71c1",
   "metadata": {},
   "source": [
    "# Regional Categorization from the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e1887-a063-4405-a145-bd1df17b9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Train PCA/Classifier on ALL tumor types, then\n",
    "# filter out Follicular-dominant samples from\n",
    "# TRAIN / VAL / TEST (based on PCA-feature model)\n",
    "# ===========================================\n",
    "\n",
    "# --- constants ---\n",
    "TISSUES3 = ['Normal', 'Follicular', 'Papillary']\n",
    "CLASS_TO_ID3 = {c: i for i, c in enumerate(TISSUES3)}\n",
    "TARGET_TUMOR = \"Follicular\"\n",
    "NONTARGET_TUMOR = {\"Papillary\": \"Follicular\",\n",
    "                   \"Follicular\": \"Papillary\"}.get(TARGET_TUMOR)\n",
    "TARGET_ID = CLASS_TO_ID3[TARGET_TUMOR]\n",
    "\n",
    "# ---- Channel selection (0-based indices) ----\n",
    "# Define channels to REMOVE by index (1-based here for readability), then convert to 0-based indices.\n",
    "REMOVE_VOXEL_CHANNELS = [1, 2, 4, 7, 9, 11, 12, 14, 16, 17, 19]\n",
    "REMOVE_VOXEL_CHANNELS = [i - 1 for i in REMOVE_VOXEL_CHANNELS]\n",
    "\n",
    "# Optionally, explicitly define channels to KEEP (overrides REMOVE_* if not None)\n",
    "KEEP_VOXEL_CHANNELS = None        # e.g., [2,3,4,5,6]\n",
    "\n",
    "# ------------------------------\n",
    "# Channel policy for PCA pipeline\n",
    "# ------------------------------\n",
    "def _sanitize_indices_pca(C, keep=None, remove=None):\n",
    "    \"\"\"\n",
    "    Compute channel indices to keep given KEEP_VOXEL_CHANNELS / REMOVE_VOXEL_CHANNELS.\n",
    "    Use either keep OR remove (or neither), but not both.\n",
    "    \"\"\"\n",
    "    if keep not in (None, []) and remove not in (None, []):\n",
    "        raise ValueError(\"Use either KEEP_VOXEL_CHANNELS or REMOVE_VOXEL_CHANNELS, not both.\")\n",
    "    if keep not in (None, []):\n",
    "        idx = sorted({int(i) for i in keep if 0 <= int(i) < C})\n",
    "    elif remove not in (None, []):\n",
    "        bad = {int(i) for i in remove if 0 <= int(i) < C}\n",
    "        idx = [i for i in range(C) if i not in bad]\n",
    "    else:\n",
    "        idx = list(range(C))\n",
    "    if not idx:\n",
    "        raise ValueError(\"No channels left after applying channel policy.\")\n",
    "    return idx\n",
    "\n",
    "def _apply_channel_filter_to_samples_pca(sample_list, keep_idx):\n",
    "    \"\"\"\n",
    "    In-place: slice d['grayscale_voxel'] to keep only channels in keep_idx\n",
    "    for every sample in sample_list.\n",
    "    \"\"\"\n",
    "    if sample_list is None:\n",
    "        return None\n",
    "    for d in sample_list:\n",
    "        if 'grayscale_voxel' not in d:\n",
    "            continue\n",
    "        v = np.asarray(d['grayscale_voxel'], np.float32)\n",
    "        if v.ndim != 3:\n",
    "            raise ValueError(f\"Expected voxel shape [H,W,C], got {v.shape}\")\n",
    "        if max(keep_idx) >= v.shape[-1]:\n",
    "            raise ValueError(\n",
    "                f\"keep_idx {keep_idx} incompatible with voxel shape {v.shape}\"\n",
    "            )\n",
    "        d['grayscale_voxel'] = v[..., keep_idx]\n",
    "    return sample_list\n",
    "\n",
    "# Apply channel policy to each set *before* PCA/scaler training\n",
    "try:\n",
    "    # Infer original channel count from train_combined\n",
    "    C0 = None\n",
    "    for d0 in train_combined:\n",
    "        if 'grayscale_voxel' in d0:\n",
    "            C0 = np.asarray(d0['grayscale_voxel']).shape[-1]\n",
    "            break\n",
    "    if C0 is None:\n",
    "        raise RuntimeError(\"Could not infer voxel channel count from train_combined.\")\n",
    "\n",
    "    keep_idx_pca = _sanitize_indices_pca(C0, KEEP_VOXEL_CHANNELS, REMOVE_VOXEL_CHANNELS)\n",
    "    print(f\"[PCA] Applying voxel channel policy: {C0} -> {len(keep_idx_pca)} using {[i+1 for i in keep_idx_pca]}\")\n",
    "\n",
    "    # Apply to all relevant sample sets in-place\n",
    "    train_combined = _apply_channel_filter_to_samples_pca(train_combined, keep_idx_pca)\n",
    "    try:\n",
    "        val_combined = _apply_channel_filter_to_samples_pca(val_combined, keep_idx_pca)\n",
    "    except NameError:\n",
    "        pass\n",
    "    try:\n",
    "        test_combined = _apply_channel_filter_to_samples_pca(test_combined, keep_idx_pca)\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "except NameError:\n",
    "    # If KEEP_VOXEL_CHANNELS / REMOVE_VOXEL_CHANNELS or train_combined not defined yet\n",
    "    print(\"[PCA] Channel policy not found or sample sets undefined here; using all voxel channels for PCA.\")\n",
    "\n",
    "# ------------------------------\n",
    "# Tissue mask from cutoff\n",
    "# ------------------------------\n",
    "def _tissue_mask_from_cutoff(cutvoxel, black_tolerance=5):\n",
    "    \"\"\"\n",
    "    v1-style tissue mask: use a single grayscale cutoff plane.\n",
    "    If cutvoxel is HxW, use it directly.\n",
    "    If cutvoxel is HxWxC, use the FIRST channel.\n",
    "    \"\"\"\n",
    "    arr = np.asarray(cutvoxel)\n",
    "    if arr.ndim == 3:  # HxWxC\n",
    "        arr = arr[..., 0]\n",
    "    arr = arr.astype(np.uint8)\n",
    "    return (arr > black_tolerance).astype(np.uint8)\n",
    "\n",
    "def _resize_mask_to(img_mask, H, W):\n",
    "    m = np.asarray(img_mask, np.uint8)\n",
    "    if m.ndim == 3 and m.shape[-1] == 1:\n",
    "        m = m[..., 0]\n",
    "    if m.shape != (H, W):\n",
    "        m = tf.image.resize(\n",
    "            m[..., None], (H, W),\n",
    "            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "        ).numpy().squeeze().astype(np.uint8)\n",
    "    return m\n",
    "\n",
    "# ------------------------------\n",
    "# Pixels for PCA ONLY\n",
    "#   - Normal slides: pixels from tissue region\n",
    "#   - Follicular/Papillary/Anaplastic slides: pixels from tumor region ∩ tissue\n",
    "# ------------------------------\n",
    "def collect_pixels_for_pca_regions(\n",
    "    samples,\n",
    "    max_pixels_per_image=200000,\n",
    "    rng=42,\n",
    "    black_tolerance=5,\n",
    "    include_anaplastic=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Return X_raw (N_pix, C) for PCA fitting or projection:\n",
    "      - Normal: pixels where tissue==1\n",
    "      - Follicular/Papillary: pixels where tumor==1 AND tissue==1\n",
    "      - Anaplastic: ONLY included if include_anaplastic=True\n",
    "    \"\"\"\n",
    "    rs = np.random.RandomState(rng)\n",
    "    chunks = []\n",
    "    for d in samples:\n",
    "        voxel = np.asarray(d['grayscale_voxel'], np.float32)  # [H,W,C] (already filtered if policy applied)\n",
    "        cut   = np.asarray(d['grayscale_image_cutoff_voxel'], np.uint8)\n",
    "        H, W, _ = voxel.shape\n",
    "        tissue = _tissue_mask_from_cutoff(cut, black_tolerance)\n",
    "        tt = d.get('tissue_type', '')\n",
    "\n",
    "        if tt == 'Normal':\n",
    "            ys, xs = np.where(tissue > 0)\n",
    "\n",
    "        elif tt in ('Follicular', 'Papillary') or (include_anaplastic and tt == 'Anaplastic'):\n",
    "            tumor = _resize_mask_to(d.get('mask', np.zeros((H, W), np.uint8)), H, W)\n",
    "            ys, xs = np.where((tumor > 0) & (tissue > 0))\n",
    "\n",
    "        else:\n",
    "            continue  # ignore anything else\n",
    "\n",
    "        if ys.size == 0:\n",
    "            continue\n",
    "        cap = min(max_pixels_per_image, ys.size)\n",
    "        idx = rs.choice(ys.size, size=cap, replace=False)\n",
    "        chunks.append(voxel[ys[idx], xs[idx], :])\n",
    "\n",
    "    if not chunks:\n",
    "        return np.empty((0, 0), np.float32)\n",
    "    return np.vstack(chunks).astype(np.float32)\n",
    "\n",
    "# ------------------------------\n",
    "# PCA + multi-scale features\n",
    "# ------------------------------\n",
    "def build_pca_and_scaler(train_X, n_components=2):\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    Xs = scaler.fit_transform(train_X)\n",
    "    pca = PCA(n_components=n_components, random_state=0)\n",
    "    pca.fit(Xs)\n",
    "    return scaler, pca\n",
    "\n",
    "def project_image_to_pcs(voxel, scaler, pca):\n",
    "    H, W, C = voxel.shape\n",
    "    Xs = scaler.transform(voxel.reshape(-1, C))\n",
    "    Z  = pca.transform(Xs).reshape(H, W, -1)   # [H,W,D]\n",
    "    return Z\n",
    "\n",
    "def _grad_mag(img):\n",
    "    gx = sobel(img, axis=0, mode='nearest')\n",
    "    gy = sobel(img, axis=1, mode='nearest')\n",
    "    return np.hypot(gx, gy)\n",
    "\n",
    "def image_features_from_pcs(pc_maps, sigmas=(0.0, 1.0, 2.0)):\n",
    "    H, W, D = pc_maps.shape\n",
    "    blurs = [pc_maps] + [\n",
    "        np.stack([gaussian_filter(pc_maps[..., i], s) for i in range(D)], axis=-1)\n",
    "        for s in sigmas if s > 0\n",
    "    ]\n",
    "    feats = blurs[:]\n",
    "    for arr in blurs:\n",
    "        gm = np.stack([_grad_mag(arr[..., i]) for i in range(arr.shape[-1])], axis=-1)\n",
    "        feats.append(gm)\n",
    "    return np.concatenate(feats, axis=-1).astype(np.float32)  # [H,W,F]\n",
    "\n",
    "# ------------------------------\n",
    "# Classifier trained on ALL tumors (modified fit: PCA source only)\n",
    "# ------------------------------\n",
    "class PixelPCAContextClassifierAll:\n",
    "    def __init__(self, n_pcs=8, sigmas=(0.0,1.0,2.0),\n",
    "                 use_linear_svc=False, calibration='sigmoid',\n",
    "                 C=1.0, max_iter=1000):\n",
    "        self.n_pcs = n_pcs\n",
    "        self.sigmas = tuple(sigmas)\n",
    "        self.use_linear_svc = use_linear_svc\n",
    "        self.calibration = calibration\n",
    "        self.C = C\n",
    "        self.max_iter = max_iter\n",
    "        self.scaler_ = None\n",
    "        self.pca_ = None\n",
    "        self.clf_ = None\n",
    "        self.full_order_ids_ = np.array([CLASS_TO_ID3[c] for c in TISSUES3])\n",
    "        self.present_ids_ = None\n",
    "\n",
    "    def fit(self, train_samples, max_pixels_per_image=200000, class_weight='balanced'):\n",
    "        # (A) PCA/scaler: train WITHOUT anaplastic\n",
    "        X_raw_pca = collect_pixels_for_pca_regions(\n",
    "            train_samples,\n",
    "            max_pixels_per_image=max_pixels_per_image,\n",
    "            rng=123,\n",
    "            black_tolerance=5,\n",
    "            include_anaplastic=False\n",
    "        )\n",
    "        if X_raw_pca.size == 0:\n",
    "            raise RuntimeError(\"No pixels collected for PCA.\")\n",
    "        self.scaler_, self.pca_ = build_pca_and_scaler(X_raw_pca, n_components=self.n_pcs)\n",
    "\n",
    "        # (B) Build contextual TRAIN set for classifier\n",
    "        feats_list, y_list = [], []\n",
    "        rs = np.random.RandomState(123)\n",
    "        for d in train_samples:\n",
    "            voxel  = np.asarray(d['grayscale_voxel'], np.float32)\n",
    "            cutvox = np.asarray(d['grayscale_image_cutoff_voxel'], np.uint8)\n",
    "            H, W, _ = voxel.shape\n",
    "            tissue = _tissue_mask_from_cutoff(cutvox)\n",
    "\n",
    "            tt = d.get('tissue_type', '')\n",
    "            if tt == 'Normal':\n",
    "                labels = np.full((H, W), CLASS_TO_ID3['Normal'], np.uint8)\n",
    "                labels[tissue == 0] = 255\n",
    "            elif tt in ('Follicular', 'Papillary'):\n",
    "                tumor = _resize_mask_to(d.get('mask', np.zeros((H, W), np.uint8)), H, W)\n",
    "                labels = np.full((H, W), CLASS_TO_ID3['Normal'], np.uint8)\n",
    "                labels[(tumor > 0) & (tissue > 0)] = CLASS_TO_ID3[tt]\n",
    "                labels[tissue == 0] = 255\n",
    "            else:\n",
    "                continue  # Anaplastic excluded from classifier labels\n",
    "\n",
    "            pcs = project_image_to_pcs(voxel, self.scaler_, self.pca_)      # [H,W,D]\n",
    "            F   = image_features_from_pcs(pcs, sigmas=self.sigmas)          # [H,W,F]\n",
    "            ys, xs = np.where(labels != 255)\n",
    "            if ys.size == 0:\n",
    "                continue\n",
    "            cap = min(30000, ys.size)\n",
    "            idx = rs.choice(ys.size, size=cap, replace=False)\n",
    "            feats_list.append(F[ys[idx], xs[idx], :])\n",
    "            y_list.append(labels[ys[idx], xs[idx]])\n",
    "\n",
    "        X_feat = np.vstack(feats_list)\n",
    "        y_feat = np.concatenate(y_list)\n",
    "\n",
    "        # (C) multinomial LR (or LinearSVC + calibration)\n",
    "        if not self.use_linear_svc:\n",
    "            base = LogisticRegression(\n",
    "                multi_class='multinomial',\n",
    "                solver='lbfgs',\n",
    "                C=self.C,\n",
    "                max_iter=self.max_iter,\n",
    "                class_weight=class_weight\n",
    "            )\n",
    "            self.clf_ = base.fit(X_feat, y_feat)\n",
    "            self.present_ids_ = self.clf_.classes_.astype(int)\n",
    "        else:\n",
    "            base = LinearSVC(C=self.C, max_iter=self.max_iter, class_weight=class_weight)\n",
    "            self.clf_ = CalibratedClassifierCV(base_estimator=base, cv=4, method=self.calibration)\n",
    "            self.clf_.fit(X_feat, y_feat)\n",
    "            self.present_ids_ = self.clf_.classes_.astype(int)\n",
    "        return self\n",
    "\n",
    "    def _expand_to_full(self, proba_small):\n",
    "        N = proba_small.shape[0]\n",
    "        proba_full = np.zeros((N, len(self.full_order_ids_)), dtype=np.float32)\n",
    "        for j, cls_id in enumerate(self.present_ids_):\n",
    "            proba_full[:, cls_id] = proba_small[:, j]\n",
    "        return proba_full\n",
    "\n",
    "    def predict_proba_map(self, sample):\n",
    "        voxel = np.asarray(sample['grayscale_voxel'], np.float32)  # already filtered channels\n",
    "        pcs   = project_image_to_pcs(voxel, self.scaler_, self.pca_)\n",
    "        F     = image_features_from_pcs(pcs, sigmas=self.sigmas)\n",
    "        H, W, Fdim = F.shape\n",
    "        proba_small = self.clf_.predict_proba(F.reshape(-1, Fdim))     # [H*W,K']\n",
    "        P = self._expand_to_full(proba_small).reshape(H, W, -1)        # [H,W,3] (N,F,P)\n",
    "        return P\n",
    "\n",
    "# ------------------------------\n",
    "# Fit PCA + classifier on TRAIN (all tumors)\n",
    "# ------------------------------\n",
    "px_model_all = PixelPCAContextClassifierAll(\n",
    "    n_pcs=8, sigmas=(0.0,1.0,2.0),\n",
    "    use_linear_svc=False, calibration='sigmoid',\n",
    "    C=1.0, max_iter=1000\n",
    ")\n",
    "print(\"[PX] Fitting PCA+Classifier on TRAIN with Normal + Follicular + Papillary ...\")\n",
    "px_model_all.fit(train_combined, max_pixels_per_image=200000, class_weight='balanced')\n",
    "print(\"[PX] ... done.\")\n",
    "\n",
    "# (optional) save\n",
    "with open(\"pixel_pca_context_classifier_all.pkl\", \"wb\") as f:\n",
    "    pickle.dump(px_model_all, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53dd31-d17e-4663-b4af-646ed57cfdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Tunable regional categorization (categorize-first, then filter)\n",
    "# =========================\n",
    "\n",
    "VALID_CLASSES3 = ['Normal', 'Follicular', 'Papillary']\n",
    "\n",
    "def _pred_label_map_over_tissue(px_model_all, d, black_tol=5):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      labels_map: [H,W] in {'Normal','Follicular','Papillary'} (string per pixel)\n",
    "      tissue:     [H,W] uint8 in {0,1}\n",
    "      P:          [H,W,3] probs in order TISSUES3\n",
    "    \"\"\"\n",
    "    P = px_model_all.predict_proba_map(d).astype(np.float32)  # [H,W,3]\n",
    "    cut = np.asarray(d['grayscale_image_cutoff_voxel'], np.uint8)\n",
    "    tissue = _tissue_mask_from_cutoff(cut, black_tolerance=black_tol)\n",
    "    if tissue.sum() == 0:\n",
    "        tissue = np.ones_like(tissue, np.uint8)\n",
    "\n",
    "    lbl_idx = np.argmax(P, axis=-1)  # [H,W] ints 0..2\n",
    "    labels_map = np.empty(lbl_idx.shape, dtype=object)\n",
    "    for k, name in enumerate(TISSUES3):\n",
    "        labels_map[lbl_idx == k] = name\n",
    "    return labels_map, tissue, P\n",
    "\n",
    "def _sliding_window_indices(H, W, win=64, stride=32):\n",
    "    ys = list(range(0, max(1, H - win + 1), stride))\n",
    "    xs = list(range(0, max(1, W - win + 1), stride))\n",
    "    if len(ys) == 0:\n",
    "        ys = [0]\n",
    "    if len(xs) == 0:\n",
    "        xs = [0]\n",
    "    if ys[-1] != max(0, H - win):\n",
    "        ys.append(max(0, H - win))\n",
    "    if xs[-1] != max(0, W - win):\n",
    "        xs.append(max(0, W - win))\n",
    "    return ys, xs\n",
    "\n",
    "def categorize_sample_regional(\n",
    "    px_model_all, d,\n",
    "    black_tol=5,\n",
    "    win=64, stride=32,\n",
    "    frac_thresh=0.60,\n",
    "    min_tissue_px_per_win=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Regional decision rule:\n",
    "\n",
    "      1) Argmax pixel-wise class labels within tissue.\n",
    "      2) Slide a window; in each window, if a cancer class\n",
    "         occupies ≥ frac_thresh of tissue pixels, mark it as present.\n",
    "      3) If multiple cancers are present, choose the one with\n",
    "         larger overall tissue presence; if none, classify as Normal.\n",
    "    \"\"\"\n",
    "    labels_map, tissue, _ = _pred_label_map_over_tissue(px_model_all, d, black_tol=black_tol)\n",
    "    H, W = tissue.shape\n",
    "\n",
    "    tissue_idx = tissue.astype(bool)\n",
    "    if tissue_idx.sum() == 0:\n",
    "        return 'Normal'\n",
    "\n",
    "    overall_counts = {\n",
    "        'Follicular': np.sum((labels_map == 'Follicular') & tissue_idx),\n",
    "        'Papillary':  np.sum((labels_map == 'Papillary')  & tissue_idx)\n",
    "    }\n",
    "\n",
    "    present_cancers = set()\n",
    "    ys, xs = _sliding_window_indices(H, W, win=win, stride=stride)\n",
    "\n",
    "    for y0 in ys:\n",
    "        for x0 in xs:\n",
    "            y1, x1 = y0 + win, x0 + win\n",
    "            sub_tissue = tissue[y0:y1, x0:x1].astype(bool)\n",
    "            tp = int(sub_tissue.sum())\n",
    "            if tp < min_tissue_px_per_win:\n",
    "                continue\n",
    "\n",
    "            sub_lbl = labels_map[y0:y1, x0:x1]\n",
    "            f_cnt = int(np.sum((sub_lbl == 'Follicular') & sub_tissue))\n",
    "            p_cnt = int(np.sum((sub_lbl == 'Papillary')  & sub_tissue))\n",
    "            f_frac = f_cnt / tp\n",
    "            p_frac = p_cnt / tp\n",
    "\n",
    "            if f_frac >= frac_thresh:\n",
    "                present_cancers.add('Follicular')\n",
    "            if p_frac >= frac_thresh:\n",
    "                present_cancers.add('Papillary')\n",
    "\n",
    "    if len(present_cancers) == 0:\n",
    "        return 'Normal'\n",
    "    if len(present_cancers) == 1:\n",
    "        return next(iter(present_cancers))\n",
    "\n",
    "    f_total = overall_counts['Follicular']\n",
    "    p_total = overall_counts['Papillary']\n",
    "    return 'Follicular' if f_total >= p_total else 'Papillary'\n",
    "\n",
    "# ------------------------------\n",
    "# Categorize FIRST, then filter out Follicular-dominant\n",
    "# ------------------------------\n",
    "def categorize_split(samples, px_model_all,\n",
    "                     black_tol=5, win=64, stride=32,\n",
    "                     frac_thresh=0.60, min_tissue_px_per_win=50):\n",
    "    \"\"\"\n",
    "    Returns list of dicts: [{ 'name':..., 'gt':..., 'pred':..., 'sample': d }, ...]\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for d in samples:\n",
    "        pred = categorize_sample_regional(\n",
    "            px_model_all, d,\n",
    "            black_tol=black_tol,\n",
    "            win=win, stride=stride,\n",
    "            frac_thresh=frac_thresh,\n",
    "            min_tissue_px_per_win=min_tissue_px_per_win\n",
    "        )\n",
    "        results.append({\n",
    "            'name': d.get('name', 'unknown_sample'),\n",
    "            'gt': d.get('tissue_type', None),\n",
    "            'pred': pred,\n",
    "            'sample': d\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def filter_after_categorization(cat_results):\n",
    "    \"\"\"\n",
    "    From categorized results, drop those predicted as the nontarget tumor.\n",
    "    Returns kept_samples, dropped_samples (original sample dicts).\n",
    "    \"\"\"\n",
    "    kept, dropped = [], []\n",
    "    for r in cat_results:\n",
    "        if r['pred'] == NONTARGET_TUMOR:\n",
    "            dropped.append(r['sample'])\n",
    "        else:\n",
    "            kept.append(r['sample'])\n",
    "    return kept, dropped\n",
    "\n",
    "# ---- RUN: categorize first, then filter ----\n",
    "REGIONAL_PARAMS = dict(black_tol=5, win=80, stride=10,\n",
    "                       frac_thresh=0.80, min_tissue_px_per_win=35)\n",
    "\n",
    "train_cat = categorize_split(train_combined, px_model_all, **REGIONAL_PARAMS)\n",
    "val_cat   = categorize_split(val_combined,   px_model_all, **REGIONAL_PARAMS)\n",
    "test_cat  = categorize_split(test_combined,  px_model_all, **REGIONAL_PARAMS)\n",
    "\n",
    "train_filtered, train_dropped = filter_after_categorization(train_cat)\n",
    "val_filtered,   val_dropped   = filter_after_categorization(val_cat)\n",
    "test_filtered,  test_dropped  = filter_after_categorization(test_cat)\n",
    "\n",
    "print(f\"[CATEGORIZE] counts (pred): \"\n",
    "      f\"train N/F/P = \"\n",
    "      f\"{sum(r['pred']=='Normal' for r in train_cat)}/\"\n",
    "      f\"{sum(r['pred']=='Follicular' for r in train_cat)}/\"\n",
    "      f\"{sum(r['pred']=='Papillary' for r in train_cat)}\")\n",
    "print(f\"[FILTER] kept: train={len(train_filtered)} val={len(val_filtered)} test={len(test_filtered)}\")\n",
    "print(f\"[FILTER] drop(Follicular-pred): train={len(train_dropped)} val={len(val_dropped)} test={len(test_dropped)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Confusion matrices & verbose misclassifications\n",
    "# ------------------------------\n",
    "def confusion_from_cat(cat_results, split_name):\n",
    "    y_true = [r['gt'] for r in cat_results if r['gt'] in VALID_CLASSES3]\n",
    "    y_pred = [r['pred'] for r in cat_results if r['gt'] in VALID_CLASSES3]\n",
    "    names  = [r['name'] for r in cat_results if r['gt'] in VALID_CLASSES3]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=VALID_CLASSES3)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=VALID_CLASSES3, yticklabels=VALID_CLASSES3)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Ground Truth\")\n",
    "    plt.title(f\"{split_name} Confusion Matrix (Regional categorization)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    mis = [(n, gt, pr) for n, gt, pr in zip(names, y_true, y_pred) if gt != pr]\n",
    "    anaplastic_as_normal = [r['name'] for r in cat_results\n",
    "                            if r['gt'] == 'Anaplastic' and r['pred'] == 'Normal']\n",
    "\n",
    "    print(f\"\\n[DETAILS] {split_name}\")\n",
    "    if mis:\n",
    "        print(f\"Misclassified samples ({len(mis)}):\")\n",
    "        for n, gt, pr in mis:\n",
    "            print(f\"  - {n}: GT={gt}, Pred={pr}\")\n",
    "    else:\n",
    "        print(\"No misclassifications in this split.\")\n",
    "\n",
    "    if anaplastic_as_normal:\n",
    "        print(f\"\\nAnaplastic → Normal cases ({len(anaplastic_as_normal)}):\")\n",
    "        for n in anaplastic_as_normal:\n",
    "            print(f\"  - {n}\")\n",
    "    print()\n",
    "\n",
    "# Pre-filter confusion matrices (to inspect gate performance)\n",
    "confusion_from_cat(train_cat, \"TRAIN (pre-filter)\")\n",
    "confusion_from_cat(val_cat,   \"VAL (pre-filter)\")\n",
    "confusion_from_cat(test_cat,  \"TEST (pre-filter)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88209b8a-2b56-4a07-b7ed-b41f6665f75d",
   "metadata": {},
   "source": [
    "# Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7da436-d347-4a76-976b-5f5380088413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Patient-level bootstrapped CIs for regional categorization\n",
    "# ------------------------------\n",
    "def _patient_id_from_name(name):\n",
    "    \"\"\"\n",
    "    Map a sample name to a patient ID.\n",
    "    Example naming: 'SSW-23-12345_A1' -> patient 'SSW-23-12345'.\n",
    "    \"\"\"\n",
    "    s = str(name)\n",
    "    parts = s.split(\"_\")\n",
    "    if len(parts) >= 2:\n",
    "        return \"_\".join(parts[:2])\n",
    "    return s\n",
    "\n",
    "\n",
    "def _extract_arrays_for_bootstrap(cat_results):\n",
    "    \"\"\"\n",
    "    From cat_results, extract:\n",
    "      y_true : np.array of GT labels (strings)\n",
    "      y_pred : np.array of predicted labels (strings)\n",
    "      pids   : np.array of patient IDs (strings)\n",
    "    Restricted to VALID_CLASSES3.\n",
    "    \"\"\"\n",
    "    y_true, y_pred, pids = [], [], []\n",
    "    for r in cat_results:\n",
    "        gt = r.get(\"gt\", None)\n",
    "        if gt not in VALID_CLASSES3:\n",
    "            continue\n",
    "        y_true.append(gt)\n",
    "        y_pred.append(r.get(\"pred\", None))\n",
    "        pids.append(_patient_id_from_name(r.get(\"name\", \"unknown_sample\")))\n",
    "    return np.array(y_true, dtype=object), np.array(y_pred, dtype=object), np.array(pids, dtype=object)\n",
    "\n",
    "\n",
    "def _compute_metrics(y_true, y_pred, classes=VALID_CLASSES3):\n",
    "    \"\"\"\n",
    "    Compute overall accuracy and per-class recall.\n",
    "    Returns:\n",
    "      acc, per_class_recall_dict\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=object)\n",
    "    y_pred = np.asarray(y_pred, dtype=object)\n",
    "\n",
    "    if y_true.size == 0:\n",
    "        return np.nan, {c: np.nan for c in classes}\n",
    "\n",
    "    acc = np.mean(y_true == y_pred)\n",
    "\n",
    "    recalls = {}\n",
    "    for c in classes:\n",
    "        mask = (y_true == c)\n",
    "        if mask.sum() == 0:\n",
    "            recalls[c] = np.nan  # avoid division by zero\n",
    "        else:\n",
    "            tp = np.sum((y_pred == c) & mask)\n",
    "            fn = np.sum((y_pred != c) & mask)\n",
    "            recalls[c] = tp / (tp + fn + 1e-6)\n",
    "    return float(acc), recalls\n",
    "\n",
    "\n",
    "def bootstrap_regional_metrics(cat_results, n_boot=1000, random_state=0):\n",
    "    \"\"\"\n",
    "    Patient-level bootstrap for regional categorization.\n",
    "\n",
    "    - Resamples patients (by ID) with replacement.\n",
    "    - Aggregates all samples for each selected patient.\n",
    "    - Computes:\n",
    "        * overall accuracy\n",
    "        * per-class recall for Normal, Follicular, Papillary\n",
    "    - Returns a dict with point estimates and 95% CIs.\n",
    "    \"\"\"\n",
    "    y_true, y_pred, pids = _extract_arrays_for_bootstrap(cat_results)\n",
    "    classes = VALID_CLASSES3\n",
    "\n",
    "    # Point estimates on the original data\n",
    "    acc_point, rec_point = _compute_metrics(y_true, y_pred, classes=classes)\n",
    "\n",
    "    # Prepare bootstrap\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    unique_pids = np.unique(pids)\n",
    "    n_patients = len(unique_pids)\n",
    "    if n_patients == 0:\n",
    "        print(\"[BOOT] No patients found for bootstrap.\")\n",
    "        return {}\n",
    "\n",
    "    acc_boot = []\n",
    "    rec_boot = {c: [] for c in classes}\n",
    "\n",
    "    # Pre-index by patient to speed sampling\n",
    "    pid_to_idx = {pid: np.where(pids == pid)[0] for pid in unique_pids}\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        # sample patients with replacement\n",
    "        sampled_pids = rng.choice(unique_pids, size=n_patients, replace=True)\n",
    "\n",
    "        # collect indices for all tiles from these patients\n",
    "        idx_list = []\n",
    "        for pid in sampled_pids:\n",
    "            idx_list.append(pid_to_idx[pid])\n",
    "        idx_all = np.concatenate(idx_list, axis=0)\n",
    "\n",
    "        y_true_b = y_true[idx_all]\n",
    "        y_pred_b = y_pred[idx_all]\n",
    "\n",
    "        acc_b, rec_b = _compute_metrics(y_true_b, y_pred_b, classes=classes)\n",
    "        acc_boot.append(acc_b)\n",
    "        for c in classes:\n",
    "            if not np.isnan(rec_b[c]):\n",
    "                rec_boot[c].append(rec_b[c])\n",
    "\n",
    "    acc_boot = np.array(acc_boot, dtype=float)\n",
    "    rec_boot = {c: np.array(vals, dtype=float) for c, vals in rec_boot.items()}\n",
    "\n",
    "    def _ci(arr):\n",
    "        arr = arr[~np.isnan(arr)]\n",
    "        if arr.size == 0:\n",
    "            return (np.nan, np.nan)\n",
    "        return (float(np.percentile(arr, 2.5)),\n",
    "                float(np.percentile(arr, 97.5)))\n",
    "\n",
    "    results = {\n",
    "        \"accuracy\": {\n",
    "            \"point\": float(acc_point),\n",
    "            \"ci_95\": _ci(acc_boot),\n",
    "        },\n",
    "        \"recall\": {},\n",
    "        \"n_patients\": int(n_patients),\n",
    "        \"n_boot\": int(n_boot),\n",
    "    }\n",
    "    for c in classes:\n",
    "        results[\"recall\"][c] = {\n",
    "            \"point\": float(rec_point[c]) if not np.isnan(rec_point[c]) else np.nan,\n",
    "            \"ci_95\": _ci(rec_boot[c]),\n",
    "        }\n",
    "\n",
    "    # Pretty print\n",
    "    print(f\"[BOOT] Patient-level bootstrap (n_patients={n_patients}, n_boot={n_boot})\")\n",
    "    acc_lo, acc_hi = results[\"accuracy\"][\"ci_95\"]\n",
    "    print(f\"  Accuracy: {results['accuracy']['point']:.3f} \"\n",
    "          f\"(95% CI {acc_lo:.3f}–{acc_hi:.3f})\")\n",
    "\n",
    "    for c in classes:\n",
    "        r_pt = results[\"recall\"][c][\"point\"]\n",
    "        r_lo, r_hi = results[\"recall\"][c][\"ci_95\"]\n",
    "        print(f\"  Recall {c:10s}: {r_pt:.3f} \"\n",
    "              f\"(95% CI {r_lo:.3f}–{r_hi:.3f})\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"\\n[BOOTSTRAP] TRAIN split (regional categorization)\")\n",
    "train_boot = bootstrap_regional_metrics(train_cat, n_boot=2000, random_state=0)\n",
    "\n",
    "print(\"\\n[BOOTSTRAP] VAL split (regional categorization)\")\n",
    "val_boot = bootstrap_regional_metrics(val_cat, n_boot=2000, random_state=1)\n",
    "\n",
    "print(\"\\n[BOOTSTRAP] TEST split (regional categorization)\")\n",
    "test_boot = bootstrap_regional_metrics(test_cat, n_boot=2000, random_state=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b088e42-e57b-4759-a5d8-469e74ae0a8c",
   "metadata": {},
   "source": [
    "# Visualizing Pixel Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3a12e-491d-4d95-8629-37de5fafee0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------ visualize per-image probability maps ------------\n",
    "def visualize_px_maps_combined(samples, model, max_imgs=20,\n",
    "                               save_path=\"px_maps_combined2.png\"):\n",
    "    \"\"\"\n",
    "    Creates ONE combined figure showing probability maps for all samples.\n",
    "    Columns = Normal, Follicular, Papillary.\n",
    "    Rows    = 1 row per sample.\n",
    "\n",
    "    samples: list of sample dicts\n",
    "    model:   trained PixelPCAContextClassifierAll\n",
    "    \"\"\"\n",
    "    k = min(max_imgs, len(samples))\n",
    "\n",
    "    # 3 columns: Normal, Follicular, Papillary\n",
    "    fig, axes = plt.subplots(k, 3, figsize=(12, 4 * k))\n",
    "\n",
    "    # Ensure axes is always 2D\n",
    "    if k == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "    for row in range(k):\n",
    "        ex = samples[row]\n",
    "        P  = model.predict_proba_map(ex)  # [H,W,3]\n",
    "\n",
    "        # Extract individual probability maps\n",
    "        pN = P[..., CLASS_TO_ID3['Normal']]\n",
    "        pF = P[..., CLASS_TO_ID3['Follicular']]\n",
    "        pP = P[..., CLASS_TO_ID3['Papillary']]\n",
    "\n",
    "        # Row plots\n",
    "        maps = [pN, pF, pP]\n",
    "        titles = [\"P(Normal)\", \"P(Follicular)\", \"P(Papillary)\"]\n",
    "\n",
    "        for col in range(3):\n",
    "            ax = axes[row, col]\n",
    "            ax.imshow(maps[col], vmin=0, vmax=1)\n",
    "            ax.set_title(titles[col])\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"[PX] Saved combined figure: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3983c191-0789-4c90-92e7-6e5686854f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_px_maps_combined(train_filtered, model=px_model_all, max_imgs=20, save_prefix=\"pixel_prob_maps_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e35ae03-0d58-4203-acec-f77ac4e3fa28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_px_maps_combined(test_filtered, model=px_model_all, max_imgs=20, save_prefix=\"pixel_prob_maps_val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead53c15-2df6-41ce-bf0d-be6f7f2c5600",
   "metadata": {},
   "source": [
    "# Visualizing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67316aa-4e8f-4199-b08c-41060ea1e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# PCA diagnostics (PC1–PC2 scatter, variance, loadings)\n",
    "# Reuses the PCA/scaler already trained in px_model_all\n",
    "# ===========================================\n",
    "\n",
    "def _boundary_mask(bin_mask):\n",
    "    from scipy.ndimage import binary_dilation, binary_erosion\n",
    "    m = np.asarray(bin_mask)\n",
    "    if m.ndim == 3 and m.shape[-1] == 1:\n",
    "        m = m[..., 0]\n",
    "    m = m.astype(bool)\n",
    "    if m.size == 0:\n",
    "        return np.zeros_like(m, dtype=np.uint8)\n",
    "    dil = binary_dilation(m, iterations=1)\n",
    "    ero = binary_erosion(m, iterations=1)\n",
    "    return np.logical_xor(dil, ero).astype(np.uint8)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Collect pixels for Normal + Follicular + Papillary\n",
    "# (balanced; boundary-emphasized for tumors)\n",
    "# ---------------------------------------------------------------------\n",
    "def collect_pixel_samples_all_tumors_balanced_boundary(\n",
    "    samples,\n",
    "    max_pixels_per_image=20000,\n",
    "    per_class_cap=150000,\n",
    "    rng=42,\n",
    "    black_tolerance=5,\n",
    "    boundary_boost=0.5\n",
    "):\n",
    "    rs = np.random.RandomState(rng)\n",
    "    per_class = {c: [] for c in TISSUES3}\n",
    "\n",
    "    # infer channel count\n",
    "    C_channels = None\n",
    "    for d0 in samples:\n",
    "        if 'grayscale_voxel' in d0:\n",
    "            C_channels = int(np.asarray(d0['grayscale_voxel']).shape[-1])\n",
    "            break\n",
    "    if C_channels is None:\n",
    "        return np.empty((0, 0), np.float32), np.empty((0,), np.int32)\n",
    "\n",
    "    for d in samples:\n",
    "        voxel = np.asarray(d['grayscale_voxel'], np.float32)  # [H,W,C]\n",
    "        cut   = np.asarray(d['grayscale_image_cutoff_voxel'], np.uint8)\n",
    "        H, W, _ = voxel.shape\n",
    "        tissue = _tissue_mask_from_cutoff(cut, black_tolerance)\n",
    "        tt = d.get('tissue_type', '')\n",
    "\n",
    "        # Normal slides: sample tissue pixels\n",
    "        if tt == 'Normal':\n",
    "            ys, xs = np.where(tissue > 0)\n",
    "            if ys.size == 0:\n",
    "                continue\n",
    "            cap = min(max_pixels_per_image, ys.size)\n",
    "            idx = rs.choice(ys.size, size=cap, replace=False)\n",
    "            per_class['Normal'].append(voxel[ys[idx], xs[idx], :])\n",
    "            continue\n",
    "\n",
    "        # Tumor slides: Follicular / Papillary\n",
    "        if tt in ('Follicular', 'Papillary'):\n",
    "            tumor = np.asarray(d.get('mask', np.zeros((H, W), np.uint8)), np.uint8)\n",
    "            if tumor.ndim == 3 and tumor.shape[-1] == 1:\n",
    "                tumor = tumor[..., 0]\n",
    "            if tumor.shape != (H, W):\n",
    "                tumor = tf.image.resize(\n",
    "                    tumor[..., None], (H, W),\n",
    "                    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "                ).numpy().squeeze().astype(np.uint8)\n",
    "\n",
    "            y_t, x_t = np.where((tumor > 0) & (tissue > 0))   # tumor pixels\n",
    "            y_n, x_n = np.where((tumor == 0) & (tissue > 0))  # same-slide normal pixels\n",
    "\n",
    "            bnd = _boundary_mask(tumor)\n",
    "            half_cap = max_pixels_per_image // 2\n",
    "            cap_b  = int(max(0, min(1.0, boundary_boost)) * half_cap)\n",
    "            cap_nt = max(0, half_cap - cap_b)\n",
    "\n",
    "            def sub(ys, xs, k):\n",
    "                if ys.size <= 0 or k <= 0:\n",
    "                    return np.empty((0,), int), np.empty((0,), int)\n",
    "                k = min(k, ys.size)\n",
    "                idx = rs.choice(ys.size, size=k, replace=False)\n",
    "                return ys[idx], xs[idx]\n",
    "\n",
    "            if y_t.size:\n",
    "                tumor_is_bnd = (bnd[y_t, x_t] > 0)\n",
    "                y_tb,  x_tb  = y_t[tumor_is_bnd],  x_t[tumor_is_bnd]\n",
    "                y_tnb, x_tnb = y_t[~tumor_is_bnd], x_t[~tumor_is_bnd]\n",
    "            else:\n",
    "                y_tb = x_tb = y_tnb = x_tnb = np.empty((0,), int)\n",
    "\n",
    "            y_tb,  x_tb  = sub(y_tb,  x_tb,  cap_b)\n",
    "            y_tnb, x_tnb = sub(y_tnb, x_tnb, cap_nt)\n",
    "            y_n,   x_n   = sub(y_n,   x_n,   half_cap)\n",
    "\n",
    "            if y_tb.size + y_tnb.size:\n",
    "                y_all = np.concatenate([y_tb, y_tnb])\n",
    "                x_all = np.concatenate([x_tb, x_tnb])\n",
    "                per_class[tt].append(voxel[y_all, x_all, :])\n",
    "            if y_n.size:\n",
    "                per_class['Normal'].append(voxel[y_n, x_n, :])\n",
    "\n",
    "    # stack & balance classes\n",
    "    for k in per_class:\n",
    "        per_class[k] = (\n",
    "            np.vstack(per_class[k]).astype(np.float32)\n",
    "            if len(per_class[k]) else\n",
    "            np.empty((0, C_channels), np.float32)\n",
    "        )\n",
    "\n",
    "    sizes = {k: per_class[k].shape[0] for k in TISSUES3}\n",
    "    nonzero = [v for v in sizes.values() if v > 0]\n",
    "    if not nonzero:\n",
    "        return np.empty((0, C_channels), np.float32), np.empty((0,), np.int32)\n",
    "    target_count = min(min(nonzero), per_class_cap)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    for cls_name in TISSUES3:\n",
    "        Xc = per_class[cls_name]\n",
    "        if Xc.shape[0] == 0:\n",
    "            continue\n",
    "        if Xc.shape[0] > target_count:\n",
    "            idx = np.random.RandomState(rng).choice(Xc.shape[0], size=target_count, replace=False)\n",
    "            Xc = Xc[idx]\n",
    "        X_list.append(Xc)\n",
    "        y_list.append(np.full((Xc.shape[0],), CLASS_TO_ID3[cls_name], np.int32))\n",
    "\n",
    "    X_raw = np.vstack(X_list)\n",
    "    y     = np.concatenate(y_list)\n",
    "    return X_raw, y\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Build pixel dataset for PCA diagnostics\n",
    "# ---------------------------------------------------------------------\n",
    "def get_pixel_dataset_for_pca(train_samples,\n",
    "                              max_pixels_per_image=12000,\n",
    "                              per_class_cap=150000,\n",
    "                              rng=123):\n",
    "    \"\"\"\n",
    "    Returns X_raw (N,C) and y (N,) using the same balanced,\n",
    "    boundary-aware sampler as the main pipeline.\n",
    "    \"\"\"\n",
    "    X_raw, y = collect_pixel_samples_all_tumors_balanced_boundary(\n",
    "        train_samples,\n",
    "        max_pixels_per_image=max_pixels_per_image,\n",
    "        per_class_cap=per_class_cap,\n",
    "        rng=rng\n",
    "    )\n",
    "    if X_raw.size == 0:\n",
    "        raise RuntimeError(\"No pixel samples collected for PCA diagnostics.\")\n",
    "    return X_raw, y\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Reuse trained PCA / scaler from px_model_all\n",
    "# ---------------------------------------------------------------------\n",
    "if getattr(px_model_all, \"scaler_\", None) is None or getattr(px_model_all, \"pca_\", None) is None:\n",
    "    raise RuntimeError(\"px_model_all must be trained first (scaler_/pca_ not found).\")\n",
    "\n",
    "scaler_used = px_model_all.scaler_\n",
    "pca_used    = px_model_all.pca_\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2D PCA scatter (PC1 vs PC2)\n",
    "# ---------------------------------------------------------------------\n",
    "def plot_pca_scatter_2d(X_raw, y, scaler, pca, sample_cap=200000, alpha=0.25):\n",
    "    \"\"\"\n",
    "    PC1 vs PC2 scatter colored by class y (0: Normal, 1: Follicular, 2: Papillary).\n",
    "    \"\"\"\n",
    "    if sample_cap is not None and X_raw.shape[0] > sample_cap:\n",
    "        rs = np.random.RandomState(0)\n",
    "        idx = rs.choice(X_raw.shape[0], size=sample_cap, replace=False)\n",
    "        X_plot = X_raw[idx]\n",
    "        y_plot = y[idx]\n",
    "    else:\n",
    "        X_plot = X_raw\n",
    "        y_plot = y\n",
    "\n",
    "    Xs = scaler.transform(X_plot)\n",
    "    Z  = pca.transform(Xs)\n",
    "    pc1, pc2 = Z[:, 0], Z[:, 1]\n",
    "\n",
    "    label_names = ['Normal', 'Follicular', 'Papillary']\n",
    "    colors = {0: 'tab:blue', 1: 'tab:orange', 2: 'tab:green'}\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    for lab in range(3):\n",
    "        m = (y_plot == lab)\n",
    "        if np.any(m):\n",
    "            plt.scatter(pc1[m], pc2[m], s=5, alpha=alpha,\n",
    "                        label=label_names[lab], c=colors[lab])\n",
    "    plt.axhline(0, lw=0.5, c='k', alpha=0.2)\n",
    "    plt.axvline(0, lw=0.5, c='k', alpha=0.2)\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title('PCA scatter (PC1 vs PC2)')\n",
    "    plt.legend(markerscale=3, frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Explained variance (bar + cumulative line)\n",
    "# ---------------------------------------------------------------------\n",
    "def plot_pca_explained_variance(pca, max_components=8):\n",
    "    r = np.asarray(pca.explained_variance_ratio_)\n",
    "    k = min(max_components, r.shape[0])\n",
    "    x = np.arange(1, k + 1)\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.bar(x, r[:k], label='Explained variance ratio')\n",
    "    cum = np.cumsum(r[:k])\n",
    "    plt.plot(x, cum, marker='o', label='Cumulative')\n",
    "    plt.xticks(x)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Variance Ratio')\n",
    "    plt.title(f'Explained Variance (first {k} PCs)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Loadings plots (stacked per channel; abs values; prints top-K)\n",
    "# ---------------------------------------------------------------------\n",
    "def plot_pca_loadings_stacked(pca, feature_names=None, max_components=8,\n",
    "                              weight_by_variance=True, top_k=10, start_index=1):\n",
    "    comps = np.asarray(pca.components_)               # (n_components, n_features)\n",
    "    evr   = np.asarray(pca.explained_variance_ratio_) # (n_components,)\n",
    "    n_comp, n_feat = comps.shape\n",
    "    k = min(max_components, n_comp)\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"ch{start_index + j}\" for j in range(n_feat)]\n",
    "\n",
    "    vals = np.abs(comps[:k, :])\n",
    "    if weight_by_variance:\n",
    "        vals = (vals.T * evr[:k]).T\n",
    "\n",
    "    importance = vals.sum(axis=0)\n",
    "\n",
    "    # Full stacked bar plot\n",
    "    x = np.arange(n_feat)\n",
    "    plt.figure(figsize=(max(10, 0.5 * n_feat), 5))\n",
    "    bottom = np.zeros(n_feat, dtype=float)\n",
    "    for pc_idx in range(k):\n",
    "        plt.bar(x, vals[pc_idx, :], bottom=bottom, label=f\"PC{pc_idx + 1}\")\n",
    "        bottom += vals[pc_idx, :]\n",
    "    plt.xticks(x, feature_names, rotation=90)\n",
    "    plt.ylabel(\"Stacked |loading|\" + (\" × EVR\" if weight_by_variance else \"\"))\n",
    "    plt.title(f\"PCA Loadings (first {k} PCs) — stacked per channel\")\n",
    "    plt.legend(ncol=min(4, k), fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Top-K channels by importance\n",
    "    order = np.argsort(importance)[::-1]\n",
    "    top_idx = order[:top_k]\n",
    "    print(f\"\\nTop {top_k} channels by importance \"\n",
    "          f\"({'|loading|×EVR' if weight_by_variance else '|loading|'} over first {k} PCs):\")\n",
    "    for rnk, i in enumerate(top_idx, 1):\n",
    "        print(f\"{rnk:2d}. {feature_names[i]}  —  score={importance[i]:.6f}\")\n",
    "\n",
    "    plt.figure(figsize=(max(8, 0.7 * top_k), 4))\n",
    "    plt.bar(np.arange(top_k), importance[top_idx])\n",
    "    plt.xticks(np.arange(top_k), [feature_names[i] for i in top_idx],\n",
    "               rotation=45, ha='right')\n",
    "    plt.ylabel(\"Importance (sum of |loading|\" +\n",
    "               (\" × EVR\" if weight_by_variance else \"\") + \")\")\n",
    "    plt.title(f\"Top {top_k} Channels by PCA Loading Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Stacked breakdown for top-K channels\n",
    "    plt.figure(figsize=(max(8, 0.7 * top_k), 4.5))\n",
    "    bottom = np.zeros(top_k, dtype=float)\n",
    "    for pc_idx in range(k):\n",
    "        plt.bar(np.arange(top_k), vals[pc_idx, top_idx],\n",
    "                bottom=bottom, label=f\"PC{pc_idx + 1}\")\n",
    "        bottom += vals[pc_idx, top_idx]\n",
    "    plt.xticks(np.arange(top_k), [feature_names[i] for i in top_idx],\n",
    "               rotation=45, ha='right')\n",
    "    plt.ylabel(\"Stacked |loading|\" + (\" × EVR\" if weight_by_variance else \"\"))\n",
    "    plt.title(f\"Top {top_k}: Stacked Contribution by PC\")\n",
    "    plt.legend(ncol=min(4, k), fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"importance\": importance,\n",
    "        \"order_desc\": order,\n",
    "        \"top_idx\": top_idx,\n",
    "        \"stack_vals\": vals,\n",
    "        \"evr\": evr,\n",
    "    }\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Per-sample PCA averages and plotting (one point per slide)\n",
    "# ---------------------------------------------------------------------\n",
    "def compute_sample_pca_averages(samples, scaler, pca, black_tolerance=5, split_tags=None):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      Z_avg:   (N, D) mean PC per sample over tissue pixels\n",
    "      names:   list of sample names\n",
    "      labels:  list of sample GT tissue_type\n",
    "      splits:  list of split tags ('train'|'val'|'test'|'unknown')\n",
    "    \"\"\"\n",
    "    if split_tags is None:\n",
    "        split_tags = ['unknown'] * len(samples)\n",
    "\n",
    "    Z_list, names, labels, splits = [], [], [], []\n",
    "    for d, sp in zip(samples, split_tags):\n",
    "        voxel  = np.asarray(d['grayscale_voxel'], np.float32)            # [H,W,C]\n",
    "        cutvox = np.asarray(d['grayscale_image_cutoff_voxel'], np.uint8) # [H,W,C]\n",
    "        pcs    = project_image_to_pcs(voxel, scaler, pca)                # [H,W,D]\n",
    "        tissue = (cutvox > black_tolerance).any(axis=-1).astype(np.uint8)\n",
    "\n",
    "        m = tissue > 0\n",
    "        if np.any(m):\n",
    "            Z_mean = pcs[m].mean(axis=0)\n",
    "        else:\n",
    "            Z_mean = pcs.reshape(-1, pcs.shape[-1]).mean(axis=0)\n",
    "\n",
    "        Z_list.append(Z_mean.astype(np.float32))\n",
    "        names.append(d.get('name', 'unknown'))\n",
    "        labels.append(d.get('tissue_type', 'unknown'))\n",
    "        splits.append(sp)\n",
    "\n",
    "    return np.vstack(Z_list), names, labels, splits\n",
    "\n",
    "def plot_sample_pca_averages(Z_avg, sample_labels, split_tags, alpha=0.9):\n",
    "    \"\"\"\n",
    "    PC1 vs PC2; one point per sample. Color = tissue label; Marker = split.\n",
    "    \"\"\"\n",
    "    color_map  = {'Normal':'tab:blue', 'Follicular':'tab:orange',\n",
    "                  'Papillary':'tab:green', 'Anaplastic':'tab:red'}\n",
    "    marker_map = {'train':'o', 'val':'s', 'test':'^', 'unknown':'x'}\n",
    "    order_lbl  = ['Normal', 'Follicular', 'Papillary', 'Anaplastic']\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    for lbl in sorted(set(sample_labels),\n",
    "                      key=lambda x: order_lbl.index(x) if x in order_lbl else 999):\n",
    "        for sp in ['train', 'val', 'test', 'unknown']:\n",
    "            idx = [i for i, (L, S) in enumerate(zip(sample_labels, split_tags))\n",
    "                   if L == lbl and S == sp]\n",
    "            if not idx:\n",
    "                continue\n",
    "            pts = Z_avg[idx]\n",
    "            plt.scatter(pts[:, 0], pts[:, 1],\n",
    "                        s=40, alpha=alpha,\n",
    "                        c=color_map.get(lbl, 'gray'),\n",
    "                        marker=marker_map.get(sp, 'x'),\n",
    "                        label=f\"{lbl} — {sp}\")\n",
    "\n",
    "    plt.axhline(0, lw=0.5, c='k', alpha=0.2)\n",
    "    plt.axvline(0, lw=0.5, c='k', alpha=0.2)\n",
    "    plt.xlabel('PC1 (sample mean)')\n",
    "    plt.ylabel('PC2 (sample mean)')\n",
    "    plt.title('Per-sample PCA (mean over tissue pixels) — split-marked')\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    uniq = dict(zip(labels, handles))\n",
    "    plt.legend(uniq.values(), uniq.keys(), frameon=True, fontsize=9, ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Example usage — BEFORE any filtering\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Pixel dataset from TRAIN\n",
    "X_raw, y = get_pixel_dataset_for_pca(\n",
    "    train_combined,\n",
    "    max_pixels_per_image=12000,\n",
    "    per_class_cap=150000,\n",
    "    rng=123\n",
    ")\n",
    "\n",
    "# 2) Pixel-level PCA diagnostics\n",
    "plot_pca_scatter_2d(X_raw, y, scaler_used, pca_used,\n",
    "                    sample_cap=200000, alpha=0.25)\n",
    "plot_pca_explained_variance(pca_used, max_components=8)\n",
    "\n",
    "feature_names = None  # auto-label as ch1..chN\n",
    "_ = plot_pca_loadings_stacked(\n",
    "    pca_used,\n",
    "    feature_names=feature_names,\n",
    "    max_components=8,\n",
    "    weight_by_variance=True,\n",
    "    top_k=10,\n",
    "    start_index=1\n",
    ")\n",
    "\n",
    "# 3) Per-sample PCA means across train/val/test\n",
    "samples_for_plot = []\n",
    "split_tags = []\n",
    "for d in train_combined:\n",
    "    samples_for_plot.append(d); split_tags.append('train')\n",
    "for d in val_combined:\n",
    "    samples_for_plot.append(d); split_tags.append('val')\n",
    "for d in test_combined:\n",
    "    samples_for_plot.append(d); split_tags.append('test')\n",
    "\n",
    "Z_avg, sample_names, sample_labels, splits = compute_sample_pca_averages(\n",
    "    samples_for_plot, scaler_used, pca_used, black_tolerance=5, split_tags=split_tags\n",
    ")\n",
    "\n",
    "plot_sample_pca_averages(Z_avg, sample_labels, splits, alpha=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf548a-7815-4d05-89c2-9bead986253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Save regional categorization results to Excel\n",
    "# -----------------------------------------\n",
    "rows = []\n",
    "\n",
    "for split_name, cat_list in [\n",
    "    (\"train\", train_cat),\n",
    "    (\"val\",   val_cat),\n",
    "    (\"test\",  test_cat),\n",
    "]:\n",
    "    for r in cat_list:\n",
    "        rows.append({\n",
    "            \"split\":        split_name,\n",
    "            \"name\":         r.get(\"name\", \"unknown_sample\"),\n",
    "            \"ground_truth\": r.get(\"gt\", None),\n",
    "            \"predicted\":    r.get(\"pred\", None),\n",
    "        })\n",
    "\n",
    "df_cat = pd.DataFrame(rows)\n",
    "excel_path = \"regional_categorization_results.xlsx\"\n",
    "df_cat.to_excel(excel_path, index=False)\n",
    "print(f\"[SAVE] Wrote regional categorization results to {excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b104042-6648-4256-910f-f1e4463009e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
